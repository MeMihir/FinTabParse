# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NhvJI1ftqndwtcNIRUFqlpYYew04qzhH

# Setup

## github and weights
"""

# %cd /content/
# !rm -r /content/FinTabParse
# !git clone https://github.com/MeMihir/FinTabParse.git
!cd FinTabParse/ && git checkout chunking

!cd FinTabParse/utils/ && pip install -e ./transformers-2.6.0/

! mkdir ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle
! chmod 600 ~/.kaggle/kaggle.json
!kaggle kernels output mihirpavuskar/tat-qa-train -p /content/FinTabParse/models/

# !cd /content/FinTabParse/models/ && git clone https://github.com/wenhuchen/HybridQA.git
# !cd /content/FinTabParse/models/HybridQA && wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip && unzip models.zip
!cd /content/FinTabParse/models/HybridR && wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip && unzip models.zip

! cp /content/drive/MyDrive/Capstone/Table_Parsing_QA/models/model_f-1_e-3.pt /content/FinTabParse/weights/questions_classifier

"""## installs"""

!pip install tensorboardX
!pip install tqdm
!pip install fuzzywuzzy
!pip install dateparser

!cd ./FinTabParse/ && pip install -r requirement.txt
!pip install https://data.pyg.org/whl/torch-1.7.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl

# !pip install tensorflow==2.8.0

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')

# !pip install -U transformers===4.0.0

"""# Code"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/FinTabParse
# !rm -r /content/FinTabParse/inputs/*

"""## main.py"""

import shutil
import torch
import copy

from transformers import  BertTokenizer, AlbertTokenizer, AlbertForQuestionAnswering
from models.questions_classifier import BertClassifier

from utils.file_handling import file_to_list, json_to_dict, dict_to_json
from preprocessing.TagOp_preprop import tagop_preprocessing
from preprocessing.HybridR_preprop import hybridr_preprocessing

def paragraph_qa_model(questions, text):
  AlTokenizer = AlbertTokenizer.from_pretrained("twmkn9/albert-base-v2-squad2")
  AlQA = AlbertForQuestionAnswering.from_pretrained("twmkn9/albert-base-v2-squad2")
  answers = copy.deepcopy(questions)

  for i,que in enumerate(questions):
    inputs = AlTokenizer(que['question'], text, return_tensors="pt")
    with torch.no_grad():
        outputs = AlQA(**inputs, return_dict=True)

    answer_start_index = outputs.start_logits.argmax()
    answer_end_index = outputs.end_logits.argmax()
    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
    answers[i]['answers'].append({
        'answer': AlTokenizer.decode(predict_answer_tokens),
        'score': outputs.start_logits.max()*outputs.end_logits.max()
        })

  return answers


def question_classifier_model(ques, model_path):
  tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
  questions = list(map(lambda x: x['question'], ques))
  question_classifier = BertClassifier()
  question_classifier.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))

  question_infs = list(map(lambda question_input: tokenizer(question_input, padding='max_length', max_length = 512, truncation=True, return_tensors="pt"), questions))
  question_preds = ques.copy()

  for i, que_inf in enumerate(question_infs):
    question_id = que_inf['input_ids']
    question_mask = que_inf['attention_mask'].squeeze(1)
    pred = question_classifier(question_id, question_mask)
    question_preds[i]['class'] = pred.argmax(dim=1).numpy()[0]

  return question_preds

import os
from models.HybridR.preprocessing import preprocessing_main

def prepare_hybridR_data(questions, paragraphs, table, question_preds):
  hybridR_questions = list(filter(lambda x: x['class']==0, question_preds))
  hybirdR_paragraphs, hybirdR_ques, hybirdR_table, has_links = hybridr_preprocessing(table, paragraphs, hybridR_questions)
  print(has_links, hybirdR_table)

  if(has_links==False): return has_links

  os.makedirs('models/HybridR/test_inputs/', exist_ok=True)
  dict_to_json(hybirdR_ques, 'models/HybridR/test_inputs/test.json')

  os.makedirs('inputs/request_tok', exist_ok=True)
  dict_to_json(hybirdR_paragraphs, 'inputs/request_tok/table_0.json')

  os.makedirs('inputs/tables_tok', exist_ok=True)
  dict_to_json(hybirdR_table, 'inputs/tables_tok/table_0.json')

  del hybirdR_ques
  del hybirdR_paragraphs
  del hybirdR_table

  preprocessing_main()

  !rm -r /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !mkdir /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !cp -r /content/FinTabParse/inputs/request_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !cp -r /content/FinTabParse/inputs/tables_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !cp -r /content/FinTabParse/inputs/tables_tmp /content/FinTabParse/models/HybridR/WikiTables-WithLinks/

  return has_links

"""## run code"""

!cp /content/FinTabParse/tests/mainTEST2/* /content/FinTabParse/inputs

"""### input & preprocessing"""

from utils.file_handling import read_inputs
from preprocessing.AlBERT_preprop import  prepare_AlQA_data
from preprocessing.TagOp_preprop import prepare_tagop_data
import copy

AlQA = ""
questions, paragraphs, table = read_inputs()
answers = {}

# questions = list(questions)
question_preds = question_classifier_model(questions, '/content/FinTabParse/weights/questions_classifier')
for que in question_preds:
  answers[que['id']] = {
      'question': que['question'],
      'id': que['id'],
      'class': que['class'],
      'answers': []
  }

dataclass = "para" if len(table)==0 else "tab_para"

from preprocessing.chunking import get_table_chunks, get_paragraphs_chunks
tab_chunks = par_chunks = []
AlQA = ""
if dataclass == "para":
  AlQA = prepare_AlQA_data(paragraphs)
  para_chunks = get_paragraphs_chunks(AlQA, 256)
else:
  tab_chunks = get_table_chunks(table, 512)



"""### processing"""

from preprocessing.AlBERT_preprop import  process_alqa_answers
from preprocessing.TagOp_preprop import process_tagop_answers
from preprocessing.HybridR_preprop import process_hybridr_answers

AlQA_pred = []
if dataclass == "para":
  for i, par_chunk in enumerate(para_chunks):
    AlQA_pred = paragraph_qa_model(questions, par_chunk)
    answers = process_alqa_answers(AlQA_pred, answers, i)
    tag_op_input = prepare_tagop_data(questions, par_chunk, table, question_preds)
    !cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/tag_op python tag_op/prepare_dataset.py --mode dev
    !cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd) python tag_op/predictor.py --data_dir tag_op/cache/ --test_data_dir tag_op/cache/ --save_dir tag_op/ --eval_batch_size 32 --model_path ./checkpoint --encoder roberta
    answers = process_tagop_answers(answers, i)
else:
  for i, tab_chunk in enumerate(tab_chunks):
    has_links = prepare_hybridR_data(questions, paragraphs, tab_chunk, question_preds)
    if(has_links):
      !cd /content/FinTabParse/models/HybridR && CUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/2020_10_03_22_47_34/checkpoint-epoch2 --stage2_model stage2/2020_10_03_22_50_31/checkpoint-epoch2/ --do_lower_case --predict_file preprocessed_test/test_inputs.json --do_eval --option stage12 --model_name_or_path  bert-large-uncased
      !cd /content/FinTabParse/models/HybridR && CUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/2020_10_03_22_51_12/checkpoint-epoch3/ --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8
      answers = process_hybridr_answers(answers, i)
      !rm /content/FinTabParse/models/HybridR/preprocessed_test/*
    tag_op_input = prepare_tagop_data(questions, paragraphs, tab_chunk, question_preds)
    !cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/tag_op python tag_op/prepare_dataset.py --mode dev
    !cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd) python tag_op/predictor.py --data_dir tag_op/cache/ --test_data_dir tag_op/cache/ --save_dir tag_op/ --eval_batch_size 32 --model_path ./checkpoint --encoder roberta
    answers = process_tagop_answers(answers, i)
    # if(i==3): break

answers

answers

AlQ

"""## Output"""

!rm -r ./outputs

from utils.file_handling import write_output

os.makedirs('outputs', exist_ok=True)
write_output(tag_op_input, AlQA_pred, dataclass)
AlQA_pred

AlQA_pred

!zip -r outputs.zip ./outputs/

"""# TEST

## chunking
"""

!cp /content/FinTabParse/tests/mainTEST1/input3/* /content/FinTabParse/inputs

from nltk.tokenize import word_tokenize

def perepare_chunk(table, row_chunk_no):
  def add_header(chunk):
    chunk.insert(0, table[0])
    return chunk
  
  chunk_vals = range(row_chunk_no[-1]+1)
  chunks = [[row for r, row in enumerate(table[1:]) if row_chunk_no[r]==x
             ] for x in chunk_vals]
  return list(map(add_header, chunks))

def get_table_chunks(table, chunk_size=512):
  header = table[0]
  header_tokens = len(word_tokenize(' '.join(table[0])))
  row_tokens = list(map(lambda row: len(word_tokenize(' '.join(row))), table[1:]))
  cum_row_tokens = [sum(row_tokens[0:x:1])//(chunk_size-header_tokens) 
    for x in range(0, len(row_tokens)+1)]
  row_chunk_no = cum_row_tokens[1:]
  # chunk_vals = range(row_chunk_no[-1]+1)
  # chunks = [[row for r, row in enumerate(table) if row_chunk_no[r]==x] for x in chunk_vals]
             
  return perepare_chunk(table, row_chunk_no)

def get_paragraphs_chunks(paragraph, chunk_size=512, overlap=16):
  tokenized = word_tokenize(paragraph)
  return [' '.join(tokenized[i:i+chunk_size]) for i in range(0, len(tokenized), chunk_size-overlap)]

def question_classifier_model(ques, model_path):
  tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
  questions = list(map(lambda x: x['question'], ques))
  question_classifier = BertClassifier()
  question_classifier.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))

  question_infs = list(map(lambda question_input: tokenizer(question_input, padding='max_length', max_length = 512, truncation=True, return_tensors="pt"), questions))
  question_preds = ques.copy()

  for i, que_inf in enumerate(question_infs):
    question_id = que_inf['input_ids']
    question_mask = que_inf['attention_mask'].squeeze(1)
    pred = question_classifier(question_id, question_mask)
    question_preds[i]['class'] = pred.argmax(dim=1).numpy()[0]

  return question_preds

import os
def prepare_hybridR_data(questions, paragraphs, table, question_preds):
  hybridR_questions = list(filter(lambda x: x['class']==0, question_preds))
  hybirdR_paragraphs, hybirdR_ques, hybirdR_table = hybridr_preprocessing(table, paragraphs, hybridR_questions)

  os.makedirs('models/HybridR/test_inputs/', exist_ok=True)
  dict_to_json(hybirdR_ques, 'models/HybridR/test_inputs/test.json')

  os.makedirs('inputs/request_tok', exist_ok=True)
  dict_to_json(hybirdR_paragraphs, 'inputs/request_tok/table_0.json')

  os.makedirs('inputs/tables_tok', exist_ok=True)
  dict_to_json(hybirdR_table, 'inputs/tables_tok/table_0.json')

  del hybirdR_ques
  del hybirdR_paragraphs
  del hybirdR_table

  preprocessing_main()

  !rm -r /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !mkdir /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !cp -r /content/FinTabParse/inputs/request_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !cp -r /content/FinTabParse/inputs/tables_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/
  !cp -r /content/FinTabParse/inputs/tables_tmp /content/FinTabParse/models/HybridR/WikiTables-WithLinks/

!rm /content/FinTabParse/models/HybridR/preprocessed_test/*

from utils.file_handling import read_inputs
from preprocessing import  prepare_AlQA_data, prepare_tagop_data

questions, paragraphs, table = read_inputs()
questions = list(questions)
# question_preds = question_classifier_model(questions, '/content/FinTabParse/weights/questions_classifier')
# prepare_hybridR_data(questions, paragraphs, table, question_preds)
# tag_op_input = prepare_tagop_data(questions, paragraphs, table, question_preds)
AlQA_data = prepare_AlQA_data(paragraphs)

def paragraph_qa_model(questions, text):
  AlTokenizer = AlbertTokenizer.from_pretrained("twmkn9/albert-base-v2-squad2")
  AlQA = AlbertForQuestionAnswering.from_pretrained("twmkn9/albert-base-v2-squad2")
  answers = questions.copy()

  for i,que in enumerate(questions):
    inputs = AlTokenizer(que['question'], text, return_tensors="pt")
    with torch.no_grad():
        outputs = AlQA(**inputs, return_dict=True)

    answer_start_index = outputs.start_logits.argmax()
    answer_end_index = outputs.end_logits.argmax()
    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
    answers[i]['answers'].append({
        'answer': AlTokenizer.decode(predict_answer_tokens),
        'score': outputs.start_logits.max()*outputs.end_logits.max()
        })

  return answers

# para_chunks = get_paragraphs_chunks(AlQA_data, 256)
paragraph_qa_model(questions, para_chunks[0])

questions

"""## tapas"""

!pip install tensorflow==2.8.0

!pip install -U transformers

import json
import shutil
import os

def json_to_dict(json_file):
  with open(json_file, 'r') as f:
    data = json.load(f)
  return data

def file_to_list(file_name):
  """
  Reads a file and returns a list of lines
  """
  with open(file_name, 'r') as f:
    data = f.readlines()
  return list(map(lambda x: x.replace('\n', ''), data))

def dict_to_json(tag_op_input, json_file):
    with open(json_file, 'w') as f:
        json.dump(tag_op_input, f)

def list_to_file(list_data, file_name):
  """
  Writes a list to a file
  """
  with open(file_name, 'w') as f:
    for line in list_data:
      if(type(line)==type(" ")):
        f.write(line + '\n')

def get_inputs(questions_path, paragraphs_path, table_path):
	questions = file_to_list(questions_path)
	paragraphs = file_to_list(paragraphs_path)
	table = json_to_dict(table_path)
	return questions, paragraphs, table

def read_inputs():
  inputs_path = './'
  paragraphs_path = os.path.join(inputs_path, 'paragraphs.txt')
  questions_path = os.path.join(inputs_path, 'questions.txt')
  table_path = os.path.join(inputs_path, 'table.json')

  questions, paragraphs, table = get_inputs(questions_path, paragraphs_path, table_path)
  
  return questions, paragraphs, table

questions, paragraphs, table = read_inputs()

import pandas as pd
TaPaSQA = pd.DataFrame.from_records(table[1:], columns=table[0])

from transformers import TapasTokenizer, TapasForQuestionAnswering

def generate_outputs(table, queries):
  # table = pd.DataFrame.from_dict(table_ip)
  tokenizer = TapasTokenizer.from_pretrained("google/tapas-base-finetuned-wtq")
  model = TapasForQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")
  inputs = tokenizer(table=table, queries=queries, padding="max_length", return_tensors="tf", truncation=True)
  outputs = model(**inputs)
  predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(
      inputs, outputs.logits, outputs.logits_aggregation
  )
  return (predicted_answer_coordinates, predicted_aggregation_indices)

generate_outputs(TaPaSQA, questions)

import tensorflow as tf
tf.__version__

!pip install tensorflow==2.8.0