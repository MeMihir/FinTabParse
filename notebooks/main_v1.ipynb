{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ir4WVRpLpV3Y",
        "UGKVh-Vin0EG",
        "yCiEkXjbnyQ6",
        "VN35Uv0ghrxH",
        "ePDtUBOenrEJ",
        "Odgmww0UntjI",
        "vQWzA65pWmBd",
        "yOF7UU7Nj-Ak",
        "zAhFE7Qoj6P-",
        "_DV6UQLJkI0j",
        "WgD9QKtt1F95"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Ir4WVRpLpV3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## github and weights"
      ],
      "metadata": {
        "id": "UGKVh-Vin0EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -r /content/FinTabParse\n",
        "!git clone https://github.com/MeMihir/FinTabParse.git\n",
        "!cd FinTabParse/ && git checkout trans2-test"
      ],
      "metadata": {
        "id": "SP2K2T0wlxI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://files.pythonhosted.org/packages/e9/45/6c28dccc896c5e4f2b219054b6c661228861699698c4aa8737efbc928915/transformers-2.6.0.tar.gz"
      ],
      "metadata": {
        "id": "A-ilxK95G0hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd FinTabParse/utils/ && pip install -e ./transformers-2.6.0/"
      ],
      "metadata": {
        "id": "OZs1Zj6CIAJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle kernels output mihirpavuskar/tat-qa-train -p /content/FinTabParse/models/"
      ],
      "metadata": {
        "id": "NQypW9-E4GNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/FinTabParse/models/ && git clone https://github.com/wenhuchen/HybridQA.git\n",
        "# !cd /content/FinTabParse/models/HybridQA && wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip && unzip models.zip\n",
        "!cd /content/FinTabParse/models/HybridR && wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip && unzip models.zip"
      ],
      "metadata": {
        "id": "qAPYraZ4mMr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/Capstone/Table_Parsing_QA/models/model_f-1_e-3.pt /content/FinTabParse/weights/questions_classifier"
      ],
      "metadata": {
        "id": "3MAMEJWXpYdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installs"
      ],
      "metadata": {
        "id": "yCiEkXjbnyQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install tqdm\n",
        "!pip install fuzzywuzzy\n",
        "!pip install dateparser"
      ],
      "metadata": {
        "id": "TQIOl8S51BbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./FinTabParse/ && pip install -r requirement.txt\n",
        "!pip install https://data.pyg.org/whl/torch-1.7.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBurVzwh6cG",
        "outputId": "95bd2fbf-0dfb-4464-fdcc-334720a7661d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.96\n",
            "    Uninstalling sentencepiece-0.1.96:\n",
            "      Successfully uninstalled sentencepiece-0.1.96\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.6.0\n",
            "    Uninstalling filelock-3.6.0:\n",
            "      Successfully uninstalled filelock-3.6.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.6.0\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.9 which is incompatible.\n",
            "dateparser 1.1.1 requires regex!=2019.02.19,!=2021.8.27,<2022.3.15, but you have regex 2022.4.24 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed allennlp-0.8.4 awscli-1.23.5 blis-0.2.4 colorama-0.4.4 conllu-0.11 dataclasses-0.6 datasets-1.1.0 docutils-0.15.2 filelock-3.0.12 flaky-3.7.0 flask-cors-3.0.10 ftfy-6.1.1 gast-0.3.3 gevent-21.12.0 h5py-2.10.0 jsonnet-0.18.0 jsonpickle-2.1.0 nltk-3.5 numpy-1.19.5 numpydoc-1.3.1 overrides-6.1.0 pandas-1.1.5 parsimonious-0.9.0 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 regex-2022.4.24 responses-0.20.0 rsa-4.7.2 scikit-learn-0.23.2 sentencepiece-0.1.91 spacy-2.1.9 sphinx-4.5.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0 thinc-7.0.8 tokenizers-0.9.3 torch-1.7.0 tqdm-4.28.1 transformers-3.5.0 typing-utils-0.1.0 unidecode-1.3.4 word2number-1.1 xxhash-3.0.0 zope.event-4.5.0 zope.interface-5.4.0\n",
            "Collecting torch-scatter==2.0.7\n",
            "  Downloading https://data.pyg.org/whl/torch-1.7.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHWJxXWjO__0",
        "outputId": "a2321755-6e3b-4649-8f5c-b5e1f151237a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "VN35Uv0ghrxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/FinTabParse"
      ],
      "metadata": {
        "id": "-DTvy4RAhs7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cead90ec-69fa-4069-b643-446c66694eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FinTabParse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main.py"
      ],
      "metadata": {
        "id": "ePDtUBOenrEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  BertTokenizer\n",
        "import torch\n",
        "\n",
        "from utils.file_handling import file_to_list, json_to_dict, dict_to_json\n",
        "from models.questions_classifier import BertClassifier\n",
        "from preprocessing import tagop_preprocessing, hybridr_preprocessing\n",
        "\n",
        "def get_inputs(questions_path, paragraphs_path, table_path):\n",
        "\tquestions = file_to_list(questions_path)\n",
        "\tparagraphs = file_to_list(paragraphs_path)\n",
        "\ttable = json_to_dict(table_path)\n",
        "\treturn questions, paragraphs, table\n",
        "\n",
        "\n",
        "def question_classifier_model(questions, model_path):\n",
        "\ttokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        " \t\n",
        "\tquestion_classifier = BertClassifier()\n",
        "\tquestion_classifier.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        " \n",
        "\tquestion_infs = list(map(lambda question_input: tokenizer(question_input, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"), questions))\n",
        "\tquestion_preds = []\n",
        "\n",
        "\tfor que_inf in question_infs:\n",
        "\t\tquestion_id = que_inf['input_ids']\n",
        "\t\tquestion_mask = que_inf['attention_mask'].squeeze(1)\n",
        "\t\tpred = question_classifier(question_id, question_mask)\n",
        "\t\tquestion_preds.append(pred.argmax(dim=1).numpy()[0])\n",
        "\t\n",
        "\treturn question_preds"
      ],
      "metadata": {
        "id": "tNTTlMsijib3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run code"
      ],
      "metadata": {
        "id": "Odgmww0UntjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/FinTabParse/tests/test3/* /content/FinTabParse/inputs"
      ],
      "metadata": {
        "id": "9KOlg-nFo6n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input and classification"
      ],
      "metadata": {
        "id": "vQWzA65pWmBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "inputs_path = '/content/FinTabParse/inputs/'\n",
        "paragraphs_path = os.path.join(inputs_path, 'paragraphs.txt')\n",
        "questions_path = os.path.join(inputs_path, 'questions.txt')\n",
        "table_path = os.path.join(inputs_path, 'table.json')\n",
        "\n",
        "questions, paragraphs, table = get_inputs(questions_path, paragraphs_path, table_path)"
      ],
      "metadata": {
        "id": "_HPHEMutjmIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_preds = question_classifier_model(questions, '/content/FinTabParse/weights/questions_classifier')"
      ],
      "metadata": {
        "id": "TOQq1k3Okia3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_preds"
      ],
      "metadata": {
        "id": "S5mvuDnfXFPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59666b27-846f-40f1-9f43-6f94e7fe041f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 0, 0, 1, 0, 1, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hybridR"
      ],
      "metadata": {
        "id": "yOF7UU7Nj-Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hybridR_questions = list(map(lambda x: x[1], filter(lambda x: x[0]==0, zip(question_preds, questions))))\n",
        "hybirdR_paragraphs, hybirdR_ques, hybirdR_table = hybridr_preprocessing(table, paragraphs, hybridR_questions)\n",
        "\n",
        "os.makedirs('models/HybridR/test_inputs/', exist_ok=True)\n",
        "dict_to_json(hybirdR_ques, 'models/HybridR/test_inputs/test.json')\n",
        "\n",
        "os.makedirs('inputs/request_tok', exist_ok=True)\n",
        "dict_to_json(hybirdR_paragraphs, 'inputs/request_tok/table_0.json')\n",
        "\n",
        "os.makedirs('inputs/tables_tok', exist_ok=True)\n",
        "dict_to_json(hybirdR_table, 'inputs/tables_tok/table_0.json')\n",
        "\n",
        "del hybirdR_ques\n",
        "del hybirdR_paragraphs\n",
        "del hybirdR_table"
      ],
      "metadata": {
        "id": "dlSNh4E4z81E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.HybridR.preprocessing import preprocessing_main\n",
        "preprocessing_main()"
      ],
      "metadata": {
        "id": "t2emtz0oxbVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d59485f-9bd0-40a7-acdf-b818cfcc57ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'question_id': 'que_0', 'question': 'Which student got the lowest marks in FAT?', 'table_id': 'table_0', 'question_postag': 'JJ NN VBD DT JJS NNS IN NNP .'}, {'question_id': 'que_1', 'question': 'Which student has the highest total marks?', 'table_id': 'table_0', 'question_postag': 'JJ NN VBZ DT JJS JJ NNS .'}, {'question_id': 'que_2', 'question': 'How many students got higher than 50 in FAT?', 'table_id': 'table_0', 'question_postag': 'WRB JJ NNS VBD JJR IN CD IN NNP .'}, {'question_id': 'que_3', 'question': 'Which student has the highest marks in Quiz1 and Quiz2 combined?', 'table_id': 'table_0', 'question_postag': 'JJ NN VBZ DT JJS NNS IN NNP CC NNP VBD .'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "!mkdir /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "!cp -r /content/FinTabParse/inputs/request_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "!cp -r /content/FinTabParse/inputs/tables_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "!cp -r /content/FinTabParse/inputs/tables_tmp /content/FinTabParse/models/HybridR/WikiTables-WithLinks/"
      ],
      "metadata": {
        "id": "Zci8-vb8VzAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/FinTabParse/models/HybridR && CUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/2020_10_03_22_47_34/checkpoint-epoch2 --stage2_model stage2/2020_10_03_22_50_31/checkpoint-epoch2/ --do_lower_case --predict_file preprocessed_test/test_inputs.json --do_eval --option stage12 --model_name_or_path  bert-large-uncased"
      ],
      "metadata": {
        "id": "vZ_slnjMbTjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "402962c3-78a0-448d-b97a-4a8cf435aabe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/03/2022 06:59:16 - INFO - transformers2.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at /tmp/6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
            "05/03/2022 06:59:16 - INFO - transformers2.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/03/2022 06:59:16 - INFO - transformers2.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /tmp/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "05/03/2022 06:59:16 - INFO - filelock -   Lock 139870137762512 acquired on /tmp/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6.lock\n",
            "05/03/2022 06:59:16 - INFO - transformers2.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpfnmaao5l\n",
            "Downloading: 100% 1.34G/1.34G [00:29<00:00, 45.1MB/s]\n",
            "05/03/2022 06:59:46 - INFO - transformers2.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin in cache at /tmp/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "05/03/2022 06:59:46 - INFO - transformers2.file_utils -   creating metadata file for /tmp/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "05/03/2022 06:59:46 - INFO - filelock -   Lock 139870137762512 released on /tmp/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6.lock\n",
            "05/03/2022 06:59:46 - INFO - transformers2.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /tmp/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "05/03/2022 07:00:04 - INFO - transformers2.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin from cache at /tmp/54da47087cc86ce75324e4dc9bbb5f66c6e83a7c6bd23baea8b489acc8d09aa4.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
            "05/03/2022 07:00:15 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='/tmp/', config_name='', device=device(type='cuda'), dim=1024, do_eval=True, do_lower_case=True, do_train=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, model_name_or_path='bert-large-uncased', model_type='bert', n_gpu=1, num_train_epochs=3.0, option='stage12', output_dir=None, predict_file='preprocessed_test/test_inputs.json', resource_dir='WikiTables-WithLinks/', save_steps=5000, seed=42, stage1_model='stage1/2020_10_03_22_47_34/checkpoint-epoch2', stage2_model='stage2/2020_10_03_22_50_31/checkpoint-epoch2/', tokenizer_name='', train_file=None, warmup_steps=0, weight_decay=0.0)\n",
            "Evaluation: 0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/FinTabParse/models/HybridR && CUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/2020_10_03_22_51_12/checkpoint-epoch3/ --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8"
      ],
      "metadata": {
        "id": "x6eO3m_TbTjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6732b59-d07a-441a-86df-5ac3fb82df56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/03/2022 07:00:38 - INFO - transformers2.configuration_utils -   loading configuration file stage3/2020_10_03_22_51_12/checkpoint-epoch3/config.json\n",
            "05/03/2022 07:00:38 - INFO - transformers2.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/03/2022 07:00:38 - INFO - transformers2.tokenization_utils -   Model name 'stage3/2020_10_03_22_51_12/checkpoint-epoch3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'stage3/2020_10_03_22_51_12/checkpoint-epoch3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/03/2022 07:00:38 - INFO - transformers2.tokenization_utils -   Didn't find file stage3/2020_10_03_22_51_12/checkpoint-epoch3/added_tokens.json. We won't load it.\n",
            "05/03/2022 07:00:38 - INFO - transformers2.tokenization_utils -   loading file stage3/2020_10_03_22_51_12/checkpoint-epoch3/vocab.txt\n",
            "05/03/2022 07:00:38 - INFO - transformers2.tokenization_utils -   loading file None\n",
            "05/03/2022 07:00:38 - INFO - transformers2.tokenization_utils -   loading file stage3/2020_10_03_22_51_12/checkpoint-epoch3/special_tokens_map.json\n",
            "05/03/2022 07:00:38 - INFO - transformers2.tokenization_utils -   loading file stage3/2020_10_03_22_51_12/checkpoint-epoch3/tokenizer_config.json\n",
            "05/03/2022 07:00:38 - INFO - transformers2.modeling_utils -   loading weights file stage3/2020_10_03_22_51_12/checkpoint-epoch3/pytorch_model.bin\n",
            "05/03/2022 07:01:03 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='/tmp/', config_name='', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_stage3=True, do_train=False, doc_stride=128, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, model_name_or_path='stage3/2020_10_03_22_51_12/checkpoint-epoch3/', model_type='bert', n_best_size=20, n_gpu=1, null_score_diff_threshold=0.0, num_train_epochs=3.0, output_dir='stage3/2022_05_03_07_00_38', overwrite_cache=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=12, predict_file='predictions.intermediate.json', resource_dir='WikiTables-WithLinks/', save_steps=500, seed=42, threads=8, tokenizer_name='', train_file=None, verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "05/03/2022 07:01:03 - INFO - __main__ -   Loading checkpoint stage3/2020_10_03_22_51_12/checkpoint-epoch3/ for evaluation\n",
            "05/03/2022 07:01:03 - INFO - transformers2.configuration_utils -   loading configuration file stage3/2020_10_03_22_51_12/checkpoint-epoch3/config.json\n",
            "05/03/2022 07:01:03 - INFO - transformers2.configuration_utils -   Model config BertConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "05/03/2022 07:01:03 - INFO - transformers2.modeling_utils -   loading weights file stage3/2020_10_03_22_51_12/checkpoint-epoch3/pytorch_model.bin\n",
            "0it [00:00, ?it/s]\n",
            "convert squad examples to features: 0it [00:00, ?it/s]\n",
            "add example index and unique id: 0it [00:00, ?it/s]\n",
            "05/03/2022 07:01:18 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/03/2022 07:01:18 - INFO - __main__ -     Num examples = 0\n",
            "05/03/2022 07:01:18 - INFO - __main__ -     Batch size = 8\n",
            "Evaluating: 0it [00:00, ?it/s]\n",
            "05/03/2022 07:01:18 - INFO - transformers2.data.metrics.squad_metrics -   Writing predictions to: /tmp/predictions_.json\n",
            "05/03/2022 07:01:18 - INFO - transformers2.data.metrics.squad_metrics -   Writing nbest to: /tmp/nbest_predictions_.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tagOP"
      ],
      "metadata": {
        "id": "zAhFE7Qoj6P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagop_questions = list(map(lambda x: x[1], filter(lambda x: x[0]==1, zip(question_preds, questions))))\n",
        "tag_op_input = tagop_preprocessing(table, paragraphs, tagop_questions)\n",
        "dict_to_json([tag_op_input], '/content/FinTabParse/models/TAT-QA/dataset_tagop/tatqa_dataset_dev.json')"
      ],
      "metadata": {
        "id": "zXq1DXw7czgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/tag_op python tag_op/prepare_dataset.py --mode dev"
      ],
      "metadata": {
        "id": "84OjFW_Io3cU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579709c7-59d2-46e5-ade9-a4943946de32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.95it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 5, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd) python tag_op/predictor.py --data_dir tag_op/cache/ --test_data_dir tag_op/cache/ --save_dir tag_op/ --eval_batch_size 32 --model_path ./checkpoint --encoder roberta"
      ],
      "metadata": {
        "id": "z30OHXYD0WNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33396590-ce13-4a25-dbd8-1b3fadddab92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 5.\n",
            "05/03/2022 07:01:59 Below are the result on Dev set...\n",
            "100% 1/1 [00:01<00:00,  1.32s/it]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             5\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.6\n",
            "\n",
            "global op:0.2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "_DV6UQLJkI0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./outputs"
      ],
      "metadata": {
        "id": "pEcg9m8wm7Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('outputs', exist_ok=True)\n",
        "import shutil"
      ],
      "metadata": {
        "id": "U3u8bgvZpzz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagop_pred = json_to_dict('models/TAT-QA/tag_op/pred_result_on_dev.json')\n",
        "shutil.move('models/TAT-QA/tag_op/pred_result_on_dev.json', 'outputs/tag_op_pred.json')\n",
        "\n",
        "tagop_answers = []\n",
        "for que in tag_op_input['questions']:\n",
        "  tagop_answers.append({\n",
        "      'id': que['uid'],\n",
        "      'question': que['question'],\n",
        "      'answer': f\"{tagop_pred[que['uid']][0]} {tagop_pred[que['uid']][1]}\"\n",
        "  })\n",
        "dict_to_json(tagop_answers, 'outputs/tagop_answers.json')"
      ],
      "metadata": {
        "id": "pyKvVqFWkKTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move('models/HybridR/predictions.intermediate.json', 'outputs/')\n",
        "shutil.move('models/HybridR/predictions.json', 'outputs/hybridR_predictions.json')\n",
        "hybridR_pred = json_to_dict('outputs/hybridR_predictions.json')"
      ],
      "metadata": {
        "id": "bNTRkMCrpHTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybridR_answers = []\n",
        "for que in hybridR_pred:\n",
        "  hybridR_answers.append({\n",
        "      \"question\": que[\"question\"],\n",
        "      'id': que['question_id'],\n",
        "      'answer': que['pred']\n",
        "  })\n",
        "dict_to_json(hybridR_answers, 'outputs/hybridR_answers.json')"
      ],
      "metadata": {
        "id": "cGxRf2jexv1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r outputs.zip ./outputs/"
      ],
      "metadata": {
        "id": "nHrDYP67XJmK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665e8b92-0aa1-42c4-f1f1-fb63e93b8f79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: outputs/ (stored 0%)\n",
            "  adding: outputs/hybridR_answers.json (stored 0%)\n",
            "  adding: outputs/predictions.intermediate.json (stored 0%)\n",
            "  adding: outputs/hybridR_predictions.json (stored 0%)\n",
            "  adding: outputs/tagop_answers.json (deflated 54%)\n",
            "  adding: outputs/tag_op_pred.json (deflated 38%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "WgD9QKtt1F95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  BertTokenizer\n",
        "import torch\n",
        "\n",
        "from utils.file_handling import file_to_list, json_to_dict\n",
        "from models.questions_classifier import BertClassifier\n",
        "from preprocessing import tagop_preprocessing\n",
        "\n",
        "def get_inputs(questions_path, paragraphs_path, table_path):\n",
        "\tques = file_to_list(questions_path)\n",
        "\tparagraphs = file_to_list(paragraphs_path)\n",
        "\ttable = json_to_dict(table_path)\n",
        "\tquestions = []\n",
        "\tfor i, que in enumerate(ques):\n",
        "\t\tquestions.append({\n",
        "\t\t\t\t\"id\": f\"que_{i}\",\n",
        "\t\t\t\t\"question\": que\n",
        "\t\t})\n",
        "\n",
        "\treturn questions, paragraphs, table\n",
        "\n",
        "\n",
        "def question_classifier_model(questions, model_path):\n",
        "\ttokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\tques = map(lambda x: x[\"question\"], questions)\n",
        " \t\n",
        "\tquestion_classifier = BertClassifier()\n",
        "\tquestion_classifier.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        " \n",
        "\tquestion_infs = list(map(lambda question_input: tokenizer(question_input, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"), ques))\n",
        "\tpreds = []\n",
        "\n",
        "\tfor que_inf in question_infs:\n",
        "\t\tquestion_id = que_inf['input_ids']\n",
        "\t\tquestion_mask = que_inf['attention_mask'].squeeze(1)\n",
        "\t\tpred = question_classifier(question_id, question_mask)\n",
        "\t\tpreds.append(pred.argmax(dim=1).numpy()[0])\n",
        "\t\n",
        "\tfor i,pred in enumerate(preds):\n",
        "\t\tquestions[i][\"category\"] = pred\n",
        "\n",
        "\treturn questions"
      ],
      "metadata": {
        "id": "F0X9f0WG1HAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/FinTabParse/tests/test3/* /content/FinTabParse/inputs\n"
      ],
      "metadata": {
        "id": "ZVuCn_jF4nHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "inputs_path = '/content/FinTabParse/inputs/'\n",
        "paragraphs_path = os.path.join(inputs_path, 'paragraphs.txt')\n",
        "questions_path = os.path.join(inputs_path, 'questions.txt')\n",
        "table_path = os.path.join(inputs_path, 'table.json')\n",
        "\n",
        "questions, paragraphs, table = get_inputs(questions_path, paragraphs_path, table_path)"
      ],
      "metadata": {
        "id": "HA6Waj7s4vpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_preds = question_classifier_model(questions.copy(), '/content/FinTabParse/weights/questions_classifier')"
      ],
      "metadata": {
        "id": "IZPjMcRF46pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3pXEBHX4760",
        "outputId": "941f9152-8de8-4952-9c16-78cb98402837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'category': 1,\n",
              "  'id': 'que_0',\n",
              "  'question': 'what is the average payment volume per transaction for american express?'},\n",
              " {'category': 1,\n",
              "  'id': 'que_1',\n",
              "  'question': 'what is the average payment volume per transaction for JCB?'},\n",
              " {'category': 1,\n",
              "  'id': 'que_2',\n",
              "  'question': 'what is the difference between total volume and payment volume for american express?'},\n",
              " {'category': 1,\n",
              "  'id': 'que_3',\n",
              "  'question': 'what is the difference between the total volume for JCB and total volume for mastercard?'},\n",
              " {'category': 1,\n",
              "  'id': 'que_4',\n",
              "  'question': 'what is exclueded from the mastercard figures?'},\n",
              " {'category': 1,\n",
              "  'id': 'que_5',\n",
              "  'question': 'which companies are the largest operators of open-loop and closed-loop retail electronic payments networks?'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}