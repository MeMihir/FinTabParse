{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Ir4WVRpLpV3Y",
        "UGKVh-Vin0EG",
        "yCiEkXjbnyQ6",
        "ePDtUBOenrEJ",
        "Odgmww0UntjI",
        "vQWzA65pWmBd",
        "zAhFE7Qoj6P-",
        "_DV6UQLJkI0j"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ec09062737fe478b98aec4be1a00d401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e288f086542f4735a3c29a2c598cff78",
              "IPY_MODEL_6fdc87e7ee2a4838a1876ba36567f754"
            ],
            "layout": "IPY_MODEL_5cfe31b89ece4ffdb66940b85a5261c6"
          }
        },
        "e288f086542f4735a3c29a2c598cff78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f73e8a368f143d98c687851339d1e9c",
            "max": 262028,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7bc1b1b774464be898449337f7d99aef",
            "value": 262028
          }
        },
        "6fdc87e7ee2a4838a1876ba36567f754": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2181ce48d8a4430a82390e626c932401",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a2a0d732f97e47c198167fd8bdc88a74",
            "value": "100% 256k/256k [00:00&lt;00:00, 768kB/s]"
          }
        },
        "5cfe31b89ece4ffdb66940b85a5261c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f73e8a368f143d98c687851339d1e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bc1b1b774464be898449337f7d99aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "2181ce48d8a4430a82390e626c932401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a0d732f97e47c198167fd8bdc88a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "071693931eca4902aa567212258411fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cc8ef1936ff48a78e6749e78a01762c",
              "IPY_MODEL_fb4cc0d332e141a083484b2808dc5a59"
            ],
            "layout": "IPY_MODEL_a790c2a924c445609682119fd177c097"
          }
        },
        "2cc8ef1936ff48a78e6749e78a01762c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ca6e054c9048529bfcb5706aa8f1a4",
            "max": 154,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24b71fecfd0c4a0fa2852e5038514570",
            "value": 154
          }
        },
        "fb4cc0d332e141a083484b2808dc5a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_467cbbb9a6424b67b2362fb558d805e7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d5c20de762304386ab2463a422e867c2",
            "value": "100% 154/154 [00:00&lt;00:00, 3.69kB/s]"
          }
        },
        "a790c2a924c445609682119fd177c097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ca6e054c9048529bfcb5706aa8f1a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b71fecfd0c4a0fa2852e5038514570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "467cbbb9a6424b67b2362fb558d805e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c20de762304386ab2463a422e867c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9889a9023814540ac86cd3ff966c0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a56c8069d94c4b5697032cd398b373ed",
              "IPY_MODEL_0cb521113b8242c69761ee7a501900b2"
            ],
            "layout": "IPY_MODEL_9e44028568ea48239dfd2c8402e374fd"
          }
        },
        "a56c8069d94c4b5697032cd398b373ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading",
            "description_tooltip": null,
            "layout": "IPY_MODEL_394dfbe6f7a741fb8fc06a25a9ca8e07",
            "max": 490,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95e3d712c68045e098bb503c80ef6efa",
            "value": 490
          }
        },
        "0cb521113b8242c69761ee7a501900b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a6c443a7696423287b833ac8e167bc4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_22411c42b7644282b872bec146574da9",
            "value": "100% 490/490 [00:00&lt;00:00, 17.7kB/s]"
          }
        },
        "9e44028568ea48239dfd2c8402e374fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394dfbe6f7a741fb8fc06a25a9ca8e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95e3d712c68045e098bb503c80ef6efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "7a6c443a7696423287b833ac8e167bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22411c42b7644282b872bec146574da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21fac092925e4f68a86775d0291844d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24d54678f66e4534b3397bccf7b39f44",
              "IPY_MODEL_a82dd56a2d5448ddb20fa891dda15de7"
            ],
            "layout": "IPY_MODEL_7b71ca3c9efe4a29a181c8725bbb6dbf"
          }
        },
        "24d54678f66e4534b3397bccf7b39f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1afb5188892b4def8bbbf131f620ff51",
            "max": 1657,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c81f56c76c94ec088f757b46ba8c4b0",
            "value": 1657
          }
        },
        "a82dd56a2d5448ddb20fa891dda15de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32ed1ef0af8c439697c02ab16c5017a1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9e416604e8774d6c8be15df33844710c",
            "value": "100% 1.62k/1.62k [00:00&lt;00:00, 32.1kB/s]"
          }
        },
        "7b71ca3c9efe4a29a181c8725bbb6dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1afb5188892b4def8bbbf131f620ff51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c81f56c76c94ec088f757b46ba8c4b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "32ed1ef0af8c439697c02ab16c5017a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e416604e8774d6c8be15df33844710c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4dfc1c5366d4635b6ccecd8f8990201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82ccd4ceac6a4b59ae0d7783d53a755c",
              "IPY_MODEL_a6c14b2d92214ddfaa8e8f8d52684c94"
            ],
            "layout": "IPY_MODEL_0e1a739006fe4ce1b0372f139f9072b6"
          }
        },
        "82ccd4ceac6a4b59ae0d7783d53a755c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98d7ccbc27a9443cab5f99ea4a2b222e",
            "max": 443010576,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b5ae23f73c2473588a5087737ecb3d5",
            "value": 443010576
          }
        },
        "a6c14b2d92214ddfaa8e8f8d52684c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc410c20e6b413a898b5bf4bee5aea0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7ef48b4820aa495cb735d24212504fe5",
            "value": "100% 422M/422M [00:09&lt;00:00, 46.6MB/s]"
          }
        },
        "0e1a739006fe4ce1b0372f139f9072b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d7ccbc27a9443cab5f99ea4a2b222e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5ae23f73c2473588a5087737ecb3d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "5fc410c20e6b413a898b5bf4bee5aea0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ef48b4820aa495cb735d24212504fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Ir4WVRpLpV3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## github and weights"
      ],
      "metadata": {
        "id": "UGKVh-Vin0EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!rm -r /content/FinTabParse\n",
        "!git clone https://github.com/MeMihir/FinTabParse.git\n",
        "!cd FinTabParse/ && git checkout chunking"
      ],
      "metadata": {
        "id": "SP2K2T0wlxI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b2ac37-678e-4e2f-bbb8-48b5464008c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "rm: cannot remove '/content/FinTabParse': No such file or directory\n",
            "Cloning into 'FinTabParse'...\n",
            "remote: Enumerating objects: 2732, done.\u001b[K\n",
            "remote: Counting objects: 100% (2732/2732), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2579/2579), done.\u001b[K\n",
            "remote: Total 2732 (delta 143), reused 2701 (delta 114), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2732/2732), 19.63 MiB | 18.19 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n",
            "Branch 'chunking' set up to track remote branch 'chunking' from 'origin'.\n",
            "Switched to a new branch 'chunking'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd FinTabParse/utils/ && pip install -e ./transformers-2.6.0/"
      ],
      "metadata": {
        "id": "OZs1Zj6CIAJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "225f5668-7910-4e18-a633-70d77c633659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/FinTabParse/utils/transformers-2.6.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (1.21.6)\n",
            "Collecting tokenizers==0.5.2\n",
            "  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.6 MB 14.7 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.23.6-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132 kB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.6.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 880 kB 56.6 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79 kB 9.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.27.0,>=1.26.6\n",
            "  Downloading botocore-1.26.6-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.8 MB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.6->boto3->transformers==2.6.0) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 138 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.6->boto3->transformers==2.6.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (2022.5.18.1)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 127 kB 18.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.6.0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.6.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fbefa9aba81e8ab51096d28b0051a8f792a21515e7b4f0c01044ebd4cce4b55f\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, tokenizers, sentencepiece, sacremoses, boto3, transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Running setup.py develop for transformers\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.23.6 botocore-1.26.6 jmespath-1.0.0 s3transfer-0.5.2 sacremoses-0.0.53 sentencepiece-0.1.96 tokenizers-0.5.2 transformers-2.6.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle kernels output mihirpavuskar/tat-qa-train -p /content/FinTabParse/models/\n",
        "!mv /content/FinTabParse/models/TagOp/tag_op/prepare_dataset.py /content/FinTabParse/models/TAT-QA/tag_op"
      ],
      "metadata": {
        "id": "NQypW9-E4GNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846e0378-2346-4664-9e25-3f86cba6f3f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/HEAD\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/config\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/description\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/applypatch-msg.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/commit-msg.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/fsmonitor-watchman.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/post-update.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/pre-applypatch.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/pre-commit.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/pre-merge-commit.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/pre-push.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/pre-rebase.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/pre-receive.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/prepare-commit-msg.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/hooks/update.sample\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/index\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/info/exclude\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/logs/HEAD\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/logs/refs/heads/master\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/logs/refs/remotes/origin/HEAD\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/objects/pack/pack-4c0d07a2e38c2e873bf86fa2af856d00144947f3.idx\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/objects/pack/pack-4c0d07a2e38c2e873bf86fa2af856d00144947f3.pack\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/packed-refs\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/refs/heads/master\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.git/refs/remotes/origin/HEAD\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/.gitignore\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/LICENSE\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/README.md\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/__pycache__/tatqa_eval.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/__pycache__/tatqa_metric.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/__pycache__/tatqa_utils.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/checkpoint/args.json\n",
            "tcmalloc: large alloc 2885328896 bytes == 0xd23a6000 @  0x7f8538ef31e7 0x4a3940 0x5b438c 0x5b46f7 0x59afff 0x515655 0x59a257 0x570bf0 0x511ee1 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x511e2c 0x549576 0x4bca8a\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/checkpoint/checkpoint_best.ot\n",
            "tcmalloc: large alloc 1446887424 bytes == 0x17e3c2000 @  0x7f8538ef31e7 0x4a3940 0x5b438c 0x5b46f7 0x59afff 0x515655 0x59a257 0x570bf0 0x511ee1 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x511e2c 0x549576 0x4bca8a\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/checkpoint/checkpoint_best.pt\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/checkpoint/train.log\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_raw/tatqa_dataset_dev.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_raw/tatqa_dataset_test.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_raw/tatqa_dataset_train.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_tagop/roberta.large/config.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_tagop/roberta.large/merges.txt\n",
            "tcmalloc: large alloc 1425948672 bytes == 0xd0e26000 @  0x7f8538ef31e7 0x4a3940 0x5b438c 0x5b46f7 0x59afff 0x515655 0x59a257 0x570bf0 0x511ee1 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x59c019 0x595ef6 0x5134a6 0x549576 0x4bca8a 0x5134a6 0x549576 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x511e2c 0x549576 0x4bca8a\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_tagop/roberta.large/pytorch_model.bin\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_tagop/roberta.large/vocab.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_tagop/tatqa_dataset_dev.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/dataset_tagop/tatqa_dataset_train.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/assets/next.jpeg\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/assets/tatqa-sample.png\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/Lobster-Regular.ttf\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/OFL.txt\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/glyphicons-halflings-regular.eot\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/glyphicons-halflings-regular.svg\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/glyphicons-halflings-regular.ttf\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/glyphicons-halflings-regular.woff\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/fonts/glyphicons-halflings-regular.woff2\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/index.html\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/scripts/bootstrap.min.js\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/scripts/jquery-3.2.1.min.js\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/scripts/jquery.easing.min.js\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/scripts/scrolling-nav.js\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/scripts/show-json.js\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/styles/bootstrap.min.css\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/styles/common.css\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/docs/styles/show-json.css\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/requirement.txt\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/sample_prediction.json\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/__pycache__/__init__.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/__pycache__/options.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/cache/tagop_roberta_cached_train.pkl\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/__pycache__/__init__.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/__pycache__/data_util.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/__pycache__/file_utils.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/__pycache__/tatqa_batch_gen.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/__pycache__/tatqa_dataset.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/data_util.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/file_utils.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/tatqa_batch_gen.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/tatqa_dataset.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/__pycache__/__init__.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/__pycache__/logging.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/dummy_flax_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/dummy_pt_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/dummy_sentencepiece_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/dummy_tf_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/dummy_tokenizers_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/hp_naming.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/logging.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/notebook.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/data/utils/sentencepiece_model_pb2.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/options.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/predictor.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/prepare_dataset.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/__pycache__/__init__.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/__pycache__/model.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/__pycache__/modeling_tagop.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/__pycache__/optimizer.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/__pycache__/util.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/model.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/modeling_tagop.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/optimizer.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/tools/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/tools/__pycache__/__init__.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/tools/__pycache__/allennlp.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/tools/__pycache__/util.cpython-37.pyc\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/tools/allennlp.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/tools/util.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/util.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/__init__.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/dummy_flax_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/dummy_pt_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/dummy_sentencepiece_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/dummy_tf_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/dummy_tokenizers_objects.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/hp_naming.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/logging.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/notebook.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/tagop/utils/sentencepiece_model_pb2.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tag_op/trainer.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tatqa_eval.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tatqa_metric.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tatqa_metric_test.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tatqa_utils.py\n",
            "Output file downloaded to /content/FinTabParse/models/TAT-QA/tatqa_utils_test.py\n",
            "Kernel log downloaded to /content/FinTabParse/models/tat-qa-train.log \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd /content/FinTabParse/models/ && git clone https://github.com/wenhuchen/HybridQA.git\n",
        "# !cd /content/FinTabParse/models/HybridQA && wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip && unzip models.zip\n",
        "!cd /content/FinTabParse/models/HybridR && wget https://hybridqa.s3-us-west-2.amazonaws.com/models.zip && unzip models.zip"
      ],
      "metadata": {
        "id": "qAPYraZ4mMr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "573ee737-4d4a-47d9-cb84-8f5c255d4e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-24 15:01:15--  https://hybridqa.s3-us-west-2.amazonaws.com/models.zip\n",
            "Resolving hybridqa.s3-us-west-2.amazonaws.com (hybridqa.s3-us-west-2.amazonaws.com)... 52.218.250.17\n",
            "Connecting to hybridqa.s3-us-west-2.amazonaws.com (hybridqa.s3-us-west-2.amazonaws.com)|52.218.250.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6170000875 (5.7G) [application/zip]\n",
            "Saving to: â€˜models.zipâ€™\n",
            "\n",
            "models.zip          100%[===================>]   5.75G  1.25MB/s    in 6m 44s  \n",
            "\n",
            "2022-05-24 15:08:00 (14.6 MB/s) - â€˜models.zipâ€™ saved [6170000875/6170000875]\n",
            "\n",
            "Archive:  models.zip\n",
            "   creating: stage1/2020_10_03_22_47_34/checkpoint-epoch2/\n",
            "  inflating: stage1/2020_10_03_22_47_34/checkpoint-epoch2/config.json  \n",
            "  inflating: stage1/2020_10_03_22_47_34/checkpoint-epoch2/pytorch_model.bin  \n",
            "  inflating: stage1/2020_10_03_22_47_34/checkpoint-epoch2/training_args.bin  \n",
            "  inflating: stage1/2020_10_03_22_47_34/checkpoint-epoch2/vocab.txt  \n",
            "  inflating: stage1/2020_10_03_22_47_34/checkpoint-epoch2/special_tokens_map.json  \n",
            " extracting: stage1/2020_10_03_22_47_34/checkpoint-epoch2/tokenizer_config.json  \n",
            "   creating: stage2/2020_10_03_22_50_31/checkpoint-epoch2/\n",
            "  inflating: stage2/2020_10_03_22_50_31/checkpoint-epoch2/config.json  \n",
            "  inflating: stage2/2020_10_03_22_50_31/checkpoint-epoch2/pytorch_model.bin  \n",
            "  inflating: stage2/2020_10_03_22_50_31/checkpoint-epoch2/training_args.bin  \n",
            "  inflating: stage2/2020_10_03_22_50_31/checkpoint-epoch2/vocab.txt  \n",
            "  inflating: stage2/2020_10_03_22_50_31/checkpoint-epoch2/special_tokens_map.json  \n",
            " extracting: stage2/2020_10_03_22_50_31/checkpoint-epoch2/tokenizer_config.json  \n",
            "   creating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/\n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/config.json  \n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/scheduler.pt  \n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/pytorch_model.bin  \n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/training_args.bin  \n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/vocab.txt  \n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/special_tokens_map.json  \n",
            " extracting: stage3/2020_10_03_22_51_12/checkpoint-epoch3/tokenizer_config.json  \n",
            "  inflating: stage3/2020_10_03_22_51_12/checkpoint-epoch3/optimizer.pt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/Capstone/Table_Parsing_QA/models/model_f-1_e-3.pt /content/FinTabParse/weights/questions_classifier"
      ],
      "metadata": {
        "id": "3MAMEJWXpYdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installs"
      ],
      "metadata": {
        "id": "yCiEkXjbnyQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install tqdm\n",
        "!pip install fuzzywuzzy\n",
        "!pip install dateparser"
      ],
      "metadata": {
        "id": "TQIOl8S51BbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8b1b8a-6e78-440a-9100-bc7adc3a6762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125 kB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.1.1-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 288 kB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser) (2022.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27,<2022.3.15 in /usr/local/lib/python3.7/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./FinTabParse/ && pip install -r requirement.txt\n",
        "!pip install https://data.pyg.org/whl/torch-1.7.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DBurVzwh6cG",
        "outputId": "6bc0190e-c33a-4944-c881-d15bc648db1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 776.7 MB 4.6 kB/s \n",
            "\u001b[?25hCollecting transformers==3.5.0\n",
            "  Downloading transformers-3.5.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 53.4 MB/s \n",
            "\u001b[?25hCollecting allennlp==0.8.4\n",
            "  Downloading allennlp-0.8.4-py3-none-any.whl (5.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.7 MB 22.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.2.0\n",
            "  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 516.2 MB 4.8 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.28.1\n",
            "  Downloading tqdm-4.28.1-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.8 MB 54.1 MB/s \n",
            "\u001b[?25hCollecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4 MB 28.7 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.8 MB 60.9 MB/s \n",
            "\u001b[?25hCollecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.5 MB 56.3 MB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting datasets==1.1.0\n",
            "  Downloading datasets-1.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 147 kB 72.4 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirement.txt (line 1)) (4.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r requirement.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0->-r requirement.txt (line 2)) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0->-r requirement.txt (line 2)) (0.0.53)\n",
            "Collecting tokenizers==0.9.3\n",
            "  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0->-r requirement.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0->-r requirement.txt (line 2)) (3.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.5.0->-r requirement.txt (line 2)) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading jsonpickle-2.2.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading responses-0.20.0-py3-none-any.whl (27 kB)\n",
            "Collecting parsimonious>=0.8.0\n",
            "  Downloading parsimonious-0.9.0.tar.gz (48 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (2.5)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (1.4.1)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.18.0.tar.gz (592 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 592 kB 62.0 MB/s \n",
            "\u001b[?25hCollecting awscli>=1.11.91\n",
            "  Downloading awscli-1.24.6-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.9 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (3.1.0)\n",
            "Collecting spacy<2.2,>=2.0.18\n",
            "  Downloading spacy-2.1.9-cp37-cp37m-manylinux1_x86_64.whl (30.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8 MB 107 kB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading overrides-6.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (1.23.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (2022.1)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading numpydoc-1.3.1-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 600 kB/s \n",
            "\u001b[?25hCollecting flaky\n",
            "  Downloading flaky-3.7.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting conllu==0.11\n",
            "  Downloading conllu-0.11-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting gevent>=1.3.6\n",
            "  Downloading gevent-21.12.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.8 MB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (0.5.3)\n",
            "Collecting flask-cors>=3.0.7\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (0.4.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==0.8.4->-r requirement.txt (line 3)) (3.6.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.0.0)\n",
            "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 454 kB 22.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (0.37.1)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.46.1)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0->-r requirement.txt (line 4)) (1.14.1)\n",
            "Collecting tensorboard<2.3.0,>=2.2.0\n",
            "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0 MB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirement.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->-r requirement.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.5->-r requirement.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirement.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.1.0->-r requirement.txt (line 11)) (6.0.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.1.0->-r requirement.txt (line 11)) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212 kB 76.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.1.0->-r requirement.txt (line 11)) (0.3.5.1)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from awscli>=1.11.91->allennlp==0.8.4->-r requirement.txt (line 3)) (0.5.2)\n",
            "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli>=1.11.91->allennlp==0.8.4->-r requirement.txt (line 3)) (3.13)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: botocore==1.26.6 in /usr/local/lib/python3.7/dist-packages (from awscli>=1.11.91->allennlp==0.8.4->-r requirement.txt (line 3)) (1.26.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore==1.26.6->awscli>=1.11.91->allennlp==0.8.4->-r requirement.txt (line 3)) (1.25.11)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.26.6->awscli>=1.11.91->allennlp==0.8.4->-r requirement.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.8.4->-r requirement.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.8.4->-r requirement.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask>=1.0.2->allennlp==0.8.4->-r requirement.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp==0.8.4->-r requirement.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent>=1.3.6->allennlp==0.8.4->-r requirement.txt (line 3)) (1.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251 kB 75.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask>=1.0.2->allennlp==0.8.4->-r requirement.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4->-r requirement.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4->-r requirement.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->allennlp==0.8.4->-r requirement.txt (line 3)) (1.4.2)\n",
            "Collecting sphinx>=3.0\n",
            "  Downloading Sphinx-4.5.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1 MB 62.4 MB/s \n",
            "\u001b[?25hCollecting regex!=2019.12.17\n",
            "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 749 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0->-r requirement.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0->-r requirement.txt (line 2)) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.5.0->-r requirement.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli>=1.11.91->allennlp==0.8.4->-r requirement.txt (line 3)) (0.4.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4->-r requirement.txt (line 3)) (1.0.5)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.2 MB 57.6 MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82 kB 555 kB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4->-r requirement.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4->-r requirement.txt (line 3)) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<2.2,>=2.0.18->allennlp==0.8.4->-r requirement.txt (line 3)) (0.9.1)\n",
            "Collecting thinc<7.1.0,>=7.0.8\n",
            "  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.1 MB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (4.11.3)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (2.10.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (1.3.0)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 121 kB 79.5 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (0.7.12)\n",
            "Collecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (2.2.0)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100 kB 13.9 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (1.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx>=3.0->numpydoc>=0.8.0->allennlp==0.8.4->-r requirement.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0->-r requirement.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp==0.8.4->-r requirement.txt (line 3)) (0.2.5)\n",
            "Collecting typing-utils>=0.0.3\n",
            "  Downloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4->-r requirement.txt (line 3)) (8.13.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4->-r requirement.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4->-r requirement.txt (line 3)) (21.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4->-r requirement.txt (line 3)) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==0.8.4->-r requirement.txt (line 3)) (0.7.1)\n",
            "Building wheels for collected packages: nltk, jsonnet, parsimonious, word2number\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434692 sha256=f597eea5ea6a29fe93a23e10f768df9ca910c53c2d8bfad0e08f137e2824d94d\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.18.0-cp37-cp37m-linux_x86_64.whl size=3994698 sha256=a74c4e229fffebdcf54482178fb59a44354abdd397e67e90505574fa99bf1453\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/63/f9/a653f9c21575e6ff271ee6a49939aa002005174cea6c35919d\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.9.0-py3-none-any.whl size=44314 sha256=aea9ff9622ffda5ef0867d2892752bdce720ff3f27a4362cd8818ec172f19847\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/54/88/c1ee7de0eabd1fb817cbf35824e4c2cba664d5816ddc64efb1\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=8b7a46a7fe90171edfa5ddb50f58d734374e40198dd00d898cd32d452ab6bee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built nltk jsonnet parsimonious word2number\n",
            "Installing collected packages: rsa, numpy, tqdm, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, preshed, plac, docutils, dataclasses, blis, zope.interface, zope.event, typing-utils, torch, thinc, sphinx, regex, colorama, xxhash, word2number, unidecode, tokenizers, tensorflow-estimator, tensorboard, spacy, sentencepiece, scikit-learn, responses, pytorch-pretrained-bert, parsimonious, pandas, overrides, numpydoc, nltk, jsonpickle, jsonnet, h5py, gevent, gast, ftfy, flask-cors, flaky, filelock, conllu, awscli, transformers, tensorflow, datasets, allennlp\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.8\n",
            "    Uninstalling rsa-4.8:\n",
            "      Successfully uninstalled rsa-4.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.6\n",
            "    Uninstalling preshed-3.0.6:\n",
            "      Successfully uninstalled preshed-3.0.6\n",
            "  Attempting uninstall: plac\n",
            "    Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.5.2\n",
            "    Uninstalling tokenizers-0.5.2:\n",
            "      Successfully uninstalled tokenizers-0.5.2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.96\n",
            "    Uninstalling sentencepiece-0.1.96:\n",
            "      Successfully uninstalled sentencepiece-0.1.96\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.7.0\n",
            "    Uninstalling filelock-3.7.0:\n",
            "      Successfully uninstalled filelock-3.7.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 2.6.0\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0+zzzcolab20220506162203\n",
            "    Uninstalling tensorflow-2.8.0+zzzcolab20220506162203:\n",
            "      Successfully uninstalled tensorflow-2.8.0+zzzcolab20220506162203\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.28.1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.9 which is incompatible.\n",
            "dateparser 1.1.1 requires regex!=2019.02.19,!=2021.8.27,<2022.3.15, but you have regex 2022.4.24 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed allennlp-0.8.4 awscli-1.24.6 blis-0.2.4 colorama-0.4.4 conllu-0.11 dataclasses-0.6 datasets-1.1.0 docutils-0.16 filelock-3.0.12 flaky-3.7.0 flask-cors-3.0.10 ftfy-6.1.1 gast-0.3.3 gevent-21.12.0 h5py-2.10.0 jsonnet-0.18.0 jsonpickle-2.2.0 nltk-3.5 numpy-1.19.5 numpydoc-1.3.1 overrides-6.1.0 pandas-1.1.5 parsimonious-0.9.0 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 regex-2022.4.24 responses-0.20.0 rsa-4.7.2 scikit-learn-0.23.2 sentencepiece-0.1.91 spacy-2.1.9 sphinx-4.5.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0 thinc-7.0.8 tokenizers-0.9.3 torch-1.7.0 tqdm-4.28.1 transformers-3.5.0 typing-utils-0.1.0 unidecode-1.3.4 word2number-1.1 xxhash-3.0.0 zope.event-4.5.0 zope.interface-5.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-scatter==2.0.7\n",
            "  Downloading https://data.pyg.org/whl/torch-1.7.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.7 MB 14.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "id": "xa9ZTm5xE6LD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b52aa1-58fe-4790-aa7d-23c9705bb540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[K     / 668.3 MB 4.0 MB/s\n",
            "\u001b[?25hCollecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.8 MB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.46.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (2.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.0.0)\n",
            "Collecting numpy>=1.20\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.7 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (0.26.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (14.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (4.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.0) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.3.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (1.25.11)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.0) (3.2.0)\n",
            "Installing collected packages: numpy, tf-estimator-nightly, tensorboard, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.28.1 which is incompatible.\n",
            "en-core-web-sm 2.2.5 requires spacy>=2.2.2, but you have spacy 2.1.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.6 tensorboard-2.8.0 tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHWJxXWjO__0",
        "outputId": "c960a6ef-f74b-41a0-faf7-b626fbf337f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers===4.17.0"
      ],
      "metadata": {
        "id": "tSu768NU_YV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b89ee1-40fb-4514-b479-f80ca1950e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers===4.17.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8 MB 14.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (0.0.53)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 60.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (2022.4.24)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (21.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers===4.17.0) (4.28.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers===4.17.0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers===4.17.0) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers===4.17.0) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers===4.17.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers===4.17.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers===4.17.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers===4.17.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers===4.17.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers===4.17.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers===4.17.0) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.9.3\n",
            "    Uninstalling tokenizers-0.9.3:\n",
            "      Successfully uninstalled tokenizers-0.9.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 3.5.0\n",
            "    Uninstalling transformers-3.5.0:\n",
            "      Successfully uninstalled transformers-3.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "awscli 1.24.6 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0 which is incompatible.\u001b[0m\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "VN35Uv0ghrxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/FinTabParse\n",
        "# !rm -r /content/FinTabParse/inputs/*"
      ],
      "metadata": {
        "id": "-DTvy4RAhs7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae5eff7-7ee0-483c-8812-382f9fbe6666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FinTabParse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main.py"
      ],
      "metadata": {
        "id": "ePDtUBOenrEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from transformers import  BertTokenizer, AlbertTokenizer, AlbertForQuestionAnswering, TapasTokenizer, TFTapasForQuestionAnswering\n",
        "from models.questions_classifier import BertClassifier\n",
        "\n",
        "from utils.file_handling import file_to_list, json_to_dict, dict_to_json\n",
        "from preprocessing.TagOp_preprop import tagop_preprocessing\n",
        "from preprocessing.HybridR_preprop import hybridr_preprocessing\n",
        "\n",
        "def paragraph_qa_model(questions, text):\n",
        "  AlTokenizer = AlbertTokenizer.from_pretrained(\"twmkn9/albert-base-v2-squad2\")\n",
        "  AlQA = AlbertForQuestionAnswering.from_pretrained(\"twmkn9/albert-base-v2-squad2\")\n",
        "  answers = copy.deepcopy(questions)\n",
        "\n",
        "  for i,que in enumerate(questions):\n",
        "    inputs = AlTokenizer(que['question'], text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = AlQA(**inputs, return_dict=True)\n",
        "\n",
        "    answer_start_index = outputs.start_logits.argmax()\n",
        "    answer_end_index = outputs.end_logits.argmax()\n",
        "    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "    answers[i]['answers'].append({\n",
        "        'answer': AlTokenizer.decode(predict_answer_tokens),\n",
        "        'score': outputs.start_logits.max()*outputs.end_logits.max()\n",
        "        })\n",
        "\n",
        "  return answers\n",
        "\n",
        "\n",
        "def question_classifier_model(ques, model_path):\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "  questions = list(map(lambda x: x['question'], ques))\n",
        "  question_classifier = BertClassifier()\n",
        "  question_classifier.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "  question_infs = list(map(lambda question_input: tokenizer(question_input, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\"), questions))\n",
        "  question_preds = ques.copy()\n",
        "\n",
        "  for i, que_inf in enumerate(question_infs):\n",
        "    question_id = que_inf['input_ids']\n",
        "    question_mask = que_inf['attention_mask'].squeeze(1)\n",
        "    pred = question_classifier(question_id, question_mask)\n",
        "    question_preds[i]['class'] = pred.argmax(dim=1).numpy()[0]\n",
        "\n",
        "  return question_preds\n",
        "\n",
        "def table_qa_model(table, queries):\n",
        "  # table = pd.DataFrame.from_dict(table_ip)\n",
        "  tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
        "  model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
        "  inputs = tokenizer(table=table, queries=queries, padding=\"max_length\", return_tensors=\"tf\", truncation=True)\n",
        "  outputs = model(**inputs)\n",
        "  scores = np.amax(outputs.logits.numpy(), axis=1)\n",
        "  predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n",
        "      inputs, outputs.logits, outputs.logits_aggregation\n",
        "  )\n",
        "  return predicted_answer_coordinates, predicted_aggregation_indices, scores\n"
      ],
      "metadata": {
        "id": "tNTTlMsijib3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from models.HybridR.preprocessing import preprocessing_main\n",
        "\n",
        "def prepare_hybridR_data(questions, paragraphs, table, question_preds):\n",
        "  hybridR_questions = list(filter(lambda x: x['class']==0, question_preds))\n",
        "  hybirdR_paragraphs, hybirdR_ques, hybirdR_table, has_links = hybridr_preprocessing(table, paragraphs, hybridR_questions)\n",
        "  print(has_links, hybirdR_table)\n",
        "\n",
        "  if(has_links==False): return has_links\n",
        "\n",
        "  os.makedirs('models/HybridR/test_inputs/', exist_ok=True)\n",
        "  dict_to_json(hybirdR_ques, 'models/HybridR/test_inputs/test.json')\n",
        "\n",
        "  os.makedirs('inputs/request_tok', exist_ok=True)\n",
        "  dict_to_json(hybirdR_paragraphs, 'inputs/request_tok/table_0.json')\n",
        "\n",
        "  os.makedirs('inputs/tables_tok', exist_ok=True)\n",
        "  dict_to_json(hybirdR_table, 'inputs/tables_tok/table_0.json')\n",
        "\n",
        "  del hybirdR_ques\n",
        "  del hybirdR_paragraphs\n",
        "  del hybirdR_table\n",
        "\n",
        "  preprocessing_main()\n",
        "\n",
        "  !rm -r /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "  !mkdir /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "  !cp -r /content/FinTabParse/inputs/request_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "  !cp -r /content/FinTabParse/inputs/tables_tok /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "  !cp -r /content/FinTabParse/inputs/tables_tmp /content/FinTabParse/models/HybridR/WikiTables-WithLinks/\n",
        "\n",
        "  return has_links"
      ],
      "metadata": {
        "id": "jhAegJHOCzHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc172277-69d5-4a7d-a61f-604678754d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run code"
      ],
      "metadata": {
        "id": "Odgmww0UntjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp /content/FinTabParse/tests/mainTEST1/input1/* /content/FinTabParse/inputs"
      ],
      "metadata": {
        "id": "9KOlg-nFo6n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2aa2546-5bcd-4d02-c98f-461fa72eb338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: -r not specified; omitting directory '/content/FinTabParse/tests/mainTEST1/input1/outputs1'\n",
            "cp: -r not specified; omitting directory '/content/FinTabParse/tests/mainTEST1/input1/outputs2'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input & preprocessing"
      ],
      "metadata": {
        "id": "vQWzA65pWmBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.file_handling import read_inputs\n",
        "\n",
        "from preprocessing.TagOp_preprop import prepare_tagop_data\n",
        "\n",
        "import copy\n",
        "\n",
        "AlQA = \"\"\n",
        "questions, paragraphs, table = read_inputs()\n",
        "answers = {}\n",
        "\n",
        "# questions = list(questions)\n",
        "question_preds = question_classifier_model(questions, '/content/FinTabParse/weights/questions_classifier')\n",
        "for que in question_preds:\n",
        "  answers[que['id']] = {\n",
        "      'question': que['question'],\n",
        "      'id': que['id'],\n",
        "      'class': int(que['class']),\n",
        "      'answers': []\n",
        "  }\n",
        "\n",
        "dataclass = \"para\" if len(table)==0 else \"tab\" if len(paragraphs)==0 else \"tab_para\""
      ],
      "metadata": {
        "id": "_HPHEMutjmIH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f812a6-d516-4d53-d810-565c53c8756a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.chunking import get_table_chunks, get_paragraphs_chunks\n",
        "from preprocessing.AlBERT_preprop import  prepare_AlQA_data\n",
        "from preprocessing.tapas_preprop import prepare_tapas_data\n",
        "\n",
        "tab_chunks = para_chunks = []\n",
        "AlQA = \"\"\n",
        "if dataclass == \"para\":\n",
        "  AlQA = prepare_AlQA_data(paragraphs)\n",
        "  para_chunks = get_paragraphs_chunks(AlQA, 256)\n",
        "  print(f\"PARAGRAPH ONLY\\n{len(para_chunks)} chunks\")\n",
        "else:\n",
        "  tab_chunks = get_table_chunks(table, 512)\n",
        "  print(f\"TABLE AND/OR PARAGRAPH\\n{len(tab_chunks)} chunks\")"
      ],
      "metadata": {
        "id": "xfQOM5l7GM8o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75e3f8f-ddd3-4cfe-d264-53207390abbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TABLE AND/OR PARAGRAPH\n",
            "125 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tab_chunks = tab_chunks[:5]"
      ],
      "metadata": {
        "id": "TNga3MWTlkZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### processing"
      ],
      "metadata": {
        "id": "zAhFE7Qoj6P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.AlBERT_preprop import  process_alqa_answers\n",
        "from preprocessing.TagOp_preprop import process_tagop_answers\n",
        "from preprocessing.HybridR_preprop import process_hybridr_answers\n",
        "from preprocessing.tapas_preprop import process_tapas_answers\n",
        "\n",
        "\n",
        "def process_AlBERT(questions, paragraph, answers, i):\n",
        "  AlQA_pred = paragraph_qa_model(questions, paragraph)\n",
        "  answers = process_alqa_answers(AlQA_pred, answers, i)\n",
        "  return answers\n",
        "\n",
        "def process_TagOp(questions, paragraph, table, question_preds, answers, i):\n",
        "  tag_op_input = prepare_tagop_data(questions, paragraph, table, question_preds)\n",
        "  !cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/tag_op python tag_op/prepare_dataset.py --mode dev\n",
        "  !cd /content/FinTabParse/models/TAT-QA && PYTHONPATH=$PYTHONPATH:$(pwd) python tag_op/predictor.py --data_dir tag_op/cache/ --test_data_dir tag_op/cache/ --save_dir tag_op/ --eval_batch_size 32 --model_path ./checkpoint --encoder roberta\n",
        "  answers = process_tagop_answers(answers, i)\n",
        "  return answers\n",
        "\n",
        "def process_tapas(questions, table, answers, i):\n",
        "  tapas_tab, tapas_ques = prepare_tapas_data(table, questions)\n",
        "  predicted_answer_coordinates, predicted_aggregation_indices, scores = table_qa_model(tapas_tab, tapas_ques)\n",
        "  answers = process_tapas_answers(predicted_answer_coordinates, predicted_aggregation_indices, tapas_tab, answers, i, scores)\n",
        "  return answers\n",
        "\n",
        "def process_HybridR(answers, i):\n",
        "  !cd /content/FinTabParse/models/HybridR && CUDA_VISIBLE_DEVICES=0 python train_stage12.py --stage1_model stage1/2020_10_03_22_47_34/checkpoint-epoch2 --stage2_model stage2/2020_10_03_22_50_31/checkpoint-epoch2/ --do_lower_case --predict_file preprocessed_test/test_inputs.json --do_eval --option stage12 --model_name_or_path  bert-large-uncased\n",
        "  !cd /content/FinTabParse/models/HybridR && CUDA_VISIBLE_DEVICES=0 python train_stage3.py --model_name_or_path stage3/2020_10_03_22_51_12/checkpoint-epoch3/ --do_stage3   --do_lower_case  --predict_file predictions.intermediate.json --per_gpu_train_batch_size 12  --max_seq_length 384   --doc_stride 128 --threads 8\n",
        "  answers = process_hybridr_answers(answers, i)\n",
        "  !rm /content/FinTabParse/models/HybridR/preprocessed_test/*\n",
        "  return answers"
      ],
      "metadata": {
        "id": "n1sEwEQ7iDot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AlQA_pred = []\n",
        "if dataclass == \"para\":\n",
        "  for i, par_chunk in enumerate(para_chunks):\n",
        "    answers = process_AlBERT(questions, par_chunk, answers, i)\n",
        "    answers = process_TagOp(questions, par_chunk, table, question_preds, answers, i)\n",
        "elif dataclass == \"tab\":\n",
        "  for i, tab_chunk in enumerate(tab_chunks):\n",
        "    answers = process_tapas(questions, tab_chunk, answers, i)\n",
        "    answers = process_TagOp(questions, paragraphs, tab_chunk, question_preds, answers, i)\n",
        "else:\n",
        "  for i, tab_chunk in enumerate(tab_chunks):\n",
        "    has_links = prepare_hybridR_data(questions, paragraphs, tab_chunk, question_preds)\n",
        "    if(has_links):\n",
        "      process_HybridR(answers, i)\n",
        "    else:\n",
        "      answers = process_tapas(questions, tab_chunk, answers, i)\n",
        "    answers = process_TagOp(questions, paragraphs, tab_chunk, question_preds, answers, i)"
      ],
      "metadata": {
        "id": "vZ_slnjMbTjP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ec09062737fe478b98aec4be1a00d401",
            "e288f086542f4735a3c29a2c598cff78",
            "6fdc87e7ee2a4838a1876ba36567f754",
            "5cfe31b89ece4ffdb66940b85a5261c6",
            "3f73e8a368f143d98c687851339d1e9c",
            "7bc1b1b774464be898449337f7d99aef",
            "2181ce48d8a4430a82390e626c932401",
            "a2a0d732f97e47c198167fd8bdc88a74",
            "071693931eca4902aa567212258411fc",
            "2cc8ef1936ff48a78e6749e78a01762c",
            "fb4cc0d332e141a083484b2808dc5a59",
            "a790c2a924c445609682119fd177c097",
            "24ca6e054c9048529bfcb5706aa8f1a4",
            "24b71fecfd0c4a0fa2852e5038514570",
            "467cbbb9a6424b67b2362fb558d805e7",
            "d5c20de762304386ab2463a422e867c2",
            "e9889a9023814540ac86cd3ff966c0c4",
            "a56c8069d94c4b5697032cd398b373ed",
            "0cb521113b8242c69761ee7a501900b2",
            "9e44028568ea48239dfd2c8402e374fd",
            "394dfbe6f7a741fb8fc06a25a9ca8e07",
            "95e3d712c68045e098bb503c80ef6efa",
            "7a6c443a7696423287b833ac8e167bc4",
            "22411c42b7644282b872bec146574da9",
            "21fac092925e4f68a86775d0291844d4",
            "24d54678f66e4534b3397bccf7b39f44",
            "a82dd56a2d5448ddb20fa891dda15de7",
            "7b71ca3c9efe4a29a181c8725bbb6dbf",
            "1afb5188892b4def8bbbf131f620ff51",
            "9c81f56c76c94ec088f757b46ba8c4b0",
            "32ed1ef0af8c439697c02ab16c5017a1",
            "9e416604e8774d6c8be15df33844710c",
            "a4dfc1c5366d4635b6ccecd8f8990201",
            "82ccd4ceac6a4b59ae0d7783d53a755c",
            "a6c14b2d92214ddfaa8e8f8d52684c94",
            "0e1a739006fe4ce1b0372f139f9072b6",
            "98d7ccbc27a9443cab5f99ea4a2b222e",
            "5b5ae23f73c2473588a5087737ecb3d5",
            "5fc410c20e6b413a898b5bf4bee5aea0",
            "7ef48b4820aa495cb735d24212504fe5"
          ]
        },
        "outputId": "e185adc7-e3dc-4da1-f7ed-8b008343d810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=262028, style=ProgressStyle(description_widâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec09062737fe478b98aec4be1a00d401"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=154, style=ProgressStyle(description_width=â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "071693931eca4902aa567212258411fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=490, style=ProgressStyle(description_width=â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9889a9023814540ac86cd3ff966c0c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1657, style=ProgressStyle(description_widthâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21fac092925e4f68a86775d0291844d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=443010576, style=ProgressStyle(description_â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4dfc1c5366d4635b6ccecd8f8990201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_37']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.39it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:20:56 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.14it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_75']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:21:37 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.07it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_113']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.27it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:22:13 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.15it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_151']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.34it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:22:40 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_189']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.71it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:23:06 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_227']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.53it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:23:33 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_265']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.77it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:23:59 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_303']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.55it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:24:24 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_341']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.51it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:24:49 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_379']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.30it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:25:14 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_417']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.82it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:25:38 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_455']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.13it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:26:03 Below are the result on Dev set...\n",
            "  0% 0/1 [00:00<?, ?it/s]/content/FinTabParse/models/TAT-QA/tag_op/tagop/modeling_tagop.py:671: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  answer = np.around(operand_one / operand_two, 4)\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_493']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.97it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:26:28 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_531']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.94it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:26:52 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_569']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  8.30it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:27:16 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_607']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.36it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:27:41 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_645']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.68it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:28:05 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.5714285714285714\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_683']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.07it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:28:29 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_721']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.82it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:28:53 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_759']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.07it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:29:18 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_797']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.21it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:29:43 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_835']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.69it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:30:08 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_873']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.00it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:30:32 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_911']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.32it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:30:57 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_949']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.16it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:31:21 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_987']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.98it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:31:46 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.5714285714285714\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1025']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.21it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:32:10 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1063']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.10it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:32:35 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1101']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.54it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:32:59 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1139']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.70it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:33:24 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1177']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.39it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:33:48 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.14285714285714285\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1215']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.81it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:34:14 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1253']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.24it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:34:38 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1291']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.54it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:35:03 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.14285714285714285\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1329']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.71it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:35:28 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1367']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.20it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:35:53 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1405']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.38it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:36:18 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1443']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.68it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:36:43 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1481']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.46it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:37:08 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1519']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.30it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:37:33 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1557']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.00it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:37:58 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.20it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1595']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.04it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:38:24 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1633']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.52it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:38:49 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1671']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.40it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:39:14 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1709']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.35it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:39:39 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1747']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.14it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:40:03 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1785']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.08it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:40:28 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1823']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.40it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:40:52 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.5714285714285714\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1861']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.32it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:41:17 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.14285714285714285\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1899']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.45it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:41:42 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1937']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.45it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:42:06 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_1975']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.32it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:42:30 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2013']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.74it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:42:55 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.27it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2051']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:43:20 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2089']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.80it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:43:44 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2127']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.94it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:44:08 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2165']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.05it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:44:33 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2203']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.29it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:44:57 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2241']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.26it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:45:22 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2279']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.08it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:45:46 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2317']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.27it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:46:11 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.27it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2355']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.07it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:46:35 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2393']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.87it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:46:59 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2431']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.68it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:47:23 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2469']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.63it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:47:48 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.14285714285714285\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2507']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.06it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:48:12 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2545']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.61it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:48:37 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2583']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.25it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:49:01 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2621']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.96it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:49:26 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.14285714285714285\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2659']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.33it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:49:50 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.14285714285714285\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2697']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.13it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:50:14 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2735']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.09it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:50:39 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2773']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.13it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:51:03 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2811']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.96it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:51:28 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2849']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.25it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:51:52 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2887']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:52:16 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2925']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.72it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:52:41 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_2963']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.35it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:53:05 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3001']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.78it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:53:30 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3039']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.27it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:53:54 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.5714285714285714\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3077']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:54:19 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3115']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.02it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:54:44 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3153']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.00it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:55:09 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3191']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:55:34 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3229']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.22it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:55:59 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.27it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3267']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.21it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:56:23 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3305']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.35it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:56:48 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3343']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.19it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:57:12 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3381']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.25it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:57:37 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3419']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.12it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:58:01 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3457']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.36it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:58:26 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3495']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.87it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:58:50 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3533']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.00it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:59:15 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3571']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.22it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 03:59:39 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3609']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.18it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:00:03 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.5714285714285714\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3647']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.00it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:00:28 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3685']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.40it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:00:52 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.29it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3723']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.48it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:01:16 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.28it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3761']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.65it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:01:40 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3799']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.36it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:02:04 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.28it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3837']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.73it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:02:29 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3875']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.92it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:02:53 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3913']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.64it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:03:18 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.5714285714285714\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3951']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.67it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:03:43 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_3989']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.42it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:04:07 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4027']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.49it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:04:31 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4065']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.00it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:04:56 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4103']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.57it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:05:20 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4141']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.52it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:05:44 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4179']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.54it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:06:09 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4217']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.85it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:06:33 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.23it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4255']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.50it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:06:58 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4293']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.86it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:07:22 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4331']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  6.62it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:07:46 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4369']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.15it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:08:10 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4407']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.50it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:08:35 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.14285714285714285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4445']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.49it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:08:59 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.27it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4483']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  6.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:09:24 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4521']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "100% 1/1 [00:00<00:00,  7.79it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:09:48 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.25it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.42857142857142855\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4559']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  7.31it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:10:13 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.27it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4597']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  9.05it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:10:37 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4635']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.82it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:11:01 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.5714285714285714\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4673']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.09it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:11:25 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.26it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.2857142857142857\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4711']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00,  8.08it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:11:50 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.21it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at google/tapas-base-finetuned-wtq were not used when initializing TFTapasForQuestionAnswering: ['dropout_570']\n",
            "- This IS expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFTapasForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFTapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base-finetuned-wtq and are newly initialized: ['dropout_4749']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== NOTE ====: encoder:roberta, mode:dev\n",
            "Reading file at %s ./dataset_tagop/tatqa_dataset_dev.json\n",
            "Reading the tatqa dataset\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 10.34it/s]\n",
            "{'Span-in-text': 0, 'Cell-in-table': 0, 'Spans': 0, 'Sum': 0, 'Count': 0, 'Average': 0, 'Multiplication': 0, 'Division': 0, 'Difference': 0, 'Change ratio': 0}\n",
            "{'': 7, 'thousand': 0, 'million': 0, 'billion': 0, 'percent': 0}\n",
            "0\n",
            "Save data to ./tag_op/cache/tagop_roberta_cached_dev.pkl.\n",
            "Namespace(ablation_mode=0, bert_learning_rate=None, bert_weight_decay=None, cuda=True, data_dir='tag_op/cache/', encoder='roberta', eval_batch_size=32, gpu_num=1, log_file='train.log', mode=1, model_path='./checkpoint', op_mode=0, roberta_model='dataset_tagop/roberta.large', save_dir='tag_op/', test_data_dir='tag_op/cache/')\n",
            "tag_op/cache/tagop_roberta_cached_dev.pkl\n",
            "Load data from tagop_roberta_cached_dev.pkl.\n",
            "Load data size 7.\n",
            "Some weights of the model checkpoint at dataset_tagop/roberta.large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "05/24/2022 04:12:14 Below are the result on Dev set...\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "raw matrix:            em\n",
            "answer_from   \n",
            "answer_type   \n",
            "             7\n",
            "\n",
            "detail em:              em\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "detail f1:              f1\n",
            "answer_from     \n",
            "answer_type     \n",
            "             0.0\n",
            "\n",
            "global em:0.0\n",
            "\n",
            "global f1:0.0\n",
            "\n",
            "global scale:0.2857142857142857\n",
            "\n",
            "global op:0.42857142857142855\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stQ701eQqc7t",
        "outputId": "dc7a96c3-5fd0-4784-f566-334b0638f66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'que_0': {'answers': [{'answer': 'AAUSat-3',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.967117309570312},\n",
              "   {'answer': 'Advanced Orion 5 (NRO L-32, USA 223)',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.066797256469727},\n",
              "   {'answer': 'AGILE (Astro-rivelatore Gamma a Immagini Leggaro)',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.589176654815674},\n",
              "   {'answer': 'Akebono (EXOS-D)',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.288186073303223},\n",
              "   {'answer': 'AMC-11 (Americom-11, GE 11)',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.1662840843200684},\n",
              "   {'answer': 'AMC-6 (Americom-6, GE-6)',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.2941250801086426},\n",
              "   {'answer': 'Anik F1 ',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.646323204040527},\n",
              "   {'answer': '', 'chunk': 7, 'model': 'TaPaS', 'score': -0.47064208984375},\n",
              "   {'answer': '', 'chunk': 8, 'model': 'TaPaS', 'score': -0.9863042831420898},\n",
              "   {'answer': 'Astra 1L',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.846750259399414},\n",
              "   {'answer': 'Astra 3B',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.6175954341888428},\n",
              "   {'answer': 'Brazilsat B-3 (Brasilsat B-3)',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.457268714904785},\n",
              "   {'answer': 'BSAT-3A',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.949718475341797},\n",
              "   {'answer': 'Can-X2 (Canadian Advanced Nanospace experiment)',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.2703801393508911},\n",
              "   {'answer': 'CFESat (Cibola Flight Experiment Satellite)',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.011791467666626},\n",
              "   {'answer': 'Ciel-2',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.661331653594971},\n",
              "   {'answer': 'Compass G-3 (Beidou G3)',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.568620681762695},\n",
              "   {'answer': 'COMSATBw-1 (COmmunications SATellite fï¿½r BundesWehr)',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.3440124988555908},\n",
              "   {'answer': 'COSMIC-A (Formosat-3A, Constellation Observing System for Meteorology, Ionosphere and Climate)',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.32168197631836},\n",
              "   {'answer': 'COSMO-Skymed 4 (Constellation of small Satellites for Mediterranean basin Observation)',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.49551010131836},\n",
              "   {'answer': 'DirecTV-1R ',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.455231666564941},\n",
              "   {'answer': 'DirecTV-4S ',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.852252960205078},\n",
              "   {'answer': 'DMSP 5D-3 F18 (Defense Meteorological Satellites Program, USA 210)',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.5876653790473938},\n",
              "   {'answer': 'DubaiSat-1',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.87574005126953},\n",
              "   {'answer': 'DubaiSat-2',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.321165084838867},\n",
              "   {'answer': 'ELISA-E12 (ELectronic Intelligence by SAtellite)',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.904001235961914},\n",
              "   {'answer': 'ELISA-E24 (ELectronic Intelligence by SAtellite)',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.04624366760254},\n",
              "   {'answer': 'EstCube-1',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 112.50678253173828},\n",
              "   {'answer': 'Eutelsat 25B (Esï¿½hail 1)',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.931838989257812},\n",
              "   {'answer': 'Eutelsat 48A (Eutelsat W-48, Eurobird 9, Hot Bird 2, Eutelsat 2-F8, Eutelsat HB2)',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.10186767578125},\n",
              "   {'answer': 'Eutelsat 9A (Eurobird 9A, Hot Bird 7A)',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.628567218780518},\n",
              "   {'answer': 'Express-A1R (Express 4A, Ekspress-A No. 4)',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.853404998779297},\n",
              "   {'answer': '', 'chunk': 32, 'model': 'TaPaS', 'score': -2.722170829772949},\n",
              "   {'answer': 'FUNCube-1 (AO-73)',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.005446910858154},\n",
              "   {'answer': 'Galaxy-11',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.295761108398438},\n",
              "   {'answer': 'GATOSS (Global Air Traffic Awareness and Optimizing through Spaceborne Surveillance)',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.6765265464782715},\n",
              "   {'answer': 'Genesis-2',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.151159286499023},\n",
              "   {'answer': 'Globalstar M037 (Globalstar 16)',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.96161651611328},\n",
              "   {'answer': 'Globalstar M072 (Globalstar 72)',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.101139068603516},\n",
              "   {'answer': 'Globalstar M083 (Globalstar 83, Globalstar 2-7)',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.028045654296875},\n",
              "   {'answer': 'Glonass 701 (Glonass-K, Cosmos 2471)',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.834100246429443},\n",
              "   {'answer': 'Glonass 721 (Glonass 37-1, Cosmos 2434)',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.50175094604492},\n",
              "   {'answer': 'Glonass 733 (Glonass 41-2, Cosmos 2457)',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.57024002075195},\n",
              "   {'answer': 'Glonass 747 (Glonass-M, Cosmos 2485)',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.5399668216705322},\n",
              "   {'answer': 'Gonets M-12',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.64923095703125},\n",
              "   {'answer': 'Grace 2 (Gravity Recovery and Climate Experiment, \"Tom and Jerry\")',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.588031768798828},\n",
              "   {'answer': 'Helios 2A ',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.962505340576172},\n",
              "   {'answer': 'Hispasat 1D ',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.26807165145874},\n",
              "   {'answer': 'Hubble Space Telescope (HST, Space Telescope)',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.371597290039062},\n",
              "   {'answer': 'IGS-8A (Information Gathering Satellite 8A, IGS Radar 4)',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.98402214050293},\n",
              "   {'answer': 'Integral (INTErnational Gamma-Ray Astrophysics Laboratory)',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.437224388122559},\n",
              "   {'answer': 'Intelsat 10 (PAS-10)',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.948074340820312},\n",
              "   {'answer': 'Intelsat 21 (IS-21)',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.167884826660156},\n",
              "   {'answer': 'Intelsat 706 ',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.74534225463867},\n",
              "   {'answer': 'Intelsat New Dawn',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.34217643737793},\n",
              "   {'answer': 'Iridium 18 (Iridium SV018)',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.62322998046875},\n",
              "   {'answer': 'Iridium 31 (Iridium SVO31)',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.41905975341797},\n",
              "   {'answer': 'Iridium 45 (Iridium SV045)',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.86754608154297},\n",
              "   {'answer': 'Iridium 56 (Iridium SV056)',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.74763870239258},\n",
              "   {'answer': 'Iridium 66 (Iridium SV066)',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.412906646728516},\n",
              "   {'answer': 'Iridium 8 (Iridium SV008)',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.37535095214844},\n",
              "   {'answer': 'Iridium 95 (Iridium SV095)',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.337797164916992},\n",
              "   {'answer': 'JCSat 10 (Japan Communications Satellite 10, JCSat 3A)',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.279354095458984},\n",
              "   {'answer': 'Kalpana-1 (Metsat-1)',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.26969909667969},\n",
              "   {'answer': 'Kondor',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.3378429412841797},\n",
              "   {'answer': 'Koreasat 5 (Mugungwha 5)',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.14752107858657837},\n",
              "   {'answer': 'Lotos-S (Cosmos 2455)',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.6102657318115234},\n",
              "   {'answer': 'Megha-Tropiques',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.5577983856201172},\n",
              "   {'answer': 'Meteosat 10 (MSGalaxy-3,MSG 3)',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.034502983093262},\n",
              "   {'answer': 'MKA-FKI-1 (Zond PP)',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.029154777526855},\n",
              "   {'answer': 'MTI (Multispectral Thermal Imager)',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.099916458129883},\n",
              "   {'answer': 'Navstar GPS II-14 (Navstar SVN 26, PRN 26, USA 83)',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.5222225189209},\n",
              "   {'answer': 'Navstar GPS IIF-3 (Navstar SVN 65, USA 239)',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.879547119140625},\n",
              "   {'answer': 'Navstar GPS IIR-3 (Navstar SVN 46, PRN 11, USA 145)',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.33014678955078},\n",
              "   {'answer': 'Navstar GPS IIR-M-3 (Navstar SVN 58, PRN 12, USA 192)',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.5235595703125},\n",
              "   {'answer': 'Nilesat 102 ',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.0350916385650635},\n",
              "   {'answer': 'NSS-10 (AMC-12, Americom 12, Worldsat 2)',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.210958480834961},\n",
              "   {'answer': 'NSS-9',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.45792007446289},\n",
              "   {'answer': 'Odin ',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.403255462646484},\n",
              "   {'answer': 'Optus D1 ',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.44723892211914},\n",
              "   {'answer': 'ORBCOMM FM-19 (ORBCOMM B7)',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.164941787719727},\n",
              "   {'answer': 'ORBCOMM FM-5 (ORBCOMM A6)',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.55746841430664},\n",
              "   {'answer': 'ï¿½rsted ',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.04959297180175781},\n",
              "   {'answer': 'Parus-92 (Cosmos 2378)',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.577373504638672},\n",
              "   {'answer': 'Proba 1 (Project for On-Board Autonomy)',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.8950514793395996},\n",
              "   {'answer': 'Proba 2 (Project for On-Board Autonomy)',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.512542724609375},\n",
              "   {'answer': '', 'chunk': 86, 'model': 'TaPaS', 'score': -0.6374341249465942},\n",
              "   {'answer': 'Rapid Pathfinder Program (NROL-66, USA 225)',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.43208885192871},\n",
              "   {'answer': 'Rodnik (Cosmos 2438, Strela 3M), Rodnik (Cosmos 2439, Strela 3M)',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.8734495043754578},\n",
              "   {'answer': 'Rodnik (Cosmos 2451, Strela 3M)',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.75910949707031},\n",
              "   {'answer': 'Rodnik (Cosmos 2496, Strela 3M)',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.0566587448120117},\n",
              "   {'answer': 'Sapphire (Space Surveillance Mission of Canada)',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.26592445373535},\n",
              "   {'answer': 'Saudicomsat-1',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.488054275512695},\n",
              "   {'answer': 'SBIRS GEO 2 (Space Based Infrared System Geosynchronous 2, USA 241)',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.5087151527404785},\n",
              "   {'answer': 'SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L23) ',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.13147735595703},\n",
              "   {'answer': 'SCD-1 (Satï¿½lite de Coleta de Dados)',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.067781925201416},\n",
              "   {'answer': 'SDS-4 (Small Demonstration Satellite-4)',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.5911149978637695},\n",
              "   {'answer': 'SES-8',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.217552185058594},\n",
              "   {'answer': 'Shijian 16 (SJ-16)',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.856565475463867},\n",
              "   {'answer': 'Shijian 6G (SJ6-04A)',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.224640846252441},\n",
              "   {'answer': 'Sirius 1 (SD Radio 1)',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.423437595367432},\n",
              "   {'answer': 'Sirius 2 (SD Radio 2)',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.098133087158203},\n",
              "   {'answer': 'Skynet 5C',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.899747848510742},\n",
              "   {'answer': 'SNaP-3-1ï¿½ï¿½(Space and Missile Defense Command NanoSat Program)',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.380706787109375},\n",
              "   {'answer': 'Spot 5 (Systï¿½me Probatoire dï¿½Observation de la Terre)',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.400167465209961},\n",
              "   {'answer': 'STRaND-1 (Surrey Training, Research and Nanosatellite Demonstrator 1)',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.811675071716309},\n",
              "   {'answer': 'Strela 3 (Cosmos 2400)',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.8683500289917},\n",
              "   {'answer': 'STUDSat (Student Satellite)',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.329278945922852},\n",
              "   {'answer': 'Syracuse 3B (Systeme de Radio Communications Utilisant un Satellite)',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.383210182189941},\n",
              "   {'answer': 'TDRS-5 (Tracking and Data Relay Satellite, TDRS-E)',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.73735809326172},\n",
              "   {'answer': '', 'chunk': 110, 'model': 'TaPaS', 'score': -0.1251220703125},\n",
              "   {'answer': 'Thor-5 (Thor 2R)',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.584548950195312},\n",
              "   {'answer': 'Tiangong-1 (TG-1)',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.86854362487793},\n",
              "   {'answer': 'Turksat 3A',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.529605865478516},\n",
              "   {'answer': 'UKube-1 (UK Cubesat 1)',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.410308361053467},\n",
              "   {'answer': 'Unisat-3 (University Satellite 3)',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.082563400268555},\n",
              "   {'answer': '',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -0.07886719703674316},\n",
              "   {'answer': 'Wren',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.531702041625977},\n",
              "   {'answer': 'XM Radio 3 (Rhythm)',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.606482028961182},\n",
              "   {'answer': 'Yahsat-1A (Y1A)',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.638433933258057},\n",
              "   {'answer': 'Yaogan 14 (Remote Sensing Satellite 14)',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.580564498901367},\n",
              "   {'answer': 'Yaogan 17C (Remote Sensing Satellite 17C)',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.023250579833984},\n",
              "   {'answer': 'Yaogan 6 (Remote Sensing Satellite 6, Jian Bing 7-A)',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.1662380695343018},\n",
              "   {'answer': 'Zhongxing 10 (XZ-10, Chinasat 10)',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.8945670127868652},\n",
              "   {'answer': 'Zhongxing 20A ',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.865185499191284}],\n",
              "  'class': 0,\n",
              "  'id': 'que_0',\n",
              "  'question': 'What is the name of the satellite from Estonia ?'},\n",
              " 'que_1': {'answers': [{'answer': 'AAUSat-3, ABS-2 (Koreasat-8, ST-3), ABS-3 (Agila 2, Mabuhay 1), ABS-4 (ABS-2i, MBSat, Mobile Broadcasting Satellite, Han Byul), ABS-6 (ABS-1, LMI-1, Lockheed Martin-Intersputnik-1), ABS-7 (Koreasat 3, Mugungwha 3), AcrimSat (Active Cavity Radiometer Irradiance Monitor), Advanced Orion 2 (NROL 6, USA 139), Advanced Orion 3 (NROL 19, USA 171), Advanced Orion 4 (NRO L-26, USA 202)',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.00596618652344},\n",
              "   {'answer': 'Advanced Orion 5 (NRO L-32, USA 223), Advanced Orion 6 (NRO L-15, USA 237), AEHF-1 (Advanced Extremely High Frequency satellite-1, USA 214), AEHF-2 (Advanced Extremely High Frequency satellite-2, USA 235), AEHF-3 (Advanced Extremely High Frequency satellite-3, USA 246), Aeneas, Aerocube 4.5A, Aerocube 4.5B, Aerocube 5A, Aerocube 5B, Aerocube 6A',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.53026580810547},\n",
              "   {'answer': 'Aerocube 6B, Afghansat-1 (Eutelsat 28B, Eutelsat 48B, Eutelsat W2M), Afristar , AGILE (Astro-rivelatore Gamma a Immagini Leggaro), AIM (Aeronomy of Ice in Mesosphere), AISat-1, AISSat-1 (Automatic Identification System Satellite-1), AISSat-2 (Automatic Identification System Satellite-2), AIST-1, AIST-2',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.259765625},\n",
              "   {'answer': 'Akebono (EXOS-D), ALiCEï¿½ï¿½(AFIT LEO iMESA CNT E), Alphasat I-XLï¿½ï¿½(Inmarsat IV-A F4), Alsat-2A (Algeria Satellite 2A), Amazonas-1 , Amazonas-2, Amazonas-3, Amazonas-4A, AMC-1 (Americom 1, GE-1), AMC-10 (Americom-10, GE 10)',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 123.1959457397461},\n",
              "   {'answer': 'AMC-11 (Americom-11, GE 11), AMC-15 (Americom-15), AMC-16 (Americom-16), AMC-18 (Americom 18), AMC-2 (Americom 2, GE-2), AMC-21 (Americom 21), AMC-3 (Americom 3, GE-3), AMC-4 (Americom-4, GE-4), AMC-5 (Americom-5, GE-5)',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 96.745361328125},\n",
              "   {'answer': 'AMC-6 (Americom-6, GE-6), AMC-7 (Americom-7, GE-7), AMC-8 (Americom-8, GE-8, Aurora 3), AMC-9 (Americom 9, GE-12), Amos 2 , Amos 3, Amos 5, Amsat-Oscar 7 (AO-7), Angels (Automated Navigation and Guidance Experiment for Local Space, USA 255)',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.633487701416016},\n",
              "   {'answer': 'Anik F1 , Anik F1R, Anik F2 , Anik F3, Anik G1, AntelSat, AprizeSat 1 (LatinSat-C), AprizeSat 10, AprizeSat 2 (LatinSat-D), AprizeSat 3, AprizeSat 4, AprizeSat 8',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 156.05404663085938},\n",
              "   {'answer': 'AprizeSat 9, Apstar 1, Apstar 1A, Apstar 6, Apstar 7, Arabsat 5C, Arabsat 7F (Nimiq 1) , Ardusat-1, Artemis (Advanced Data Relay and Technology Mission Satellite) , AsiaSat-3S (Asiasat 3SA)',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 93.2017593383789},\n",
              "   {'answer': 'AsiaSat-4 , AsiaSat-5, AsiaSat-7',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 113.62275695800781},\n",
              "   {'answer': 'Astra 1L, Astra 1M, Astra 1N, Astra 2A , Astra 2B , Astra 2C , Astra 2D, Astra 2E, Astra 2F, Astra 3A ',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.09992980957031},\n",
              "   {'answer': 'Astra 3B, Astra 5B, Athena-Fidusï¿½ï¿½(Access on THeatres for European Nations Allied forces - French Italian Dual Use Satellite), Auroraï¿½ï¿½(Tabletsat-2U-EO), Azerspace/Africasat-1a (Azersat-1), Badr 2 (Badr B), Badr 4 (Arabsat 4B), Badr 5 (Arabsat 5B), Badr 5A (Arabsat 5A), Badr 6 (Arabsat 4AR)',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.36354064941406},\n",
              "   {'answer': 'BeeSat-1 (Berlin Experimental and Educational Satellite 1), BeeSat-2 (Berlin Experimental and Educational Satellite 2), BeeSat-3 (Berlin Experimental and Educational Satellite 3), BeijinGalaxy-1 (Beijing 1 [Tsinghua], Tsinghau-2, China DMC+4), Bird 2 (Bispectral InfraRed Detector 2), BKA (BelKA 2), Bonum 1 (Most 1), Brazilsat B-2 (Brasilsat B-2), Brazilsat B-3 (Brasilsat B-3)',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 128.60569763183594},\n",
              "   {'answer': 'Brazilsat B-4 (Brasilsat B-4), BRITE-CA-1 (BRITE Toronto), BRITE-PL-1 (BRIght-star Target Explorer - Poland), BSAT-3A, BSAT-3B, BSAT-3C/JCSat 110-R, Bugsat-1 (Tita), C/NOFS (Communication/Navigation Outage Forecasting System), Calipso (Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation), Canopus-Bï¿½(Kanopus Vulcan 1)ï¿½',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.68563079833984},\n",
              "   {'answer': 'CartoSat 1 (IRS P5), CartoSat 2 (IRS P7, CartoSat 2AT), CartoSat 2A',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.60320281982422},\n",
              "   {'answer': 'CartoSat 2B',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 144.70228576660156},\n",
              "   {'answer': 'Ciel-2, CINEMA-1 (Cubesat for Ion, Neutral, Electron, Magnetic fields), CINEMA-2 (Cubesat for Ion, Neutral, Electron, Magnetic fields), CINEMA-3 (Cubesat for Ion, Neutral, Electron, Magnetic fields), Cloudsat, Compass G-1 (Beidou G1), Compass G-10 (Beidou ISGO-5), Compass G-11 (Beidou G5)',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 117.16310119628906},\n",
              "   {'answer': 'Compass G-3 (Beidou G3), Compass G-4 (Beidou G4), Compass G-5 (Beidou IGSO-1), Compass G-6 (Beidou 2-16), Compass G-7 (Beidou IGSO-2), Compass G-8 (Beidou IGSO-3), Compass G-9 (Beidou ISGO-4), Compass M1 (Beidou M1)',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.43466186523438},\n",
              "   {'answer': 'Compass M3 (Beidou 2-12), Compass M4 (Beidou 2-13), Compass M5 (Beidou 2-14), Compass M6 (Beidou 2-15), COMS-1 (Communication, Ocean and Meteorological Satellite; Cheollian), COMSATBw-1 (COmmunications SATellite fï¿½r BundesWehr), COMSATBw-2 (COmmunications SATellite fï¿½r BundesWehr), Coriolis (Windsat)',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 130.738037109375},\n",
              "   {'answer': 'COSMIC-A (Formosat-3A, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-B (Formosat-3B, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-D (Formosat-3D, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-E (Formosat-3E, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-F (Formosat-3F, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMO-Skymed 1 (Constellation of small Satellites for Mediterranean basin Observation), COSMO-Skymed 2 (Constellation of small Satellites for Mediterranean basin Observation), COSMO-Skymed 3 (Constellation of small Satellites for Mediterranean basin Observation)',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.11225891113281},\n",
              "   {'answer': 'COSMO-Skymed 4 (Constellation of small Satellites for Mediterranean basin Observation), Cryosat-2 , Cubebug 1ï¿½ï¿½(Capitï¿½n Beto), Cubebug 2 (Manolito, LO-74), Cubesat XI-IV (Oscar 57, CO-57), Cubesat XI-V (Oscar 58, CO-58), CUNYSat-1ï¿½ï¿½(City University of New York Satellite-1), CUSat-1 (Cornell University Satellite 1), Cute-1 (Cubical Titech Engineering satellite, Oscar 55, CO-55), Cute-1.7 + APD II (Cubical Titech Engineering satellite+Avalanche Photodiode)',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 101.9793472290039},\n",
              "   {'answer': 'Daichi-2 (Advanced Land Observing Satellite-2, ALOS 2 2), Daichi-2 (ALOS -2, Advanced Land Observing Satellite-2), DANDE (Drag and Atmospheric Neutral Density Explorer), Deimos 1, Deimos 2 , Delfi-C3 (DO-64), DirecTV-10, DirecTV-11, DirecTV-12, DirecTV-1R ',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.08821105957031},\n",
              "   {'answer': 'DirecTV-4S , DLR Tubsat ',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.40335464477539},\n",
              "   {'answer': 'DMSP 5D-3 F18 (Defense Meteorological Satellites Program, USA 210), DMSP 5D-3 F19 (Defense Meteorological Satellites Program, USA 249), Dove-2, Dove-3, Dove-4, DRTS (Data Relay Test Satellite, Kodama), DSCS III-A3 (USA 167, DSCS III-A3) (Defense Satellite Communications System), DSCS III-B6 (USA 170, DSCS III B-6) (Defense Satellite Communications System), DSCS III-F10 (USA 135, DSCS III B-13) (Defense Satellite Communications System)',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.87236022949219},\n",
              "   {'answer': 'DSCS III-F11 (USA 148, DSCS III B-8) (Defense Satellite Communications System), DSCS III-F12 (USA 153, DSCS III B-11) (Defense Satellite Communications System), DSCS III-F6 (USA 82, DSCS III B-12) (Defense Satellite Communications System), DSCS III-F8 (USA 97, DSCS III B-10) (Defense Satellite Communications System), DSCS III-F9 (USA 113, DSCS III B-7) (Defense Satellite Communications System), DSP 18 (USA 130) (Defense Support Program), DSP 20 (USA 149) (Defense Support Program), DSP 21 (USA 159) (Defense Support Program), DSP 22 (USA 176) (Defense Support Program), DubaiSat-1',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.52497100830078},\n",
              "   {'answer': 'DubaiSat-2, DX-1 (Dauria Experimental 1), Echostar 1 ',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.763023376464844},\n",
              "   {'answer': 'Echostar 3 , Egyptsat-2 (Misrsat 2), Electro-L1 (GOMS 2 [Geostationary Ioperational Meteorological Satellite 2], ELISA-E12 (ELectronic Intelligence by SAtellite)',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.5745964050293},\n",
              "   {'answer': 'ELISA-E24 (ELectronic Intelligence by SAtellite), ELISA-W11 (ELectronic Intelligence by SAtellite), ELISA-W23 (ELectronic Intelligence by SAtellite), EO-1 (Earth Observing 1), EOS-AM Terra , EOS-CHEM Aura , EOS-PM Aqua (Advanced Microwave Scanning Radiometer for EOS, EOS PM-1), EROS A1 (Earth Resources Observation Satellite), EROS B1 (Earth Resources Observation Satellite), e-st@r',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.45964813232422},\n",
              "   {'answer': 'EstCube-1, Eurasiasat 1 (Turksat 2A), Europe*Star 1 (Intelsat 12, Europe*Star FM1, IS-12, PAS-12), Eutelsat 10A (Eutelsat W-2A), Eutelsat 12 West A (Atlantic Bird 1) , Eutelsat 16A (Eutelsat W3C), Eutelsat 16B (Eurobird 16, Atlantic Bird 4, Hot Bird 4) , Eutelsat 16C (SESAT-1, Siberia-Europe Satellite), Eutelsat 172A (GE-23, AMC-23, Worldsat 3, Americom 23)',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 132.57479858398438},\n",
              "   {'answer': 'Eutelsat 25B (Esï¿½hail 1), Eutelsat 25C (Eutelsat 70A, Eutelsat W-5) , Eutelsat 28A (Eurobird 1, Eurosat 1), Eutelsat 33A (Eurobird 3, E-bird) , Eutelsat 36A (Eutelsat W-4) , Eutelsat 36B (Eutelsat W-7), Eutelsat 3A (Sinosat-5C, Sinosat-3, Xinnuo 3, Chinasat-5C, XN-3), Eutelsat 3B, Eutelsat 3D ',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 129.17237854003906},\n",
              "   {'answer': 'Eutelsat 48A (Eutelsat W-48, Eurobird 9, Hot Bird 2, Eutelsat 2-F8, Eutelsat HB2), Eutelsat 48B (Eutelsat 21B, Eutelsat W-6A), Eutelsat 48C (Eutelsat 21A, W-6, Eutelsat W-3), Eutelsat 5 West A (Atlantic Bird 3, Stellat 5), Eutelsat 7 West A (Atlantic Bird 7), Eutelsat 70B (Eutelsat W5A), Eutelsat 7A (Eutelsat W3A) , Eutelsat 8 West A (Atlantic Bird 2) , Eutelsat 8 West C (Eutelsat Hot Bird 13A, Hot Bird 6)',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 144.58201599121094},\n",
              "   {'answer': 'Eutelsat 9A (Eurobird 9A, Hot Bird 7A), Eutelsat Hot Bird  13D (Eutelsat 3C, Hot Bird 10, Atlantic Bird 4A), Eutelsat Hot Bird 13B (Hot Bird 8), Eutelsat Hot Bird 13C (Hot Bird 9), Eutelsat KA-SAT 9A (KA-SAT), exactView 1, exactView 5 (AprizeSat 5), exactView5R, exactView6 (AprizeSat 6), Express AM22 (Sesat 2), Express-2A (Ekspress A No. 2)',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 168.8777618408203},\n",
              "   {'answer': 'Express-A1R (Express 4A, Ekspress-A No. 4), Express-AM2, Express-AM3, Express-AM33, Express-AM44, Express-AM5, Express-AT1, Express-AT2, Falconsat-3, FAST 1 (Sara Lilly and Emma, USA 222), Fengniao 1 (Hummingbird 1)',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.69342041015625},\n",
              "   {'answer': 'Fengniao 1A (Hummingbird 1A), Fengyun 2D (FY-2D), Fengyun 2E (FY-2E), Fengyun 2F (FY-2F), Fengyun 3A (FY-3A), Fengyun 3B (FY-3B), Fengyun 3C (FY-3C), Fermi Gamma-Ray Space Telescope (formerly GLAST), FIA Radar 1 (Future Imagery Architecture (FIA) Radar 1, NROL-41, USA 215, Topaz)',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 157.05555725097656},\n",
              "   {'answer': 'FIA Radar 2 (Future Imagery Architecture (FIA) Radar 2, NROL-25, USA 234, Topaz), FIA Radar 3 (Future Imagery Architecture (FIA) Radar 3, NROL-39 , USA 247, Topaz), Firebird-A (Focused Investigations of Relativistic Electron Burst, Intensity, Range, and Dynamics), Firebird-B (Focused Investigations of Relativistic Electron Burst, Intensity, Range, and Dynamics), Firefly, FLTSATCOM-8 (USA 46), Formosat-2 (ROCSAT-2, Republic of China Satellite 2) , FORTï¿½ (Fast On-orbit Recording of Transient Events), FUNCube-1 (AO-73)',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.601318359375},\n",
              "   {'answer': 'Galaxy-11, Galaxy-12, Galaxy-13 (Horizons 1, Galaxy 13L), Galaxy-14, Galaxy-15, Galaxy-16 , Galaxy-17, Galaxy-18, Galaxy-19, Galaxy-25 (G-25, Intelsat 1A-5, Telstar 5), Galaxy-27 (G-27, Intelsat IA-7, Telstar 7)',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 80.34148406982422},\n",
              "   {'answer': 'Galaxy-28 (G-28, Intelsat IA-8, Telstar 8), Galaxy-3C, Galileo IOV-1 FM2, Galileo IOV-1 PFM, Galileo IOV-2 FM3, Galileo IOV-2 FM4, Gaofen 1, Garpun-1 (Cosmos 2473), Garuda-1 (Aces 1), GATOSS (Global Air Traffic Awareness and Optimizing through Spaceborne Surveillance), Genesis-1',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.91939544677734},\n",
              "   {'answer': 'Genesis-2, GeoEye-1 (Orbview 5), Geotail (Geomagnetic Tail Laboratory), Global Change Observation Mission - 1 Water (GCOM-1, Shikuzu), Globalstar M004 (Globalstar 4), Globalstar M006 (Globalstar 6), Globalstar M023 (Globalstar 9), Globalstar M027 (Globalstar 34), Globalstar M028 (Globalstar 30), Globalstar M029 (Globalstar 47), Globalstar M031 (Globalstar 44)',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.15318298339844},\n",
              "   {'answer': 'Globalstar M037 (Globalstar 16)',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.93754959106445},\n",
              "   {'answer': 'Globalstar M072 (Globalstar 72), Globalstar M073 (Globalstar 73, Globalstar 2-6), Globalstar M074 (Globalstar 74, Globalstar 2-2), Globalstar M075 (Globalstar 75, Globalstar 2-5), Globalstar M076 (Globalstar 76, Globalstar 2-3), Globalstar M077 (Globalstar 77, Globalstar 2-4), Globalstar M078 (Globalstar 95, Globalstar 2-23), Globalstar M079 (Globalstar 79, Globalstar 2-1), Globalstar M080 (Globalstar 80, Globalstar 2-14), Globalstar M081 (Globalstar 81, Globalstar 2-11), Globalstar M082 (Globalstar 82, Globalstar 2-15)',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 96.39653778076172},\n",
              "   {'answer': 'Globalstar M083 (Globalstar 83, Globalstar 2-7), Globalstar M084 (Globalstar 84, Globalstar 2-13) , Globalstar M085 (Globalstar 85, Globalstar 2-10), Globalstar M086 (Globalstar 86, Globalstar 2-18), Globalstar M088 (Globalstar 88, Globalstar 2-8), Globalstar M089 (Globalstar 89, Globalstar 2-12), Globalstar M090 (Globalstar 90, Globalstar 2-17), Globalstar M091 (Globalstar 91, Globalstar 2-9), Globalstar M092 (Globalstar 92, Globalstar 2-16), Globalstar M093 (Globalstar 87, Globalstar 2-20), Globalstar M094 (Globalstar 93, Globalstar 2-21)',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 87.87458801269531},\n",
              "   {'answer': 'Globalstar M095 (Globalstar 96, Globalstar 2-24), Globalstar M096 (Globalstar 94, Globalstar 2-22), Globalstar M097 (Globalstar 78, Globalstar 2-19), Glonass 701 (Glonass-K, Cosmos 2471), Glonass 712 (Glonass M, Cosmos 2413), Glonass 714 (Cosmos 2419), Glonass 715 (Glonass 35-1, Cosmos 2424), Glonass 716 (Glonass 35-2, Cosmos 2425), Glonass 717 (Glonass 35-3, Cosmos 2426), Glonass 719 (Glonass 36-2, Cosmos 2432), Glonass 720 (Glonass 36-3, Cosmos 2433)',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 146.7766876220703},\n",
              "   {'answer': 'Glonass 721 (Glonass 37-1, Cosmos 2434), Glonass 722 (Glonass 37-2, Cosmos 2435), Glonass 723 (Glonass 37-3, Cosmos 2436), Glonass 724 (Glonass 38-1, Cosmos 2442), Glonass 725 (Glonass 38-2, Cosmos 2443), Glonass 726 (Glonass 38-3, Cosmos 2444), Glonass 727 (Glonass 39-1, Cosmos 2447), Glonass 729 (Glonass 39-3, Cosmos 2449), Glonass 730 (Glonass 41-1, Cosmos 2456), Glonass 731 (Glonass 42-1, Cosmos 2459), Glonass 732 (Glonass 42-3, Cosmos 2460)',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 162.07275390625},\n",
              "   {'answer': 'Glonass 733 (Glonass 41-2, Cosmos 2457), Glonass 734 (Glonass 41-3, Cosmos 2458), Glonass 735 (Glonass 42-2, Cosmos 2461), Glonass 736 (Glonass 43-1, Cosmos 2464), Glonass 737 (Glonass 43-2, Cosmos 2465), Glonass 738 (Glonass 43-3, Cosmos 2466), Glonass 742 (Glonass-M, Cosmos 2474), Glonass 743 (Glonass 44-2, Cosmos 2476), Glonass 744 (Glonass 44-3, Cosmos 2477), Glonass 745 (Glonass 44-1, Cosmos 2475), Glonass 746 (Glonass-M, Cosmos 2478)',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 154.18472290039062},\n",
              "   {'answer': 'Glonass 747 (Glonass-M, Cosmos 2485), Glonass 754 (Glonass-M, Cosmos 2491), Glonass 755 (Glonass-M, Cosmos 2500), Gonets D1-2',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.41316986083984},\n",
              "   {'answer': 'Gonets M-12, Gonets M-13, Gonets M-14, Gonets M-15, Gonets M-16, Gonets M-17, Gonets M-18, Gonets M-19, Gonets M-20, GPM Core Observatory (Global Precipitation Measurement), Grace 1 (Gravity Recovery and Climate Experiment, \"Tom and Jerry\")',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.84864807128906},\n",
              "   {'answer': 'GSAT-10, GSAT-12, GSAT-14, GSAT-2, GSAT-7, GSAT-8',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.20268249511719},\n",
              "   {'answer': 'HAMSat (VUSat Oscar 52)',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.69471740722656},\n",
              "   {'answer': 'Hispasat 1D , Hispasat 1E, HJ-1A (Huan Jing 1A), HJ-1B (Huan Jing 1B), HJ-1C (Huan Jing 1C), Hodoyoshi-3, Horizons 2, Horyu-2 (High Voltage Technology Demonstration Satellite-2)',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.5078125},\n",
              "   {'answer': 'Hubble Space Telescope (HST, Space Telescope), HumSat-D, HYLAS 1 (Highly Adaptable Satellite), HYLAS 2 (Highly Adaptable Satellite), ICube, IGS-1A (Information Gathering Satellite 1A), IGS-3A (Information Gathering Satellite 3A), IGS-5A (Information Gathering Satellite 5A, IGS Optical 3), IGS-6A (Information Gathering Satellite 6A, IGS Optical 4), IGS-7A (Information Gathering Satellite 7A, IGS Radar 3)',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 130.64231872558594},\n",
              "   {'answer': 'IGS-8A (Information Gathering Satellite 8A, IGS Radar 4), IGS-8B (Information Gathering Satellite 8B, IGS Optical Demonstrator), Ikonos-2 , Improved Trumpet 4 (NROL-22, National Reconnaissance Office Launch-22, SBIRS HEO-1, Twins 1, USA 184), Improved Trumpet 5 (NROL-28, National Reconnaissance Office Launch-28, SBIRS HEO-2, Twins 2, USA 200), INMARSAT 3 F1 , INMARSAT 3 F2 , INMARSAT 3 F4 , INMARSAT 3 F5 , INMARSAT 4 F1',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.31898498535156},\n",
              "   {'answer': 'INSAT 3A (Indian National Satellite), INSAT 3C (Indian National Satellite), INSAT 3D (Indian National Satellite), INSAT 4A (Indian National Satellite), INSAT 4B (Indian National Satellite), INSAT 4CR (Indian National Satellite)',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.04264831542969},\n",
              "   {'answer': 'Intelsat 10 (PAS-10)',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.946998596191406},\n",
              "   {'answer': 'Intelsat 21 (IS-21)',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.89728546142578},\n",
              "   {'answer': 'Intelsat 706 , Intelsat 8 (IS-8, PAS-8) , Intelsat 805 ',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.912445068359375},\n",
              "   {'answer': 'Intelsat New Dawn, International Space Station (ISS [first element Zarya]), Interstellar Boundary EXplorer (IBEX), IPEX (Intelligent Payload Experiment, CalPoly 8), Iridium 10 (Iridium SV010), Iridium 11A (Iridium SVO88), Iridium 12 (Iridium SV012), Iridium 13 (Iridium SV013), Iridium 14A (Iridium SV092), Iridium 15 (Iridium SV015)',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.73799133300781},\n",
              "   {'answer': 'Iridium 18 (Iridium SV018)',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.98104858398438},\n",
              "   {'answer': 'Iridium 31 (Iridium SVO31)',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.20684814453125},\n",
              "   {'answer': 'Iridium 45 (Iridium SV045)',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.64769744873047},\n",
              "   {'answer': 'Iridium 56 (Iridium SV056)',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.04364776611328},\n",
              "   {'answer': 'Iridium 66 (Iridium SV066)',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.63926696777344},\n",
              "   {'answer': 'Iridium 8 (Iridium SV008)',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.17459106445312},\n",
              "   {'answer': 'IRNSS-1A (Indian Regional Navigation Satellite System), IRNSS-1B (Indian Regional Navigation Satellite System), IRS-P6 (Resourcesat-1) ',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.71002197265625},\n",
              "   {'answer': 'Jugnu',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.033607482910156},\n",
              "   {'answer': 'Kalpana-1 (Metsat-1)',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.6546401977539},\n",
              "   {'answer': 'Keyhole 7 (NRO L65, Advanced KH-11, Improved Crystal, USA 245), Kiku-8 (ETS-8, Engineering Test Satellite 8), Kizuna (WINDS - Wideband InterNetworking engineering Demonstration Satellite), KKS-1 (Kouku Kousen Satellite One, Kiseki), Kobalt-M (Cosmos 2495), Kompsat-2 (Arirang 2, Korean planned Multipurpose Satellite-2), Kompsat-3 (Arirang 3, Korean Multipurpose Satellite-3), Kompsat-5 (Arirang 5, Korean Multipurpose Satellite-4), Kondor',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 116.49756622314453},\n",
              "   {'answer': 'Koreasat 5 (Mugungwha 5), Koreasat 6 (Mugungwha 6), Kuaizhou-1 (KZ-1), Lacrosse/Onyx 3 (Lacrosse-3, USA 133), Lacrosse/Onyx 4 (Lacrosse-4, USA 152), Lacrosse/Onyx 5 (Lacrosse-5, NROL 16, USA 182), Landsat 7 , Landsat 8, LAPAN-Tubsat, LatinSat A, LatinSat B ',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.16452026367188},\n",
              "   {'answer': 'Leasat 5 (Syncom IV-5, Leased Satellite F5), Lemur-1, Lotos-S (Cosmos 2455), Luch 5A, Luch 5B, Luch 5V, Maroc Tubsat , MaSat 1  (Magyar Satellite/Oscar 72) , M-Cubed/E1P-U2 (Michigan Multipurpose Minisat; Explorer 1 Prime - Unit 2 -  HRBE [William A. Hiscock Radiation Belt Explorer]), Measat 2 (Malaysia East Asia Sat 2)',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 135.78868103027344},\n",
              "   {'answer': 'Megha-Tropiques',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.14665222167969},\n",
              "   {'answer': 'Meteosat 10 (MSGalaxy-3,MSG 3), Meteosat 7 (MTP 1) , Meteosat 8 (MSGalaxy-1, MSG-1), Meteosat 9 (MSGalaxy-2, MSG 2), MetOp-A (Meteorological Operational satellite), MetOp-B (Meteorological Operational satellite), Mexsat-3 (Mexsat Bicentenario), Milstar DFS-1 (USA 99, Milstar 1-F1) (Military Strategic and Tactical Relay)',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 123.97537994384766},\n",
              "   {'answer': 'Milstar DFS-2 (USA 115, Milstar 1-F2) (Military Strategic and Tactical Relay), Milstar DFS-4 (USA 157, Milstar 2-F2) (Military Strategic and Tactical Relay), Milstar DFS-5 (USA 164, Milstar 2-F3) (Military Strategic and Tactical Relay), Milstar DFS-6 (USA 169) (Military Strategic and Tactical Relay), MiRï¿½ï¿½(Mikhail Reshetnev [MiR], Yubileiny-2/RS-40), MKA-FKI-1 (Zond PP), MOST (Microvariability & Oscillations of STars), Mozhayets 4 (RS-22), MSAT 1 , MSAT 2 (AMSC-1, ACTel-1)',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 119.19084167480469},\n",
              "   {'answer': 'MTI (Multispectral Thermal Imager), MTSAT-1R (Himawari 6), MTSAT-2 (Multi-Functional Transport Satellite), MUOS-1 (Mobile User Objective System 1), MUOS-2 (Mobile User Objective System 2), Nanosat-1, Nanosat-1B, NanosatC-Br1ï¿½, NATO 4-B (USA 98), Navstar GPS II-10 (Navstar SVN 23, PRN 32,  USA 66)',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 109.50464630126953},\n",
              "   {'answer': 'Navstar GPS II-14 (Navstar SVN 26, PRN 26, USA 83), Navstar GPS II-21 (Navstar SVN 39, PRN 09, USA 92), Navstar GPS II-23 (Navstar SVN 34, PRN 04, USA 96), Navstar GPS II-24 (Navstar SVN 36, PRN 06, USA 100), Navstar GPS II-25 (Navstar SVN 33, PRN 03, USA 117), Navstar GPS II-26 (Navstar SVN 40, PRN 10, USA 126), Navstar GPS II-28 (Navstar SVN 38, PRN 08, USA 135), Navstar GPS II-35 (Navstar SVN 35, PRN 30, USA 94), Navstar GPS IIF-1 (Navstar SVN 62, PRN 25, USA 213), Navstar GPS IIF-2 (Navstar SVN 63, PRN 01, USA 232)',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 158.1626434326172},\n",
              "   {'answer': 'Navstar GPS IIF-3 (Navstar SVN 65, USA 239), Navstar GPS IIF-4 (Navstar SVN 66, USA 242), Navstar GPS IIF-5 (Navstar SVN 64, USA 248), Navstar GPS IIF-6 (Navstar SVN 67, USA 251), Navstar GPS IIR-10 (Navstar SVN 47, PRN 22, USA 175), Navstar GPS IIR-11 (Navstar SVN 59, PRN 19, USA 177), Navstar GPS IIR-12 (Navstar SVN 60, PRN 23, USA 178), Navstar GPS IIR-13 (Navstar SVN 61, PRN 02, USA 180), Navstar GPS IIR-2 (Navstar SVN 43, PRN 13, USA 132)',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 118.03340911865234},\n",
              "   {'answer': 'Navstar GPS IIR-3 (Navstar SVN 46, PRN 11, USA 145), Navstar GPS IIR-4 (Navstar SVN 51, PRN 20, USA 150), Navstar GPS IIR-5 (Navstar SVN 44, PRN 28, USA 151), Navstar GPS IIR-6 (Navstar SVN 41, PRN 14, USA 154), Navstar GPS IIR-7 (Navstar SVN 54, PRN 18, USA 156), Navstar GPS IIR-8 (Navstar SVN 56, PRN 16, USA 166), Navstar GPS IIR-9 (Navstar SVN 45, PRN 21, USA 168), Navstar GPS IIR-M-1 (Navstar SVN 53, PRN 17, USA 183), Navstar GPS IIR-M-2 (Navstar SVN 52, PRN 31, USA 190)',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.1425018310547},\n",
              "   {'answer': 'Navstar GPS IIR-M-3 (Navstar SVN 58, PRN 12, USA 192), Navstar GPS IIR-M-4 (Navstar SVN 55, PRN 15, USA 196), Navstar GPS IIR-M-5 (Navstar SVN 57, PRN 29, USA 199), Navstar GPS IIR-M-6 (Navstar SVN 48, PRN 07, USA 201), Navstar GPS IIR-M-8 (Navstar SVN 50, PRN 05, USA 206), NEOSSatï¿½(Near Earth Object Surveillance Satellite), NFIRE (Near Field InfraRed Experiment), NigComSat-1R, NigeriaSat-2, Nigeriasat-X',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 153.83949279785156},\n",
              "   {'answer': 'Nilesat 102 , Nilesat 201, Nimiq 2 , Nimiq 4, Nimiq 5, Nimiq 6, NOAA-15 (NOAA-K), NOAA-18 (NOAA-N, COSPAS-SARSAT), NOAA-19 (NOAA-N Prime, COSPAS-SARSAT), NPP (National Polar-orbiting Operational Environmental Satellite System [NPOESS])',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 146.53271484375},\n",
              "   {'answer': 'NPS-SCATï¿½(Naval Post-graduate School - Solar Cell Array Tester), NSS-10 (AMC-12, Americom 12, Worldsat 2)',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.491086959838867},\n",
              "   {'answer': 'NSS-9, N-Star C , NuSTAR (Nuclear Spectroscopic Telescope Array), O/OREOS (Organism/Organic Exposure to Orbital Stresses, USA 219), O3b FM02, O3b FM03, O3b FM04, O3b FM05 (Other 3 Billion), O3b FM06, O3B FM07, O3B FM08, O3b PFM',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.30091094970703},\n",
              "   {'answer': 'Oceansat-2',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 125.41172790527344},\n",
              "   {'answer': 'Optus D1 , Optus D2, Optus D3, ORBCOMM FM-10 (ORBCOMM A2), ORBCOMM FM-11 (ORBCOMM A3), ORBCOMM FM-12 (ORBCOMM A4), ORBCOMM FM-13 (ORBCOMM B1), ORBCOMM FM-14 (ORBCOMM B2), ORBCOMM FM-15 (ORBCOMM B3), ORBCOMM FM-16 (ORBCOMM B4), ORBCOMM FM-18 (ORBCOMM B6)',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.17073059082031},\n",
              "   {'answer': 'ORBCOMM FM-19 (ORBCOMM B7)',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.770721435546875},\n",
              "   {'answer': 'ORBCOMM FM-5 (ORBCOMM A6)',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.85734558105469},\n",
              "   {'answer': 'ORS - Tech 2, ORS-1 (Operationally Responsive Space One, USA 231), ORSES (ORS Enabler Satellite), ï¿½rsted , Paksat-1R , Palapa C2 , Palapa D1, PAN-1 (Palladium at Night, P360, USA 207), PARASOL (Polarization and Anistropy of Reflectances for Atmospheric Science coupled with Observations from LIDAR), Parus-90 (Cosmos 2361), Parus-91 (Cosmos 2366)',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 121.73902893066406},\n",
              "   {'answer': 'Parus-92 (Cosmos 2378), Parus-93 (Cosmos 2389), Parus-94 (Cosmos 2398), Parus-95 (Cosmos 2407), Parus-96 (Cosmos 2414), Parus-97 (Cosmos 2429), Parus-98 (Cosmos 2454), Parus-99 (Cosmos 2463), PCSat (Prototype Communications SATellite, Navy-Oscar 44, NO-44), Perseus M1, Perseus M2, Persona-2 (Cosmos 2486)',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 135.02178955078125},\n",
              "   {'answer': 'Phonesat 2.4, Photon M4, Picard, PicoDragon, Plï¿½iades HR1A, Plï¿½iades HR1B, PolyITAN-1, Popsat-HIP, Prism (Pico-satellite for Remote-sensing and Innovative Space Missions, Hitomi), Proba 1 (Project for On-Board Autonomy)',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.87158966064453},\n",
              "   {'answer': 'Proba 2 (Project for On-Board Autonomy), Proba V (Project for On-Board Autonomy)',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.94283294677734},\n",
              "   {'answer': 'QB50P1ï¿½(EO 79), QB50P2 (EO 80), QuetzSat-1, Quickbird 2, QZS-1 (Quazi-Zenith Satellite System, Michibiki), Radarsat-2 , Radio-ROSTO (RS-15, Radio Sputnik 15, Russian Defence, Sports and Technical Organization - ROSTO), Raduga 1-8 (Raduga 1-M1, Cosmos 2450), Raduga 1-M2 (Raduga 1-9), Raduga 1-M3 ',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 124.82452392578125},\n",
              "   {'answer': 'Rapid Pathfinder Program (NROL-66, USA 225), RapidEye-1 (RapidEye-C), RapidEye-2 (RapidEye A), RapidEye-3 (RapidEye D), RapidEye-4 (RapidEye E), RapidEye-5 (RapidEye B), RASAT, Rascom-QAF 1R, RazakSat (MACSat), Reimei (Innovative Technology Demonstration Experiment Satellite - INDEX), Relek (ICA-FC1)',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.933349609375},\n",
              "   {'answer': 'Resourcesat 2, RISat-1 (Radar Imaging Satellite 1), RISat-2 (Radar Imaging Satellite 2)',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.53560256958008},\n",
              "   {'answer': 'Rodnik (Cosmos 2451, Strela 3M)',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.54782104492188},\n",
              "   {'answer': 'Rodnik (Cosmos 2496, Strela 3M), Rodnik (Cosmos 2497, Strela 3M), Rodnik (Cosmos 2498, Strela 3M), Rumba (part of Cluster quartet, Cluster 2 FM5), SAC-C (Satellite for Scientific Applications), SAC-D (Satellite for Scientific Applications), Salsa (part of Cluster quartet, Cluster 2 FM6), Samba (part of Cluster quartet, Cluster 2 FM7)',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 113.87022399902344},\n",
              "   {'answer': 'SARAL (Satellite with ARGOS and ALTIKA)',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.09368896484375},\n",
              "   {'answer': 'Saudicomsat-1, Saudicomsat-2 , Saudicomsat-3, Saudicomsat-4, Saudicomsat-5, Saudicomsat-6, Saudicomsat-7, Saudisat 1C (Oscar 50, SO 50), Saudisat-2 , Saudisat-3 ',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 114.6158447265625},\n",
              "   {'answer': 'Saudisat-4, SBIRS GEO 1 (Space Based Infrared System Geosynchronous 1, USA 230), SBIRS GEO 2 (Space Based Infrared System Geosynchronous 2, USA 241), SBSS-1 (Space Based Space Surveillance Satellite, SBSS Block 10 SV1, USA 216), SB-WASS 3-1 (Space Based Wide Area Surveillance System) (NOSS 3-1, NOSS C1-1, USA 160), SB-WASS 3-1 (Space Based Wide Area Surveillance System) (NOSS 3-1,USA 160, NOSS C1-2) , SB-WASS 3-2 (Space Based Wide Area Surveillance System) (NOSS 3-2, USA 173, NOSS C2-1) , SB-WASS 3-2 (Space Based Wide Area Surveillance System) (NOSS 3-2, USA 173, NOSS C2-2) ',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 103.5355224609375},\n",
              "   {'answer': 'SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L23) , SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L28) , SB-WASS 3-4 (Space Based Wide Area Surveillance System) NOSS 3-4, USA 194, NRO L30), SB-WASS 3-4 (Space Based Wide Area Surveillance System) NOSS 3-4, USA 194, NRO L30), SB-WASS 3-5 (Space Based Wide Area Surveillance System) NOSS 3-5, USA 229, NRO L34), SB-WASS 3-5 (Space Based Wide Area Surveillance System) NOSS 3-5, USA 229, NRO L34), SB-WASS 3-6 (Space Based Wide Area Surveillance System) NOSS 3-6, USA 238, NRO L36), SB-WASS 3-6 (Space Based Wide Area Surveillance System) NOSS 3-6, USA 238, NRO L36)',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 88.458984375},\n",
              "   {'answer': 'SCD-1 (Satï¿½lite de Coleta de Dados), SCD-2 (Satï¿½lite de Coleta de Dados), Scisat-1 (Atmospheric Chemistry Experiment), SDO (Solar Dynamics Observatory), SDS III-4 (Satellite Data System) (NRO L-1, Nemesis, USA 179) , SDS III-5 (Satellite Data System) (NRO L-24, Scorpius, USA 198) , SDS III-6 (Satellite Data System) NRO L-27, Gryphon, USA 227), SDS III-7 (Satellite Data System) NRO L-38, Drake, USA 236), SDS III-8 (Satellite Data System) NRO L-33, USA 252)',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.65550994873047},\n",
              "   {'answer': 'SDS-4 (Small Demonstration Satellite-4), SEEDS 2, Sentinel 1A, SES-1 (AMC-4R), SES-2, SES-3, SES-4, SES-5 (Sirius 5, Astra 4B), SES-6, SES-7 (Protostar 2, Indostar 2)',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.24575805664062},\n",
              "   {'answer': 'SES-8, Shijian 11-01 (SJ-11-01), Shijian 11-02 (SJ-11-02), Shijian 11-03 (SJ-11-03), Shijian 11-05 (SJ-11-05), Shijian 11-06 (SJ-11-06), Shijian 12 (SJ-12), Shijian 15 (SJ-15)',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.08099365234375},\n",
              "   {'answer': 'Shijian 16 (SJ-16), Shijian 6A (SJ-6A, Dong Fang Hong 60), Shijian 6B (SJ-6B), Shijian 6C (SJ-6-02A), Shijian 6D (SJ-6-02B), Shijian 6E (SJ6-03A, SJ-6E)',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.44735717773438},\n",
              "   {'answer': 'Shijian 6G (SJ6-04A), Shijian 6H (SJ6_04B), Shijian 7 (SJ7, Dong Fang Hong 65), Shijian 9A (SJ 9A), Shijian 9B (SJ 9B), ShindaiSat (Shinshu University Satellite), Shiyan 1 (SY 1, Tansuo 1, Experimental Satellite 1), Shiyan 3 (SY3, Experimental Satellite 3)',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 131.78164672851562},\n",
              "   {'answer': 'Shiyan 4 (SY4, Experimental Satellite 4), Shiyan 5 (SY5, Experimental Satellite 5), Shiyan 7 (SY7, Experimental Satellite 7), Sich 2, Sicral 1A , Sicral 1B, Sina-1 (Sinah 1, ZS1), Sinosat-6 (Chinasat-6A, XN-6), Sirius 1 (SD Radio 1)',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 140.438232421875},\n",
              "   {'answer': 'Sirius 2 (SD Radio 2), Skynet 4C , Skynet 4E , Skynet 4F , Skynet 5A, Skynet 5B',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.10364532470703},\n",
              "   {'answer': 'Skynet 5C, Skynet 5D, SkySat-1, SkySat-2, SkyTerra 1, SLDCOM-3 (Satellite Launch Dispenser Communications System) (USA 119), SMDC-ONE 1.1 (Techsat), SMDC-ONE 1.2 (ORSES [Operationally Responsive Space Enabler Satellite]), SMDC-ONE 2.3, SMDC-ONE 2.4, SMOS (Soil Moisture and Ocean Salinity satellite)',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 116.7040786743164},\n",
              "   {'answer': \"SNaP-3-1ï¿½ï¿½(Space and Missile Defense Command NanoSat Program), SOCRATESï¿½ï¿½(Space Optical Communications Research Advanced TEchnology Satellite), SOHLA 1 (Space Oriented Higashiosaka Leading Association, Maido 1), SORCE (SOlar Radiation and Climate Experiment), Spaceway 3, Spaceway F1, Spaceway F2, Spainsat, Spektr-R/RadioAstron, SPIRALE-A (Systï¿½me Prï¿½paratoire Infra-Rouge pour l'Alerte), SPIRALE-B (Systï¿½me Prï¿½paratoire Infra-Rouge pour l'Alerte)\",\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 130.6749267578125},\n",
              "   {'answer': 'SRMSat (Sri Ramaswamy Memorial Satellite)',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.282958984375},\n",
              "   {'answer': 'STARE-B (Horus [Space-Based Telescopes for Actionable Refinement of Ephemeris]), STPSat-2 (USA 217), STPSat-3 (Space Test Program Satellite-3) , STRaND-1 (Surrey Training, Research and Nanosatellite Demonstrator 1), Strela 3 (Cosmos 2384), Strela 3 (Cosmos 2385), Strela 3 (Cosmos 2386), Strela 3 (Cosmos 2390), Strela 3 (Cosmos 2391)',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.7682113647461},\n",
              "   {'answer': 'Strela 3 (Cosmos 2400), Strela 3 (Cosmos 2401), Strela 3 (Cosmos 2408), Strela 3 (Cosmos 2409), STSat-2C, STSat-3',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 88.2702865600586},\n",
              "   {'answer': 'STUDSat (Student Satellite)',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 132.2935791015625},\n",
              "   {'answer': 'Syracuse 3B (Systeme de Radio Communications Utilisant un Satellite), TacSat 4, TacSat 6, TanDEM-X (TerraSAR-X add-on for Digital Elevation Measurement), Tango (part of Cluster quartet, Cluster 2 FM8), TDRS-10 (Tracking and Data Relay Satellite, TDRS-J), TDRS-11 (Tracking and Data Relay Satellite, TDRS K), TDRS-12 (Tracking and Data Relay Satellite, TDRS L), TDRS-3 (Tracking and Data Relay Satellite, TDRS-C) ',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.87417602539062},\n",
              "   {'answer': 'TDRS-5 (Tracking and Data Relay Satellite, TDRS-E), TDRS-6 (Tracking and Data Relay Satellite, TDRS-F), TDRS-7 (Tracking and Data Relay Satellite, TDRS-G), TDRS-8 (Tracking and Data Relay Satellite, TDRS-H), TDRS-9 (Tracking and Data Relay Satellite, TDRS-I), TechDemoSat-1, TecSAR (Ofeq 8, Polaris), Telkom 1 , Telkom 2',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.7425765991211},\n",
              "   {'answer': 'Telstar 11N, Telstar 12 (Orion 2), Telstar 14R (Estrela do Sul 2), Telstar 18 (Apstar 5), TerraSAR-Xï¿½1 (Terra Synthetic Aperture Radar X-Band), TerraStar 1, TET-1 (Technologieerprobungstrï¿½ger 1, Technology Experiment Carrier), Thaicom-4 (Ipstar 1, Measat 5), Thaicom-5, Thaicom-6',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 132.2769317626953},\n",
              "   {'answer': 'THEMIS A (Time History of Events and Macroscale Interactions during Substorms) , THEMIS D (Time History of Events and Macroscale Interactions during Substorms) , THEMIS E (Time History of Events and Macroscale Interactions during Substorms) , THEOS (Thailand Earth Observation System), Thor-3 , Thor-5 (Thor 2R), Thor-6, Thuraya 2 , Thuraya 3, Tian Xun-1 (TX-1)',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 109.22771453857422},\n",
              "   {'answer': 'Tiangong-1 (TG-1), Tianhui 1-01, Tianhui 1-02, TianLian 1 (TL-1-01, CTDRS), TianLian 2 (TL-1-02, CTDRS), TianLian 3 (TL-1-03, CTDRS), Tiantuo 1, TIGRISat',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 143.26080322265625},\n",
              "   {'answer': 'TIMED (Thermosphere ï¿½ Ionosphere ï¿½ Mesosphere ï¿½ Energetics and Dynamics), TISat-1 (Ticano Satellite), TKSat-1 (Tï¿½pac Katari Satellite 1), TRMM (Tropical Rainfall Measuring Mission), Trumpet 3 (NROL-4, National Reconnaissance Office Launch-4, USA 136), Trumpet 4 (NRO L-67, USA 250), Tselina-2 (Cosmos 2428), TUGSat-1ï¿½ï¿½(Technische Universitï¿½t Graz Satellit, CanX-3b, BRITE-Austria), Turksat 3A, Turksat 4A',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 131.08802795410156},\n",
              "   {'answer': 'UKube-1 (UK Cubesat 1), UFO-10 (USA 146, UHF F/O F10) \"UHF Follow-On\", UFO-11  (USA 174) \"UHF Follow-On\", UFO-2 (USA 95) \"UHF Follow-On\", UFO-4 (USA 108, UFO F4 EHF) \"UHF Follow-On\", UFO-6 (USA 114, UFO F6 EHF) \"UHF Follow-On\", UFO-7 (USA 127, F7 EHF) \"UHF Follow-On\", UFO-8 (USA 138, UHF F/O F8) \"UHF Follow-On\", UK-DMC-2 (BNSCSat-2, British National Science Center Satellite 2), UNIFORM 1ï¿½ï¿½(UNiversity International FORmation Mission 1)',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.4981918334961},\n",
              "   {'answer': 'Unisat-3 (University Satellite 3), Unisat-5 (University Satellite 5), Unisat-6 (University Satellite 6), US-KS Oko 87 (Cosmos-2422), US-KS Oko 89 (Cosmos 2446), UWE-3 (University of Wï¿½rzburg Experimental Satellite 3), Van Allen Probe A (RBSP-A, Radiation Belt Storm Probes), Van Allen Probe B (RBSP-B, Radiation Belt Storm Probes), Velox 1, VeneSat 1 (Simon Bolivar)',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 112.68439483642578},\n",
              "   {'answer': 'Vermont Lunar Cubesat, Vesselsat-1, Vesselsat-2, ViaSat-1, Vinasat 1, Vinasat 2, VNREDSat 1Aï¿½ï¿½(Vietnam Natural Resources Environment and Disaster monitoring small Satellite), VRSS-1 (Venezuelan Remote Sensing Satellite, Francisco Miranda), Wideband Global Satcom 1 (WGS-1, USA 195), Wideband Global Satcom 2 (WGS-2, USA 204)',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 132.03236389160156},\n",
              "   {'answer': 'Wideband Global Satcom 3 (WGS-3, USA 211), Wideband Global Satcom 4 (WGS-4, USA 233), Wideband Global Satcom 5 (WGS-5, USA 243), Wideband Global Satcom 6 (WGS-6, USA 244), WildBlue 1, Wind (International Solar-Terrestrial Program), WNISat-1 Weather News Inc. Satellite 1), Worldview 1, Worldview 2, Wren, X37-B OTV-1 (USA 240), XaTcobeo',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 98.31646728515625},\n",
              "   {'answer': 'Xinyan 1 (XY-1)',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.39542770385742},\n",
              "   {'answer': 'Yahsat-1A (Y1A), Yahsat-1B (Y1B), Yamal-201 , Yamal-202 , Yamal-300K, Yamal-402, Yaogan 10 (Remote Sensing Satellite 10), Yaogan 11 (Remote Sensing Satellite 11), Yaogan 12 (Remote Sensing Satellite 12), Yaogan 13 (Remote Sensing Satellite 13)',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 144.53631591796875},\n",
              "   {'answer': 'Yaogan 14 (Remote Sensing Satellite 14), Yaogan 15 (Remote Sensing Satellite 15), Yaogan 16A (Remote Sensing Satellite 16A, Yaogan Weixing 16), Yaogan 16B (Remote Sensing Satellite 16B), Yaogan 16C (Remote Sensing Satlelite 16C), Yaogan 17A (Remote Sensing Satellite 17A, Yaogan Weixing 17), Yaogan 17B (Remote Sensing Satellite 17B)',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 111.4835205078125},\n",
              "   {'answer': 'Yaogan 17C (Remote Sensing Satellite 17C), Yaogan 18 (Remote Sensing Satellite 18), Yaogan 19 (Remote Sensing Satellite 19), Yaogan 2 (Remote Sensing Satellite 2, Jian Bing 5-2, JB 5-2), Yaogan 3 (Remote Sensing Satellite 3, Jian Bing 5-3, JB 5-3), Yaogan 4 (Remote Sensing Satellite 4), Yaogan 5 (Remote Sensing Satellite 5, JB 5-C, Jian Bing 5-C)',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 116.89741516113281},\n",
              "   {'answer': 'Youthsat',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.92985534667969},\n",
              "   {'answer': 'Zhangguo Ziyuan 2B (ZY-2B, JB-3B), Zhangguo Ziyuan 2C (ZY-2C, JB-3C), Zheda Pixing 1B (ZP-1B, Zhejiang University-1B), Zheda Pixing 1C (ZP-1C, Zhejiang University-1B, Zhongxing 10 (XZ-10, Chinasat 10), Zhongxing 11 (Chinasat 11), Zhongxing 12 (Chinasat 12, ZX-12), Zhongxing 1A (Chinasat 1A, Fenghuo 2)',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 118.4952392578125},\n",
              "   {'answer': 'Zhongxing 20A , Zhongxing 22A (Chinastar 22A), Zhongxing 2A (Chinasat 2A), Zhongxing 9 (Chinasat 9, Chinastar 9), Ziyuan 1-02C, Ziyuan 3 (ZY-3)',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 164.25770568847656}],\n",
              "  'class': 0,\n",
              "  'id': 'que_1',\n",
              "  'question': 'What are the names of the satellites from India ?'},\n",
              " 'que_10': {'answers': [{'answer': 'AVERAGE > ',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -96.5031967163086},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -55.06829833984375},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -47.3326301574707},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -43.04769515991211},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -94.9894790649414},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -48.34467697143555},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -84.66270446777344},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -40.463958740234375},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -36.35161209106445},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -95.91771697998047},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -63.344173431396484},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -16.429534912109375},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -19.779069900512695},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -64.03671264648438},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -97.65653991699219},\n",
              "   {'answer': 'AVERAGE > 7.120000000000001e-05',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.83443069458008},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -76.51033020019531},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -107.41641235351562},\n",
              "   {'answer': 'AVERAGE > 836',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.841991424560547},\n",
              "   {'answer': 'AVERAGE > 7.15e-05',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 123.87979125976562},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.492132186889648},\n",
              "   {'answer': 'AVERAGE > 0.00141',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.92745018005371},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -35.34873962402344},\n",
              "   {'answer': 'AVERAGE > 662',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.15497589111328},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 87.17365264892578},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.401702880859375},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -12.0003080368042},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -30.917930603027344},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -90.66822814941406},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -94.05406188964844},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -68.29704284667969},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.642934799194336},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -30.107112884521484},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -12.461862564086914},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -63.740726470947266},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.540732383728027},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -6.932325839996338},\n",
              "   {'answer': 'AVERAGE > 0.00038500000000000003, 6.42e-05',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 119.76852416992188},\n",
              "   {'answer': 'AVERAGE > 5/29/2007',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.1182632446289},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.353179931640625},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.44167709350586},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.43933868408203},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -76.78079223632812},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -26.28077507019043},\n",
              "   {'answer': 'AVERAGE > 0.000826',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.366005897521973},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -32.31376266479492},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -86.24726867675781},\n",
              "   {'answer': 'AVERAGE > 4.74e-05',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.99017333984375},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -66.1032943725586},\n",
              "   {'answer': 'AVERAGE > 0.000363',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.065407395362854},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -60.08405303955078},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -110.01298522949219},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -40.60738754272461},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -82.44694519042969},\n",
              "   {'answer': 'AVERAGE > 0.9209999999999999',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.82636260986328},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.4476318359375},\n",
              "   {'answer': 'AVERAGE > 779',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 137.3797149658203},\n",
              "   {'answer': 'AVERAGE > 779',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.01422119140625},\n",
              "   {'answer': 'AVERAGE > 779',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.1002197265625},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -74.73530578613281},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.208070755004883},\n",
              "   {'answer': 'AVERAGE > 7e-05',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.438194274902344},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -33.94861602783203},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -13.20296859741211},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -45.666866302490234},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -6.693541526794434},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -94.43167114257812},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -45.01565933227539},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -85.81625366210938},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -61.84017562866211},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -68.8648910522461},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -46.74127960205078},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -36.30332946777344},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -110.81790161132812},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -31.899045944213867},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -60.316650390625},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -108.05415344238281},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.20427322387695},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -84.48257446289062},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -38.442100524902344},\n",
              "   {'answer': 'AVERAGE > 6.979999999999999e-05',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.52954864501953},\n",
              "   {'answer': 'AVERAGE > 0.00014',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.22338604927063},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.310213565826416},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -51.1856689453125},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.704606056213379},\n",
              "   {'answer': 'AVERAGE > 0.00141',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.26031494140625},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -80.84722137451172},\n",
              "   {'answer': 'AVERAGE > 0.000198',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.165220260620117},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -95.40274810791016},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -0.07877922803163528},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -87.34557342529297},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.75237274169922},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -12.745349884033203},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -71.58851623535156},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -32.36789321899414},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -10.050262451171875},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -63.868690490722656},\n",
              "   {'answer': 'AVERAGE > 0.000213',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.563760757446289},\n",
              "   {'answer': 'AVERAGE > 0.00122',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.261558532714844},\n",
              "   {'answer': 'AVERAGE > 0.00411',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.878406524658203},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -66.89612579345703},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -78.15680694580078},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -14.299955368041992},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -15.265942573547363},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -33.62449264526367},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -80.35676574707031},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -11.596566200256348},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -3.1823887825012207},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -59.415321350097656},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -111.62998962402344},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -93.89717102050781},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -15.255120277404785},\n",
              "   {'answer': 'AVERAGE > 7.43e-05',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.86012268066406},\n",
              "   {'answer': 'AVERAGE > 7.159999999999999e-05',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.77780532836914},\n",
              "   {'answer': 'AVERAGE > 1.1900000000000001e-05',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.105316162109375},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -99.95682525634766},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -93.60469818115234},\n",
              "   {'answer': 'AVERAGE > 1.1900000000000001e-05',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.70757293701172},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -75.84337615966797},\n",
              "   {'answer': 'AVERAGE > 496',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.3962984085083008},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -75.81118774414062},\n",
              "   {'answer': 'AVERAGE > 0.0151',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.8287582397460938},\n",
              "   {'answer': 'AVERAGE > 7.269999999999999e-05',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.7662153244018555},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -55.924461364746094},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -107.93405151367188}],\n",
              "  'class': 0,\n",
              "  'id': 'que_10',\n",
              "  'question': 'What is the difference between of the Perigee and Apogee of the Angles satellite ?'},\n",
              " 'que_11': {'answers': [{'answer': '3/13/2004',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.959856033325195},\n",
              "   {'answer': '11/21/2010',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.72135925292969},\n",
              "   {'answer': '6/19/2014',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.77992248535156},\n",
              "   {'answer': '2/21/1989',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.37882480025291443},\n",
              "   {'answer': '5/19/2004',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.628389358520508},\n",
              "   {'answer': '10/22/2000',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.882108688354492},\n",
              "   {'answer': '11/21/2000, 7/18/2004',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.561716079711914},\n",
              "   {'answer': '7/3/1996',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.932069778442383},\n",
              "   {'answer': '4/12/2003',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 71.08718872070312},\n",
              "   {'answer': 'AVERAGE > 5/4/2007',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.02613639831543},\n",
              "   {'answer': '2/6/2014',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.003013610839844},\n",
              "   {'answer': '4/19/2013',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.473409652709961},\n",
              "   {'answer': '8/17/2000',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.155826568603516},\n",
              "   {'answer': '6/30/2014',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.48458862304688},\n",
              "   {'answer': '7/12/2010',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.730049133300781},\n",
              "   {'answer': '11/21/2013',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.130390167236328},\n",
              "   {'answer': 'AVERAGE > 6/2/2010, 7/31/2010',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.141047477722168},\n",
              "   {'answer': '9/18/2012',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.67654037475586},\n",
              "   {'answer': '4/15/2006',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.76097106933594},\n",
              "   {'answer': '4/26/2013',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.3039436340332},\n",
              "   {'answer': '2,120',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.965850353240967},\n",
              "   {'answer': '5/4/2004',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.04461669921875},\n",
              "   {'answer': '10/18/2009',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.393959045410156},\n",
              "   {'answer': '1/1/2000, 7/2/1992',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.725183486938477},\n",
              "   {'answer': '11/21/2013',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.579422950744629},\n",
              "   {'answer': '10/5/1997',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.45279312133789},\n",
              "   {'answer': '12/17/2011',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.57611083984375},\n",
              "   {'answer': '5/7/2013',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.436996459960938},\n",
              "   {'answer': '3/8/2001',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.099684715270996},\n",
              "   {'answer': '11/21/1996',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.77188491821289},\n",
              "   {'answer': '3/11/2006',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.13187789916992},\n",
              "   {'answer': '6/10/2002',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.13382911682129},\n",
              "   {'answer': 'SUM > 11/20/2012',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.41984748840332},\n",
              "   {'answer': '4/3/2012',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.406015396118164},\n",
              "   {'answer': '1/21/1999',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.71810531616211},\n",
              "   {'answer': '6/23/2005',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.21360778808594},\n",
              "   {'answer': '6/28/2007',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.31474685668945},\n",
              "   {'answer': '3/15/1999',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.2039680480957},\n",
              "   {'answer': '5/29/2007',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.79865264892578},\n",
              "   {'answer': '7/13/2011',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.33560943603516},\n",
              "   {'answer': '2/6/2013',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.83142852783203},\n",
              "   {'answer': '12/25/2007',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.65914154052734},\n",
              "   {'answer': '12/14/2009',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.57952117919922},\n",
              "   {'answer': '4/26/2013',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.839508056640625},\n",
              "   {'answer': '9/8/2010',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.7341079711914},\n",
              "   {'answer': '3/17/2002',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.5323429107666},\n",
              "   {'answer': '8/15/2011',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.328893661499023},\n",
              "   {'answer': '9/18/2002',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.271240234375},\n",
              "   {'answer': '4/25/1990',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.29377746582031},\n",
              "   {'answer': '1/27/2013',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.684045791625977},\n",
              "   {'answer': '12/8/2013',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.87324142456055},\n",
              "   {'answer': '5/14/2001',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.05186653137207},\n",
              "   {'answer': '8/19/2012',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.877531051635742},\n",
              "   {'answer': '5/17/1995',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.27531433105469},\n",
              "   {'answer': '4/22/2011',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.1909894943237305},\n",
              "   {'answer': '7/9/1997',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.59870147705078},\n",
              "   {'answer': '9/14/1997',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.75877380371094},\n",
              "   {'answer': '12/20/1997',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.90473937988281},\n",
              "   {'answer': '2/18/1998',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.73868179321289},\n",
              "   {'answer': '4/7/1998',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.341732025146484},\n",
              "   {'answer': '5/5/1997',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.19374084472656},\n",
              "   {'answer': 'AVERAGE > 2/11/2002',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.085670471191406},\n",
              "   {'answer': '5/15/2012',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.55791473388672},\n",
              "   {'answer': '6/19/2014',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.80476379394531},\n",
              "   {'answer': '8/28/2013',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.966442108154297},\n",
              "   {'answer': '8/22/2006',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.506141662597656},\n",
              "   {'answer': '1/9/1990',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.551239013671875},\n",
              "   {'answer': '12/11/2006',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.6031494140625},\n",
              "   {'answer': '7/5/2012',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.113059997558594},\n",
              "   {'answer': 'AVERAGE > 11/6/1995',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.62868595123291},\n",
              "   {'answer': '3/12/2000',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.556644439697266},\n",
              "   {'answer': 'AVERAGE > 7/7/1992',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.47162628173828},\n",
              "   {'answer': '2/21/2014',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.84284210205078},\n",
              "   {'answer': '10/7/1999',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.40973663330078},\n",
              "   {'answer': '11/17/2006',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.45260238647461},\n",
              "   {'answer': '8/17/2000',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.409889221191406},\n",
              "   {'answer': '2/3/2005, 10/2/2000',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.363931655883789},\n",
              "   {'answer': '2/11/2009',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.31358337402344},\n",
              "   {'answer': '9/23/2009',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.422075271606445},\n",
              "   {'answer': '8/21/2009',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.59294128417969},\n",
              "   {'answer': '8/2/1998',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.892799377441406},\n",
              "   {'answer': '12/23/1997',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.8440933227539},\n",
              "   {'answer': '11/19/2013',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.581037521362305},\n",
              "   {'answer': 'AVERAGE > 6/8/2001',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.324462890625},\n",
              "   {'answer': '11/19/2013',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.837499618530273},\n",
              "   {'answer': '11/2/2009',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 87.56878662109375},\n",
              "   {'answer': '6/19/2014',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.975830078125},\n",
              "   {'answer': '2/6/2011',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.51951599121094},\n",
              "   {'answer': '6/25/2013',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.32762908935547},\n",
              "   {'answer': '7/6/2009',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.91553497314453},\n",
              "   {'answer': '5/23/2014',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.66375732421875},\n",
              "   {'answer': '2/25/2013',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.49338912963867},\n",
              "   {'answer': 'Saudi Arabia',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.204803466796875},\n",
              "   {'answer': '6/19/2014',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.60899353027344},\n",
              "   {'answer': '2/3/2005',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.295318603515625},\n",
              "   {'answer': '2/9/1993',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.34406661987305},\n",
              "   {'answer': '4/3/2014',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.551577568054199},\n",
              "   {'answer': '12/3/2013, 7/29/2011',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.825544357299805},\n",
              "   {'answer': '10/25/2013',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 29.487411499023438},\n",
              "   {'answer': '10/6/2010',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.12836456298828},\n",
              "   {'answer': '11/20/2011, 7/19/2013',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.0071988105773926},\n",
              "   {'answer': '9/5/2000',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.89610481262207},\n",
              "   {'answer': '6/12/2008',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.2909164428711},\n",
              "   {'answer': '12/6/2013',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.26078796386719},\n",
              "   {'answer': '5/4/2002',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.08052825927734},\n",
              "   {'answer': '2/25/2013',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.8788466453552246},\n",
              "   {'answer': '8/19/2003',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.99449920654297},\n",
              "   {'answer': '7/12/2010',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.85908508300781},\n",
              "   {'answer': '8/11/2006',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.36860275268555},\n",
              "   {'answer': '8/2/1991',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.25772476196289},\n",
              "   {'answer': '2/26/2009',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.0790901184082},\n",
              "   {'answer': '2/17/2007',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.481929779052734},\n",
              "   {'answer': '9/29/2011, 5/6/2012',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.396296501159668},\n",
              "   {'answer': '12/7/2001',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.925507068634033},\n",
              "   {'answer': '7/8/2014',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.698379516601562},\n",
              "   {'answer': '6/29/2004',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.253814697265625},\n",
              "   {'answer': '1/9/2012',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.685148239135742},\n",
              "   {'answer': '12/6/2009',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.322566986083984},\n",
              "   {'answer': '10/30/2006',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.426565170288086},\n",
              "   {'answer': '4/22/2011',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.869646072387695},\n",
              "   {'answer': 'Yaogan 14 (Remote Sensing Satellite 14)',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.026958465576172},\n",
              "   {'answer': '9/1/2013, 11/20/2013',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.331165313720703},\n",
              "   {'answer': 'Yaogan 6 (Remote Sensing Satellite 6, Jian Bing 7-A)',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.064117908477783},\n",
              "   {'answer': '9/22/2010',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.69738006591797},\n",
              "   {'answer': 'AVERAGE > Zhongxing 20A ',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.032957077026367}],\n",
              "  'class': 0,\n",
              "  'id': 'que_11',\n",
              "  'question': 'When was the ABS-4 satellite purchased ?'},\n",
              " 'que_12': {'answers': [{'answer': 'AVERAGE > 100.42',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.52107238769531},\n",
              "   {'answer': ['', 'percent'], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1446.36',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.82806968688965},\n",
              "   {'answer': ['', 'percent'], 'chunk': 1, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.09',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.7390611171722412},\n",
              "   {'answer': ['', 'percent'], 'chunk': 2, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.09',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.319637298583984},\n",
              "   {'answer': ['', 'percent'], 'chunk': 3, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.789431571960449},\n",
              "   {'answer': ['', 'percent'], 'chunk': 4, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,795',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.914949417114258},\n",
              "   {'answer': [-19447.89, 'percent'], 'chunk': 5, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.5744943618774414},\n",
              "   {'answer': ['', 'percent'], 'chunk': 6, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.3612077236175537},\n",
              "   {'answer': [16.64, 'percent'], 'chunk': 7, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.587906360626221},\n",
              "   {'answer': ['', 'percent'], 'chunk': 8, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,852',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.33724975585938},\n",
              "   {'answer': ['', 'percent'], 'chunk': 9, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,793',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.73616409301758},\n",
              "   {'answer': ['', 'percent'], 'chunk': 10, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -11.147274017333984},\n",
              "   {'answer': ['', 'percent'], 'chunk': 11, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.1',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.8995356559753418},\n",
              "   {'answer': ['', 'percent'], 'chunk': 12, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 100.3',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.1279296875},\n",
              "   {'answer': ['', 'percent'], 'chunk': 13, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3808.92',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.096527099609375},\n",
              "   {'answer': ['', 'percent'], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.15',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.007889747619629},\n",
              "   {'answer': ['', 'percent'], 'chunk': 15, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,799, 35,817',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.447705268859863},\n",
              "   {'answer': ['', 'percent'], 'chunk': 16, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -1.3035433292388916},\n",
              "   {'answer': ['', 'percent'], 'chunk': 17, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -3.587728977203369},\n",
              "   {'answer': ['', 'percent'], 'chunk': 18, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 101.3',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.287779808044434},\n",
              "   {'answer': ['', 'percent'], 'chunk': 19, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.1',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.403606414794922},\n",
              "   {'answer': [5212.63, 'percent'], 'chunk': 20, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.655582427978516},\n",
              "   {'answer': ['', 'percent'], 'chunk': 21, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.12',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.060500144958496},\n",
              "   {'answer': [97.05, 'percent'], 'chunk': 22, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 680',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.35725021362305},\n",
              "   {'answer': ['', 'percent'], 'chunk': 23, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.07',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.673831939697266},\n",
              "   {'answer': ['', 'percent'], 'chunk': 24, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 694',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.603893280029297},\n",
              "   {'answer': ['', 'percent'], 'chunk': 25, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 101.08',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.5540938377380371},\n",
              "   {'answer': [38599.729999999996, 'percent'], 'chunk': 26, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,796, 35,822',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.304133415222168},\n",
              "   {'answer': ['', 'percent'], 'chunk': 27, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -14.348150253295898},\n",
              "   {'answer': ['', 'percent'], 'chunk': 28, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,797',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.955000877380371},\n",
              "   {'answer': ['', 'percent'], 'chunk': 29, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.724199295043945},\n",
              "   {'answer': ['', 'percent'], 'chunk': 30, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Express-AM44',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.91934585571289},\n",
              "   {'answer': ['', 'percent'], 'chunk': 31, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.19',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.038276672363281},\n",
              "   {'answer': ['', 'percent'], 'chunk': 32, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -10.334175109863281},\n",
              "   {'answer': ['', 'percent'], 'chunk': 33, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 35,795',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.81000900268555},\n",
              "   {'answer': ['', 'percent'], 'chunk': 34, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,798',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.1749324798584},\n",
              "   {'answer': ['', 'percent'], 'chunk': 35, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 104,552',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.523399353027344},\n",
              "   {'answer': ['', 'percent'], 'chunk': 36, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1,417',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.412559509277344},\n",
              "   {'answer': ['', 'percent'], 'chunk': 37, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1,414, 1,415',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.858113288879395},\n",
              "   {'answer': ['', 'percent'], 'chunk': 38, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,414',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.019209861755371},\n",
              "   {'answer': ['', 'percent'], 'chunk': 39, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1,418',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.30424690246582},\n",
              "   {'answer': ['', 'percent'], 'chunk': 40, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 680.91',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.784767150878906},\n",
              "   {'answer': ['', 'percent'], 'chunk': 41, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.199471950531006},\n",
              "   {'answer': ['', 'percent'], 'chunk': 42, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 19,173, 19,177, 35,788',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.471704483032227},\n",
              "   {'answer': [-97.98, 'percent'], 'chunk': 43, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1,510',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.1534309387207},\n",
              "   {'answer': ['', 'percent'], 'chunk': 44, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -10.680737495422363},\n",
              "   {'answer': ['', 'percent'], 'chunk': 45, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.08',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.876777648925781},\n",
              "   {'answer': ['', 'percent'], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.11, 1435.95',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.515825271606445},\n",
              "   {'answer': ['', 'percent'], 'chunk': 47, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 96.9, 1436.14',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.2344720363616943},\n",
              "   {'answer': ['', 'percent'], 'chunk': 48, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.08, 1436.1',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.508967399597168},\n",
              "   {'answer': ['', 'percent'], 'chunk': 49, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.382849216461182},\n",
              "   {'answer': ['', 'percent'], 'chunk': 50, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,795',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.1584062576293945},\n",
              "   {'answer': ['', 'percent'], 'chunk': 51, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Intelsat 21 (IS-21)',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.047513961791992},\n",
              "   {'answer': ['', 'percent'], 'chunk': 52, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 35,795',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.632429122924805},\n",
              "   {'answer': ['', 'percent'], 'chunk': 53, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1439.15',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.61560821533203},\n",
              "   {'answer': [-99.96000000000001, 'percent'], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.4, 100.4',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.263256072998047},\n",
              "   {'answer': ['', 'percent'], 'chunk': 55, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.4',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.451026916503906},\n",
              "   {'answer': ['', 'percent'], 'chunk': 56, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.4',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.97692108154297},\n",
              "   {'answer': ['', 'percent'], 'chunk': 57, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.4',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.04707145690918},\n",
              "   {'answer': ['', 'percent'], 'chunk': 58, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.4',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.202810287475586},\n",
              "   {'answer': ['', 'percent'], 'chunk': 59, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.4',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.73958969116211},\n",
              "   {'answer': ['', 'percent'], 'chunk': 60, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.14',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.416091918945312},\n",
              "   {'answer': ['', 'percent'], 'chunk': 61, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -12.456745147705078},\n",
              "   {'answer': ['', 'percent'], 'chunk': 62, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.08',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.7246527671813965},\n",
              "   {'answer': [-30.72, 'percent'], 'chunk': 63, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.3260629177093506},\n",
              "   {'answer': ['', 'percent'], 'chunk': 64, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.12',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.987739562988281},\n",
              "   {'answer': ['', 'percent'], 'chunk': 65, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -17.473552703857422},\n",
              "   {'answer': [566666666.67, 'percent'], 'chunk': 66, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,794',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.446136474609375},\n",
              "   {'answer': ['', 'percent'], 'chunk': 67, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -4.983577728271484},\n",
              "   {'answer': ['', 'percent'], 'chunk': 68, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -7.654067516326904},\n",
              "   {'answer': [-99.99, 'percent'], 'chunk': 69, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.676048278808594},\n",
              "   {'answer': ['', 'percent'], 'chunk': 70, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 729.18',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.655433654785156},\n",
              "   {'answer': [11579416.13, 'percent'], 'chunk': 71, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 729.39',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.3587703704834},\n",
              "   {'answer': ['', 'percent'], 'chunk': 72, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 724.28',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.455455780029297},\n",
              "   {'answer': [158.22, 'percent'], 'chunk': 73, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 722.19',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.359737396240234},\n",
              "   {'answer': ['', 'percent'], 'chunk': 74, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -1.079390525817871},\n",
              "   {'answer': ['', 'percent'], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.15',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.997419357299805},\n",
              "   {'answer': ['', 'percent'], 'chunk': 76, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.17',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.065650939941406},\n",
              "   {'answer': ['', 'percent'], 'chunk': 77, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.12',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.33156967163086},\n",
              "   {'answer': ['', 'percent'], 'chunk': 78, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -0.3392925262451172},\n",
              "   {'answer': ['', 'percent'], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 795, 792, 794, 792, 796, 795, 795',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.91747283935547},\n",
              "   {'answer': ['', 'percent'], 'chunk': 80, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 100.3, 100.3, 100.3, 100.3, 100.3',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.803325653076172},\n",
              "   {'answer': ['', 'percent'], 'chunk': 81, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 99.5, 1436.1, 104.9',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.810335636138916},\n",
              "   {'answer': [-57.19, 'percent'], 'chunk': 82, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -1.652871012687683},\n",
              "   {'answer': ['', 'percent'], 'chunk': 83, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 99.3',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.345672607421875},\n",
              "   {'answer': ['', 'percent'], 'chunk': 84, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 101.21',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.922901153564453},\n",
              "   {'answer': ['', 'percent'], 'chunk': 85, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.72033953666687},\n",
              "   {'answer': ['', 'percent'], 'chunk': 86, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 109.42, 98.4, 1436.1',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.261722564697266},\n",
              "   {'answer': [-14.29, 'percent'], 'chunk': 87, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1,511',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.982682228088379},\n",
              "   {'answer': ['', 'percent'], 'chunk': 88, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1,509',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.271638870239258},\n",
              "   {'answer': ['', 'percent'], 'chunk': 89, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3431.1',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.33916091918945},\n",
              "   {'answer': ['', 'percent'], 'chunk': 90, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.032225131988525},\n",
              "   {'answer': ['', 'percent'], 'chunk': 91, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > Saudisat-3 ',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.4703218936920166},\n",
              "   {'answer': ['', 'percent'], 'chunk': 92, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -9.172405242919922},\n",
              "   {'answer': [2624.03, 'percent'], 'chunk': 93, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 107.4',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.277928113937378},\n",
              "   {'answer': [-40.97, 'percent'], 'chunk': 94, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -1.8805131912231445},\n",
              "   {'answer': ['', 'percent'], 'chunk': 95, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.1',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.929304122924805},\n",
              "   {'answer': ['', 'percent'], 'chunk': 96, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.07',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.652385711669922},\n",
              "   {'answer': [-100.0, 'percent'], 'chunk': 97, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 96.85',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.342041015625},\n",
              "   {'answer': ['', 'percent'], 'chunk': 98, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 100.8',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.49225997924805},\n",
              "   {'answer': ['', 'percent'], 'chunk': 99, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.52',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.606315612792969},\n",
              "   {'answer': ['', 'percent'], 'chunk': 100, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.16',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.387484073638916},\n",
              "   {'answer': ['', 'percent'], 'chunk': 101, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.24',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.4641227722168},\n",
              "   {'answer': ['', 'percent'], 'chunk': 102, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 8 days',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.40750449895858765},\n",
              "   {'answer': ['', 'percent'], 'chunk': 103, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1437.78',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.19641637802124},\n",
              "   {'answer': ['', 'percent'], 'chunk': 104, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 115.7',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.063148498535156},\n",
              "   {'answer': ['', 'percent'], 'chunk': 105, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 115.7, 115.5, 115.6',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 29.862396240234375},\n",
              "   {'answer': ['', 'percent'], 'chunk': 106, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -16.531923294067383},\n",
              "   {'answer': ['', 'percent'], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,442.00',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.260608673095703},\n",
              "   {'answer': [-692014.89, 'percent'], 'chunk': 108, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -1.5405584573745728},\n",
              "   {'answer': [121.30000000000001, 'percent'], 'chunk': 109, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.084592580795288},\n",
              "   {'answer': ['', 'percent'], 'chunk': 110, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 87,304, 826, 475',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.81987762451172},\n",
              "   {'answer': [-48.05, 'percent'], 'chunk': 111, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -5.628833770751953},\n",
              "   {'answer': ['', 'percent'], 'chunk': 112, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -0.8026673793792725},\n",
              "   {'answer': ['', 'percent'], 'chunk': 113, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -4.392179012298584},\n",
              "   {'answer': ['', 'percent'], 'chunk': 114, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.28721809387207},\n",
              "   {'answer': [-28.57, 'percent'], 'chunk': 115, 'model': 'TagOp'},\n",
              "   {'answer': 'COUNT > 1436.09, 1436.12',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.44138240814209},\n",
              "   {'answer': ['', 'percent'], 'chunk': 116, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 19700.45',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.149690628051758},\n",
              "   {'answer': [-0.0, 'percent'], 'chunk': 117, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,872.15',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.519775390625},\n",
              "   {'answer': ['', 'percent'], 'chunk': 118, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.11',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.60723876953125},\n",
              "   {'answer': ['', 'percent'], 'chunk': 119, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 109.5',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.370058059692383},\n",
              "   {'answer': ['', 'percent'], 'chunk': 120, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 105.14',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.666934013366699},\n",
              "   {'answer': ['', 'percent'], 'chunk': 121, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 115.8',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.081905841827393},\n",
              "   {'answer': [368469287.76, 'percent'], 'chunk': 122, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -8.418643951416016},\n",
              "   {'answer': ['', 'percent'], 'chunk': 123, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1436.1',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.0744571685791},\n",
              "   {'answer': ['', 'percent'], 'chunk': 124, 'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_12',\n",
              "  'question': 'How much percent larger is the Period of AAUSAT-3 to AcrimSAT ?'},\n",
              " 'que_13': {'answers': [{'answer': 'COUNT > 35,778, 35,769, 35,780, 35,777, 35,780, 670, 35,589, 35,714',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.06773376464844},\n",
              "   {'answer': 'COUNT > 35,500, 35,771, 35,872, 35,700, 35,700',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.73229217529297},\n",
              "   {'answer': 'COUNT > 35,764, 35,770, 510, 570, 558',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 107.9826431274414},\n",
              "   {'answer': 'COUNT > 443, 35,786, 672, 35,776, 35,767, 35,781, 35,784, 35,772, 35,775',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 157.57839965820312},\n",
              "   {'answer': 'COUNT > AMC-11 (Americom-11, GE 11), AMC-15 (Americom-15), AMC-16 (Americom-16), AMC-18 (Americom 18), AMC-2 (Americom 2, GE-2), AMC-21 (Americom 21), AMC-3 (Americom 3, GE-3), AMC-4 (Americom-4, GE-4), AMC-5 (Americom-5, GE-5)',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 189.00021362304688},\n",
              "   {'answer': 'COUNT > 35,778, 35,773, 35,773, 35,773, 35,778, 1,440, 35,900',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 102.80175018310547},\n",
              "   {'answer': 'COUNT > 35,778, 35,777, 35,781, 35,778, 35,773',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 131.1940460205078},\n",
              "   {'answer': 'COUNT > 35,782, 35,779, 35,778, 35,864, 35,766, 35,778, 35,771, 35,780',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 124.00135803222656},\n",
              "   {'answer': 'COUNT > 35,784, 35,782',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 86.06277465820312},\n",
              "   {'answer': 'COUNT > 35,772, 35,804, 35,715, 35,776, 35,772, 35,768, 35,770, 35,781, 35,777, 35,770',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 172.33518981933594},\n",
              "   {'answer': 'COUNT > 35,768, 35,787, 35,784, 583, 35,778, 984, 35,780, 35,834, 35,605, 35,768',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 171.87570190429688},\n",
              "   {'answer': 'COUNT > 554, 681, 500, 503, 35,777, 35,776, 35,769',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.73770141601562},\n",
              "   {'answer': 'COUNT > 35,772, 35,778, 33,066, 35,784',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 71.56875610351562},\n",
              "   {'answer': 'COUNT > 642, 643, 611, 618, 632, 624',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 107.24569702148438},\n",
              "   {'answer': 'COUNT > 622, 325, 538, 9,999, 35,777',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 101.75633239746094},\n",
              "   {'answer': 'COUNT > 35,782, 702, 35,768, 35,708, 35,776',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.68487548828125},\n",
              "   {'answer': 'COUNT > 35,774, 35,764, 35,670, 35,775, 35,717, 35,693, 35,708, 21,519',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 160.26832580566406},\n",
              "   {'answer': 'COUNT > 21,460, 21,452, 21,462, 21,477, 35,783, 35,773, 35,780',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.64739990234375},\n",
              "   {'answer': 'COUNT > 764, 769, 669, 751, 772, 622, 622, 622',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 172.31515502929688},\n",
              "   {'answer': 'COUNT > 622, 713, 630, 594, 815, 678, 443',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.20553588867188},\n",
              "   {'answer': 'COUNT > 630, 630, 326, 661, 597, 35,785, 35,786, 35,785, 35,778',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.41519165039062},\n",
              "   {'answer': 'COUNT > 35,780, 35,773, 35,774, 35,777, 35,774',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 114.05349731445312},\n",
              "   {'answer': 'COUNT > 842, 839, 35,775, 35,771, 35,760, 35,757',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 101.7861328125},\n",
              "   {'answer': 'COUNT > 35,706, 35,774, 35,718, 35,756, 35,784, 35,780, 35,897, 35,765, 35,741, 662',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 155.88153076171875},\n",
              "   {'answer': 'COUNT > 585, 626, 596, 35,776, 35,780, 35,778, 35,782, 35,785, 35,789, 35,776, 35,781',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.56727600097656},\n",
              "   {'answer': 'COUNT > 35,783, 35,775, 35,782, 35,783, 35,775, 35,773, 675',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.6949691772461},\n",
              "   {'answer': 'COUNT > 677, 702, 702, 701',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.70765686035156},\n",
              "   {'answer': 'COUNT > 35,768, 35,779, 35,762, 35,754, 35,772',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.51974487304688},\n",
              "   {'answer': 'COUNT > 35,780, 35,788, 35,781, 35,766, 35,786',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 137.63233947753906},\n",
              "   {'answer': 'COUNT > 35,757, 35,777, 35,770, 35,769, 35,774, 35,780, 35,769, 35,766, 35,756',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.2496795654297},\n",
              "   {'answer': 'COUNT > 4,100, 4,892, 4,900, 4,880, 6,150, 2,600',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 107.65349578857422},\n",
              "   {'answer': 'COUNT > Express-A1R (Express 4A, Ekspress-A No. 4), Express-AM2, Express-AM3, Express-AM33, Express-AM44, Express-AM5, Express-AT1, Express-AT2',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 101.95481872558594},\n",
              "   {'answer': 'COUNT > 35,777, 35,767, 35,760, 820, 827, 827, 537',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 116.64231872558594},\n",
              "   {'answer': 'COUNT > 35,745, 890, 790',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.00068664550781},\n",
              "   {'answer': 'COUNT > 35,785, 35,775, 35,781, 35,776, 35,776, 35,775, 35,774, 35,774, 35,776, 35,771, 35,776',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 169.54148864746094},\n",
              "   {'answer': 'COUNT > 35,775, 35,784, 23,242, 23,240, 23,214, 23,217, 630, 35,782, 35,774, 593',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 130.207275390625},\n",
              "   {'answer': 'COUNT > 508, 671, 3,905, 701, 1,407, 1,413, 1,406, 1,412, 1,412, 900, 1,477',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 142.54583740234375},\n",
              "   {'answer': 'COUNT > 1,411, 1,411, 1,415, 1,413, 1,413, 1,413, 1,411, 1,412, 1,413, 1,411, 1,411, 1,412, 1,412',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.2563018798828},\n",
              "   {'answer': 'COUNT > 1,412, 1,413, 1,413, 1,413, 1,412, 1,413, 1,412, 1,413, 1,413, 1,412, 1,413',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 174.07518005371094},\n",
              "   {'answer': 'COUNT > 1,413, 1,413, 1,413, 1,413, 1,412, 1,459, 1,413, 1,413, 1,413, 1,360, 915',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 161.9130096435547},\n",
              "   {'answer': 'COUNT > 1,370, 19,116, 19,134, 19,118, 19,130, 19,130, 19,130, 19,094',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 107.74224853515625},\n",
              "   {'answer': 'COUNT > 64.66, 64.65, 64.67, 64.79, 64.79, 64.82, 64.83, 64.79, 64.81, 64.76',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.09828186035156},\n",
              "   {'answer': 'COUNT > 64.78, 64.8, 64.77, 64.82, 64.84, 64.84, 64.82, 64.8, 64.8, 64.78, 64.82',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 148.86349487304688},\n",
              "   {'answer': 'COUNT > 19,087, 19,084, 19,114, 35,768, 35,170, 35,786',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.64531707763672},\n",
              "   {'answer': 'COUNT > 1,497, 1,478, 1,482, 1,480, 1,493, 1,494, 1,479, 1,480, 1,478, 392',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.68502807617188},\n",
              "   {'answer': 'COUNT > 668, 35,777, 35,761, 35,763, 35,776, 35,757, 35,782',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.72471618652344},\n",
              "   {'answer': 'COUNT > Haiyang 2A (HY 2A), HAMSat (VUSat Oscar 52), Helios 2A , Helios 2B, HellasSat 2 (Intelsat K-TV, NSS K-TV), HESSI (RHESSI, Reuven Ramaty High Energy Solar Spectroscopic Imager), Hinode (Solar B), Hisaki (Sprint A, Spectroscopic Planet Observatory for Recognition of Interaction of Atmosphere), Hispasat 1C ',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 195.7706298828125},\n",
              "   {'answer': 'COUNT > 35,764, 35,782, 628, 621, 489, 613, 613, 35,777',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.06292724609375},\n",
              "   {'answer': 'COUNT > 555, 590, 35,770, 35,775',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.59391784667969},\n",
              "   {'answer': 'COUNT > 509, 678, 1,111, 1,112, 35,764, 35,763, 35,609, 35,769, 35,772',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.57138061523438},\n",
              "   {'answer': 'COUNT > 35,773, 35,775, 35,784',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 136.1779022216797},\n",
              "   {'answer': 'COUNT > 35,776, 35,783, 35,778, 35,771, 35,776, 35,783, 35,770, 35,775, 35,768, 35,785, 35,780',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 149.03684997558594},\n",
              "   {'answer': 'COUNT > 35,774, 35,781, 35,781, 35,778, 35,779, 35,778, 35,775, 35,776',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 127.84049224853516},\n",
              "   {'answer': 'COUNT > 35,776, 35,775, 35,782, 35,775, 35,775, 35,773, 35,782, 35,768, 35,775',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 125.7868423461914},\n",
              "   {'answer': 'COUNT > 35,800, 7,000, 776, 776, 776, 776, 708, 776',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 143.81373596191406},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 185.30484008789062},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 190.73696899414062},\n",
              "   {'answer': 'COUNT > 776, 776, 776, 776, 776, 776, 776, 776, 776, 776',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 168.53346252441406},\n",
              "   {'answer': 'COUNT > 776, 776, 776, 776, 776, 776, 776, 776, 776, 776, 776',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 176.3822021484375},\n",
              "   {'answer': 'COUNT > 776, 776, 776, 776, 776, 776, 747, 776, 776, 776',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 181.9208526611328},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 183.99488830566406},\n",
              "   {'answer': 'COUNT > 776, 745, 777, 748, 620, 35,712, 35,695, 817, 710, 1,332',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 167.46728515625},\n",
              "   {'answer': 'COUNT > 35,772, 35,784, 35,780, 35,778, 35,788, 33,782, 35,776, 35,780',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 169.20880126953125},\n",
              "   {'answer': 'COUNT > Kalpana-1 (Metsat-1), KazEOSat-1 (Kazcosmos Earth Observation Satellite), KazEOSat-2 (kazcosmos Earth Observation Satellite), KazSat-2, KazSat-3, Keyhole 3 (Advanced KH-11, KH-12-4, Advanced Keyhole, Misty-2, EIS-1, 8X Enhanced Imaging System, USA 144), Keyhole 4 (Advanced KH-11, Advanced Keyhole, Improved Crystal, EIS-2, 8X Enhanced Imaging System, NROL 14, USA 161), Keyhole 5 (Advanced KH-11, KH-12-5, Improved Crystal, EIS-3, USA 186), Keyhole 6 (NRO L49, Advanced KH-11, KH-12-6, Improved Crystal, USA 224)',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 175.52792358398438},\n",
              "   {'answer': 'COUNT > 257, 35,778, 35,776, 206, 676, 679, 535, 497',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 162.8331298828125},\n",
              "   {'answer': 'COUNT > 35,780, 35,780, 671, 574, 713, 702, 700',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.88053894042969},\n",
              "   {'answer': 'COUNT > 35,760, 900, 35,778, 35,772, 35,767, 984, 35,782',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 129.59120178222656},\n",
              "   {'answer': 'COUNT > 35,778, 35,728, 853, 35,598, 33,674, 35,500, 961, 964, 818',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 113.56531524658203},\n",
              "   {'answer': 'COUNT > 35,784, 35,770, 35,781, 35,775, 820, 819, 35,784, 35,754',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 171.07679748535156},\n",
              "   {'answer': 'COUNT > 35,765, 35,764, 35,753, 35,768, 1,481',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 117.17534637451172},\n",
              "   {'answer': 'COUNT > 500, 35,778, 35,776, 35,800, 35,800, 654, 583, 35,775, 19,781',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 121.49141693115234},\n",
              "   {'answer': 'COUNT > 19,959, 20,120, 20,104, 19,986, 20,080, 20,134, 19,912, 20,109, 20,188, 20,451',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 162.2864990234375},\n",
              "   {'answer': 'COUNT > 20,174, 20,184, 20,457, 20,174, 19,963, 20,089, 20,072, 19,938, 20,123',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.7158966064453},\n",
              "   {'answer': 'COUNT > 20,096, 20,133, 20,184, 20,177, 20,104, 20,155, 20,063, 20,142, 20,020',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.03515625},\n",
              "   {'answer': 'COUNT > 20,206, 20,149, 20,150, 20,135, 20,160, 772, 398, 35,793, 691, 661',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 154.21246337890625},\n",
              "   {'answer': 'COUNT > 35,764, 35,795, 35,779, 35,779, 35,700, 35,764, 800, 843, 845, 827',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 135.9821014404297},\n",
              "   {'answer': 'COUNT > 35,778, 35,776, 35,776, 35,776, 35,775, 35,773, 35,779',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.8489532470703},\n",
              "   {'answer': 'COUNT > 35,784, 35,772, 613, 624, 8,063, 8,060, 8,062, 8,064, 7,831, 7,827',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 147.45497131347656},\n",
              "   {'answer': 'COUNT > 722, 701, 569, 594, 35,773, 35,778',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 125.26275634765625},\n",
              "   {'answer': 'COUNT > 35,774, 35,774, 35,775, 770, 769, 770, 788, 789, 788, 789',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 151.38453674316406},\n",
              "   {'answer': 'COUNT > 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 162.92898559570312},\n",
              "   {'answer': 'COUNT > 774, 774, 774, 778, 777, 741, 741, 740, 739, 739',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 127.67463684082031},\n",
              "   {'answer': 'COUNT > 396, 499, 635, 35,786, 35,784, 35,780, 692, 967',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.67405700683594},\n",
              "   {'answer': 'COUNT > 1,011, 1,016, 1,016, 1,008, 968, 1,011, 945, 1,022, 733',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.55413818359375},\n",
              "   {'answer': 'COUNT > 249, 725, 697, 678, 553',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.76689910888672},\n",
              "   {'answer': 'COUNT > 708',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 130.3185577392578},\n",
              "   {'answer': 'COUNT > 605, 607, 35,774, 32,618, 791, 35,612, 35,784, 35,777',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 123.69075775146484},\n",
              "   {'answer': 'COUNT > 1,199, 613, 621, 621, 621, 617, 667, 35,786, 663, 595, 623',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 160.9849853515625},\n",
              "   {'answer': 'COUNT > 817, 564, 459, 538, 415, 1,436, 1,480, 1,477, 1,478',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 156.98033142089844},\n",
              "   {'answer': 'COUNT > 1,498, 1,482, 1,495, 1,493, 1,484, 1,481, 1,473, 1,473, 1,475, 1,478, 1,477, 1,482',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 152.74298095703125},\n",
              "   {'answer': 'COUNT > 1,480, 1,478, 1,477, 17,240, 702, 653, 16,809, 17,007',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 161.86965942382812},\n",
              "   {'answer': 'COUNT > 772, 782, 468, 470, 473, 448, 474, 35,777, 35,779, 35,776, 35,779',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 144.07244873046875},\n",
              "   {'answer': 'COUNT > 697, 696, 652, 649, 651, 648, 650, 696, 656',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.16061401367188},\n",
              "   {'answer': 'COUNT > Saudisat-4, SBIRS GEO 1 (Space Based Infrared System Geosynchronous 1, USA 230), SBIRS GEO 2 (Space Based Infrared System Geosynchronous 2, USA 241), SBSS-1 (Space Based Space Surveillance Satellite, SBSS Block 10 SV1, USA 216), SB-WASS 3-1 (Space Based Wide Area Surveillance System) (NOSS 3-1, NOSS C1-1, USA 160), SB-WASS 3-1 (Space Based Wide Area Surveillance System) (NOSS 3-1,USA 160, NOSS C1-2) , SB-WASS 3-2 (Space Based Wide Area Surveillance System) (NOSS 3-2, USA 173, NOSS C2-1) , SB-WASS 3-2 (Space Based Wide Area Surveillance System) (NOSS 3-2, USA 173, NOSS C2-2) ',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 187.2193603515625},\n",
              "   {'answer': 'COUNT > SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L23) , SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L28) , SB-WASS 3-4 (Space Based Wide Area Surveillance System) NOSS 3-4, USA 194, NRO L30), SB-WASS 3-4 (Space Based Wide Area Surveillance System) NOSS 3-4, USA 194, NRO L30), SB-WASS 3-5 (Space Based Wide Area Surveillance System) NOSS 3-5, USA 229, NRO L34), SB-WASS 3-6 (Space Based Wide Area Surveillance System) NOSS 3-6, USA 238, NRO L36), SB-WASS 3-6 (Space Based Wide Area Surveillance System) NOSS 3-6, USA 238, NRO L36)',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 147.0289306640625},\n",
              "   {'answer': 'COUNT > 25.0, 73.9, 28.0',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.93560791015625},\n",
              "   {'answer': 'COUNT > 661, 691, 35,778, 35,850, 35,783, 35,757, 35,783',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 114.29203033447266},\n",
              "   {'answer': 'COUNT > 35,777',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 143.15237426757812},\n",
              "   {'answer': 'COUNT > 599, 579, 587, 591, 594, 581, 578',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.71928405761719},\n",
              "   {'answer': 'COUNT > 534, 384, 586, 785',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.779930114746094},\n",
              "   {'answer': 'COUNT > 784, 739, 668, 684, 35,769, 35,788, 681, 35,794, 23,783',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 142.07650756835938},\n",
              "   {'answer': 'COUNT > 12,849, 6,179, 35,776, 35,774, 35,785, 35,779, 35,775, 35,774, 35,773, 35,771',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 136.8900604248047},\n",
              "   {'answer': 'COUNT > 35,770, 35,786, 626, 35,782',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.09906768798828},\n",
              "   {'answer': 'COUNT > 618, 657, 600, 35,785, 35,785, 35,785, 35,745',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.97462463378906},\n",
              "   {'answer': 'COUNT > 824, 696, 643, 622, 35,783, 35,703, 35,776, 35,791',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 119.35145568847656},\n",
              "   {'answer': 'COUNT > STPSat-2 (USA 217), STPSat-3 (Space Test Program Satellite-3) , Strela 3 (Cosmos 2384), Strela 3 (Cosmos 2385), Strela 3 (Cosmos 2386), Strela 3 (Cosmos 2390), Strela 3 (Cosmos 2391)',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 124.05242919921875},\n",
              "   {'answer': 'COUNT > 1,467, 1,465, 1,471, 1,474, 298, 593, 1,347, 1,339',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.27883911132812},\n",
              "   {'answer': 'COUNT > Superbird 7 (Superbird C2), Superbird-B2 (Superbird 4), Superbird-C , Suzaku (Astro E2), SWARM-A, SWARM-B, SWARM-C, Swift ',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.98570251464844},\n",
              "   {'answer': 'COUNT > 1436.09, 235.16, 94.8, 3,442.00, 1436.04, 1436.18, 1435.88, 1436.06',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 154.29385375976562},\n",
              "   {'answer': 'COUNT > 35,773, 35,773, 35,767, 35,768, 35,768, 35,778, 35,781',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.49853515625},\n",
              "   {'answer': 'COUNT > 35,774, 35,773, 35,783, 35,778, 35,783, 35,775, 35,777, 35,786',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 135.87704467773438},\n",
              "   {'answer': 'COUNT > 461, 467, 474, 824, 35,777, 35,773, 35,779, 35,764, 35,766, 464',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 160.50225830078125},\n",
              "   {'answer': 'COUNT > Tiangong-1 (TG-1), Tianhui 1-01, Tianhui 1-02, TianLian 1 (TL-1-01, CTDRS), TianLian 2 (TL-1-02, CTDRS), TianLian 3 (TL-1-03, CTDRS)',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 119.04288482666016},\n",
              "   {'answer': 'COUNT > 612, 611, 35,773, 400, 1,210, 35,500, 844, 35,778, 35,776',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 120.36465454101562},\n",
              "   {'answer': 'COUNT > 35,738, 35,786, 35,766, 35,763, 35,744, 35,738, 35,772, 660, 622',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.63760375976562},\n",
              "   {'answer': 'COUNT > 613, 542, 667, 591, 595, 35,777',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.69886779785156},\n",
              "   {'answer': 'COUNT > 494, 847, 479, 35,775, 35,782, 35,742, 682, 622, 35,783, 35,786',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 157.48489379882812},\n",
              "   {'answer': 'COUNT > 35,785, 35,785, 35,785, 35,785, 35,776, 186',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.72306823730469},\n",
              "   {'answer': 'COUNT > 35,782, 35,784, 35,784, 35,785, 7,079',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 144.774658203125},\n",
              "   {'answer': 'COUNT > 35,743, 35,773, 35,774, 35,694, 35,784',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.82395935058594},\n",
              "   {'answer': 'COUNT > 1,201',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 102.53626251220703},\n",
              "   {'answer': 'COUNT > 890, 1,201, 633, 625, 635, 440',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 126.07656860351562},\n",
              "   {'answer': 'COUNT > 511, 628, 1,192, 802, 1,479, 594',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 94.41709899902344},\n",
              "   {'answer': 'COUNT > 444, 35,775, 35,775, 35,759, 35,778',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 138.457275390625},\n",
              "   {'answer': 'COUNT > 35,773, 35,767, 35,781, 763, 500',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 153.51223754882812}],\n",
              "  'class': 0,\n",
              "  'id': 'que_13',\n",
              "  'question': 'How many times is the launch mass of Eutelsat 12 larger than Eutelsat 16A ?'},\n",
              " 'que_2': {'answers': [{'answer': '15 yrs., 15 yrs.',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.244131088256836},\n",
              "   {'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': '14 yrs.',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.47210693359375},\n",
              "   {'answer': [[], ''], 'chunk': 1, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.40413284301758},\n",
              "   {'answer': [[], ''], 'chunk': 2, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.9283561706543},\n",
              "   {'answer': [[], ''], 'chunk': 3, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.9004020690918},\n",
              "   {'answer': [[], ''], 'chunk': 4, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.3411378860473633},\n",
              "   {'answer': [[], ''], 'chunk': 5, 'model': 'TagOp'},\n",
              "   {'answer': 'nan, nan',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.037434101104736},\n",
              "   {'answer': [[], ''], 'chunk': 6, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.945200443267822},\n",
              "   {'answer': [[], ''], 'chunk': 7, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.62285041809082},\n",
              "   {'answer': [[], ''], 'chunk': 8, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -11.230125427246094},\n",
              "   {'answer': [[], ''], 'chunk': 9, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.75926971435547},\n",
              "   {'answer': [[], ''], 'chunk': 10, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs.',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.30565071105957},\n",
              "   {'answer': [[], ''], 'chunk': 11, 'model': 'TagOp'},\n",
              "   {'answer': '13 yrs.',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.098705768585205},\n",
              "   {'answer': [[], ''], 'chunk': 12, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.63916778564453},\n",
              "   {'answer': [[], ''], 'chunk': 13, 'model': 'TagOp'},\n",
              "   {'answer': '2 yrs., nan',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.1203956604003906},\n",
              "   {'answer': [[], ''], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.45402145385742},\n",
              "   {'answer': [[], ''], 'chunk': 15, 'model': 'TagOp'},\n",
              "   {'answer': '8 yrs.',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 77.9498519897461},\n",
              "   {'answer': [[], ''], 'chunk': 16, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.003542423248291},\n",
              "   {'answer': [[], ''], 'chunk': 17, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.28818416595459},\n",
              "   {'answer': [[], ''], 'chunk': 18, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.077762603759766},\n",
              "   {'answer': [[], ''], 'chunk': 19, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.598907470703125},\n",
              "   {'answer': [[], ''], 'chunk': 20, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -4.769407272338867},\n",
              "   {'answer': [[], ''], 'chunk': 21, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.029559135437012},\n",
              "   {'answer': [[], ''], 'chunk': 22, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -9.432374000549316},\n",
              "   {'answer': [[], ''], 'chunk': 23, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.98844909667969},\n",
              "   {'answer': [[], ''], 'chunk': 24, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs., 15 yrs.',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.687076091766357},\n",
              "   {'answer': [[], ''], 'chunk': 25, 'model': 'TagOp'},\n",
              "   {'answer': '6 yrs.',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.780099868774414},\n",
              "   {'answer': [[], ''], 'chunk': 26, 'model': 'TagOp'},\n",
              "   {'answer': '18 yrs.',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.601125240325928},\n",
              "   {'answer': [[], ''], 'chunk': 27, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs.',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.014777183532715},\n",
              "   {'answer': [[], ''], 'chunk': 28, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs.',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.869752407073975},\n",
              "   {'answer': [[], ''], 'chunk': 29, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.084754943847656},\n",
              "   {'answer': [[], ''], 'chunk': 30, 'model': 'TagOp'},\n",
              "   {'answer': '7-10 yrs., 12 yrs.',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.823998212814331},\n",
              "   {'answer': [[], ''], 'chunk': 31, 'model': 'TagOp'},\n",
              "   {'answer': 'nan', 'chunk': 32, 'model': 'TaPaS', 'score': 4.90548849105835},\n",
              "   {'answer': [[], ''], 'chunk': 32, 'model': 'TagOp'},\n",
              "   {'answer': '3 yrs.',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.12370681762695},\n",
              "   {'answer': [[], ''], 'chunk': 33, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -3.771939992904663},\n",
              "   {'answer': [[], ''], 'chunk': 34, 'model': 'TagOp'},\n",
              "   {'answer': '13 trs,',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.855061531066895},\n",
              "   {'answer': [[], ''], 'chunk': 35, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.19371032714844},\n",
              "   {'answer': [[], ''], 'chunk': 36, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs.',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.40343475341797},\n",
              "   {'answer': [['1,500'], ''], 'chunk': 37, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs.',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.4618911743164062},\n",
              "   {'answer': [[], ''], 'chunk': 38, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.8588638305664},\n",
              "   {'answer': [[], ''], 'chunk': 39, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.255209922790527},\n",
              "   {'answer': [['2/6/2013'], ''], 'chunk': 40, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs.',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.7332763671875},\n",
              "   {'answer': [[], ''], 'chunk': 41, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs.',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 83.40013885498047},\n",
              "   {'answer': [[], ''], 'chunk': 42, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.680131912231445},\n",
              "   {'answer': [[], ''], 'chunk': 43, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5-7 yrs.',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.49845123291016},\n",
              "   {'answer': [[], ''], 'chunk': 44, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.43474197387695},\n",
              "   {'answer': [[], ''], 'chunk': 45, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.66122817993164},\n",
              "   {'answer': [[], ''], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.59903335571289},\n",
              "   {'answer': [[], ''], 'chunk': 47, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.700066566467285},\n",
              "   {'answer': [[], ''], 'chunk': 48, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.97822952270508},\n",
              "   {'answer': [[], ''], 'chunk': 49, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs., 12 yrs.',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.78449249267578},\n",
              "   {'answer': [[], ''], 'chunk': 50, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.298758506774902},\n",
              "   {'answer': [[], ''], 'chunk': 51, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.75634765625},\n",
              "   {'answer': [[], ''], 'chunk': 52, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -6.093472480773926},\n",
              "   {'answer': [[], ''], 'chunk': 53, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.055270195007324},\n",
              "   {'answer': [[], ''], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.499454498291016},\n",
              "   {'answer': [[], ''], 'chunk': 55, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.51478576660156},\n",
              "   {'answer': [[], ''], 'chunk': 56, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.20478439331055},\n",
              "   {'answer': [[], ''], 'chunk': 57, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.16043472290039},\n",
              "   {'answer': [[], ''], 'chunk': 58, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.2026252746582},\n",
              "   {'answer': [[], ''], 'chunk': 59, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs.',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.160552978515625},\n",
              "   {'answer': [[], ''], 'chunk': 60, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -13.751383781433105},\n",
              "   {'answer': [[], ''], 'chunk': 61, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.9055118560791},\n",
              "   {'answer': [[], ''], 'chunk': 62, 'model': 'TagOp'},\n",
              "   {'answer': '5-7 yrs.',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.37966537475586},\n",
              "   {'answer': [[], ''], 'chunk': 63, 'model': 'TagOp'},\n",
              "   {'answer': '5+ yrs.',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.4917662143707275},\n",
              "   {'answer': [[], ''], 'chunk': 64, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.448970794677734},\n",
              "   {'answer': [[], ''], 'chunk': 65, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.735708236694336},\n",
              "   {'answer': [[], ''], 'chunk': 66, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.28382396697998},\n",
              "   {'answer': [[], ''], 'chunk': 67, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.769883632659912},\n",
              "   {'answer': [[], ''], 'chunk': 68, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12 yrs.',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.703441619873047},\n",
              "   {'answer': [[], ''], 'chunk': 69, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10-12 yrs.',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.934840202331543},\n",
              "   {'answer': [[], ''], 'chunk': 70, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.5 yrs.',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.873326301574707},\n",
              "   {'answer': [[], ''], 'chunk': 71, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.004520416259766},\n",
              "   {'answer': [[], ''], 'chunk': 72, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.98995590209961},\n",
              "   {'answer': [[], ''], 'chunk': 73, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.90617561340332},\n",
              "   {'answer': [[], ''], 'chunk': 74, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12 yrs.',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.200099468231201},\n",
              "   {'answer': [[], ''], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': '16 yrs., 15 yrs.',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.862579345703125},\n",
              "   {'answer': [[], ''], 'chunk': 76, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.81637191772461},\n",
              "   {'answer': [[], ''], 'chunk': 77, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.507808685302734},\n",
              "   {'answer': [[], ''], 'chunk': 78, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.950008392333984},\n",
              "   {'answer': [[], ''], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.84510040283203},\n",
              "   {'answer': [['8/2/1998'], ''], 'chunk': 80, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.6451416015625},\n",
              "   {'answer': [['12/23/1997'], ''], 'chunk': 81, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs., 2 yrs., 2-3 yrs.',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.349069595336914},\n",
              "   {'answer': [[], ''], 'chunk': 82, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2-3 yrs.',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.5838769674301147},\n",
              "   {'answer': [[], ''], 'chunk': 83, 'model': 'TagOp'},\n",
              "   {'answer': '2 yrs.',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.099565505981445},\n",
              "   {'answer': [[], ''], 'chunk': 84, 'model': 'TagOp'},\n",
              "   {'answer': '2 yrs.',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.519771575927734},\n",
              "   {'answer': [[], ''], 'chunk': 85, 'model': 'TagOp'},\n",
              "   {'answer': '15+ yrs.',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.06390380859375},\n",
              "   {'answer': [[], ''], 'chunk': 86, 'model': 'TagOp'},\n",
              "   {'answer': '3 yrs.',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.710948944091797},\n",
              "   {'answer': [[], ''], 'chunk': 87, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.4163932800293},\n",
              "   {'answer': [[], ''], 'chunk': 88, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.45491027832031},\n",
              "   {'answer': [[], ''], 'chunk': 89, 'model': 'TagOp'},\n",
              "   {'answer': '4 yrs.',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.414701461791992},\n",
              "   {'answer': [[], ''], 'chunk': 90, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs.',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.10893440246582},\n",
              "   {'answer': [[], ''], 'chunk': 91, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.95798110961914},\n",
              "   {'answer': [[], ''], 'chunk': 92, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs.',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.238875389099121},\n",
              "   {'answer': [[], ''], 'chunk': 93, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.448883056640625},\n",
              "   {'answer': [['107.4'], ''], 'chunk': 94, 'model': 'TagOp'},\n",
              "   {'answer': '3 yrs.',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.7781400680542},\n",
              "   {'answer': [[], ''], 'chunk': 95, 'model': 'TagOp'},\n",
              "   {'answer': '7 yrs.',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 9.43813419342041},\n",
              "   {'answer': [[], ''], 'chunk': 96, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.91926193237305},\n",
              "   {'answer': [[], ''], 'chunk': 97, 'model': 'TagOp'},\n",
              "   {'answer': '2 yrs.',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.899604797363281},\n",
              "   {'answer': [[], ''], 'chunk': 98, 'model': 'TagOp'},\n",
              "   {'answer': '3 yrs.',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.145378589630127},\n",
              "   {'answer': [[], ''], 'chunk': 99, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.34495162963867},\n",
              "   {'answer': [[], ''], 'chunk': 100, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.4756100177764893},\n",
              "   {'answer': [[], ''], 'chunk': 101, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.742483139038086},\n",
              "   {'answer': [[], ''], 'chunk': 102, 'model': 'TagOp'},\n",
              "   {'answer': '12.6 yrs.',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.069900512695312},\n",
              "   {'answer': [[], ''], 'chunk': 103, 'model': 'TagOp'},\n",
              "   {'answer': '5 yrs.',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.420526504516602},\n",
              "   {'answer': [[], ''], 'chunk': 104, 'model': 'TagOp'},\n",
              "   {'answer': '3 yrs.',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.01495361328125},\n",
              "   {'answer': [[], ''], 'chunk': 105, 'model': 'TagOp'},\n",
              "   {'answer': '3 yrs.',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.097084045410156},\n",
              "   {'answer': [[], ''], 'chunk': 106, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > .5 yrs.',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.61382293701172},\n",
              "   {'answer': [[], ''], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': '4 yrs.',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.2888541221618652},\n",
              "   {'answer': [[], ''], 'chunk': 108, 'model': 'TagOp'},\n",
              "   {'answer': '10 yrs.',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.1559419631958},\n",
              "   {'answer': [[], ''], 'chunk': 109, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -7.014660835266113},\n",
              "   {'answer': [[], ''], 'chunk': 110, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs.',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.187344551086426},\n",
              "   {'answer': [[], ''], 'chunk': 111, 'model': 'TagOp'},\n",
              "   {'answer': '2 yrs.',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.552730560302734},\n",
              "   {'answer': [[], ''], 'chunk': 112, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.689138412475586},\n",
              "   {'answer': [[], ''], 'chunk': 113, 'model': 'TagOp'},\n",
              "   {'answer': '4 yrs.',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.115811347961426},\n",
              "   {'answer': [[], ''], 'chunk': 114, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.516897201538086},\n",
              "   {'answer': [[], ''], 'chunk': 115, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.535120010375977},\n",
              "   {'answer': [[], ''], 'chunk': 116, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.279403686523438},\n",
              "   {'answer': [[], ''], 'chunk': 117, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs.',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.15933609008789},\n",
              "   {'answer': [[], ''], 'chunk': 118, 'model': 'TagOp'},\n",
              "   {'answer': '12 yrs.',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.322460174560547},\n",
              "   {'answer': [[], ''], 'chunk': 119, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.61679077148438},\n",
              "   {'answer': [[], ''], 'chunk': 120, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.947017669677734},\n",
              "   {'answer': [[], ''], 'chunk': 121, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.222843170166016},\n",
              "   {'answer': [[], ''], 'chunk': 122, 'model': 'TagOp'},\n",
              "   {'answer': '3-5 yrs., nan',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.736278533935547},\n",
              "   {'answer': [[], ''], 'chunk': 123, 'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.6549129486084},\n",
              "   {'answer': [[], ''], 'chunk': 124, 'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_2',\n",
              "  'question': 'What is the expected lifetime of INSAT 4A satellite ?'},\n",
              " 'que_3': {'answers': [{'answer': 'COUNT > AAUSat-3, ABS-2 (Koreasat-8, ST-3), ABS-3 (Agila 2, Mabuhay 1), ABS-4 (ABS-2i, MBSat, Mobile Broadcasting Satellite, Han Byul), ABS-6 (ABS-1, LMI-1, Lockheed Martin-Intersputnik-1), ABS-7 (Koreasat 3, Mugungwha 3), AcrimSat (Active Cavity Radiometer Irradiance Monitor), Advanced Orion 2 (NROL 6, USA 139), Advanced Orion 3 (NROL 19, USA 171), Advanced Orion 4 (NRO L-26, USA 202)',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.19161987304688},\n",
              "   {'answer': 'COUNT > 35,771',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.87618255615234},\n",
              "   {'answer': 'COUNT > Aerocube 6B, Afghansat-1 (Eutelsat 28B, Eutelsat 48B, Eutelsat W2M), Afristar , AGILE (Astro-rivelatore Gamma a Immagini Leggaro), AIM (Aeronomy of Ice in Mesosphere), AISat-1, AISSat-1 (Automatic Identification System Satellite-1), AISSat-2 (Automatic Identification System Satellite-2), AIST-1, AIST-2',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 187.5670928955078},\n",
              "   {'answer': 'COUNT > Akebono (EXOS-D), ALiCEï¿½ï¿½(AFIT LEO iMESA CNT E), Alphasat I-XLï¿½ï¿½(Inmarsat IV-A F4), Alsat-2A (Algeria Satellite 2A), Amazonas-1 , Amazonas-2, Amazonas-3, Amazonas-4A, AMC-1 (Americom 1, GE-1), AMC-10 (Americom-10, GE 10)',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.76968383789062},\n",
              "   {'answer': 'COUNT > AMC-11 (Americom-11, GE 11), AMC-15 (Americom-15), AMC-16 (Americom-16), AMC-18 (Americom 18), AMC-2 (Americom 2, GE-2), AMC-21 (Americom 21), AMC-3 (Americom 3, GE-3), AMC-4 (Americom-4, GE-4), AMC-5 (Americom-5, GE-5)',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.2742919921875},\n",
              "   {'answer': 'COUNT > AMC-6 (Americom-6, GE-6), AMC-7 (Americom-7, GE-7), AMC-8 (Americom-8, GE-8, Aurora 3), AMC-9 (Americom 9, GE-12), Amos 2 , Amos 3, Amos 4, Amos 5, Amsat-Oscar 7 (AO-7), Angels (Automated Navigation and Guidance Experiment for Local Space, USA 255)',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 162.76573181152344},\n",
              "   {'answer': 'COUNT > Anik F1 , Anik F1R, Anik F2 , Anik F3, Anik G1, AntelSat, AprizeSat 1 (LatinSat-C), AprizeSat 10, AprizeSat 2 (LatinSat-D), AprizeSat 3, AprizeSat 4, AprizeSat 8',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 176.57388305664062},\n",
              "   {'answer': 'COUNT > AprizeSat 9, Apstar 1, Apstar 1A, Apstar 6, Apstar 7, Arabsat 5C, Arabsat 7F (Nimiq 1) , Ardusat-1, Artemis (Advanced Data Relay and Technology Mission Satellite) , AsiaSat-3S (Asiasat 3SA)',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 104.34757995605469},\n",
              "   {'answer': 'COUNT > AsiaSat-4 , AsiaSat-5, AsiaSat-7, AsiaStar , Astra 1D , Astra 1E , Astra 1F , Astra 1G , Astra 1H , Astra 1KR',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 171.42269897460938},\n",
              "   {'answer': 'COUNT > Astra 1L, Astra 2F',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.577943801879883},\n",
              "   {'answer': 'COUNT > Astra 3B, Astra 5B, Athena-Fidusï¿½ï¿½(Access on THeatres for European Nations Allied forces - French Italian Dual Use Satellite), Auroraï¿½ï¿½(Tabletsat-2U-EO), Azerspace/Africasat-1a (Azersat-1), Badr 2 (Badr B), Badr 4 (Arabsat 4B), Badr 5 (Arabsat 5B), Badr 5A (Arabsat 5A), Badr 6 (Arabsat 4AR)',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 168.88693237304688},\n",
              "   {'answer': 'COUNT > BeeSat-1 (Berlin Experimental and Educational Satellite 1), BeeSat-2 (Berlin Experimental and Educational Satellite 2), BeeSat-3 (Berlin Experimental and Educational Satellite 3), BeijinGalaxy-1 (Beijing 1 [Tsinghua], Tsinghau-2, China DMC+4), Bird 2 (Bispectral InfraRed Detector 2), BKA (BelKA 2), Bonum 1 (Most 1), Brazilsat B-2 (Brasilsat B-2), Brazilsat B-3 (Brasilsat B-3)',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.29411315917969},\n",
              "   {'answer': 'COUNT > Brazilsat B-4 (Brasilsat B-4), BSAT-3A, BSAT-3B, BSAT-3C/JCSat 110-R, C/NOFS (Communication/Navigation Outage Forecasting System), Calipso (Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation), Canopus-Bï¿½(Kanopus Vulcan 1)ï¿½',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 86.53556060791016},\n",
              "   {'answer': 'COUNT > Can-X2 (Canadian Advanced Nanospace experiment), Can-X3aï¿½ï¿½(BRIght-star Target Explorer (BRITE), UniBRITE-1), Can-X4, Can-X5, Can-X6 (Canadian Advanced Nanospace experiment; NTS [Nanosatellite Tracking Ships]), CAPE-2 (Cajun Advanced Picosatellite Experiment), CartoSat 1 (IRS P5), CartoSat 2 (IRS P7, CartoSat 2AT), CartoSat 2A',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 153.3744354248047},\n",
              "   {'answer': 'COUNT > CartoSat 2B, Cassiope (CAScade SmallSat and Ionospheric Polar Explorer), CFESat (Cibola Flight Experiment Satellite), Chandra X-Ray Observatory (CXO), ChinaSat 6B (Zhong Xing 6B), Chuangxin 1-1 (Innovation 1-1, Chuang Xin 1, CZ-1-1), Chuangxin 1-2 (Innovation 1-2), Chuangxin 1-3 (Innovation 1-3), Chuangxin-3',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.04747009277344},\n",
              "   {'answer': 'COUNT > Ciel-2, CINEMA-1 (Cubesat for Ion, Neutral, Electron, Magnetic fields), Compass G-1 (Beidou G1), Compass G-11 (Beidou G5)',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.85395812988281},\n",
              "   {'answer': 'COUNT > Compass G-3 (Beidou G3), Compass G-4 (Beidou G4), Compass G-5 (Beidou IGSO-1), Compass G-6 (Beidou 2-16), Compass G-7 (Beidou IGSO-2), Compass G-8 (Beidou IGSO-3), Compass G-9 (Beidou ISGO-4), Compass M1 (Beidou M1)',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 132.19613647460938},\n",
              "   {'answer': 'COUNT > Compass M3 (Beidou 2-12), Compass M4 (Beidou 2-13), Compass M5 (Beidou 2-14), Compass M6 (Beidou 2-15)',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.54740142822266},\n",
              "   {'answer': 'COUNT > COSMIC-A (Formosat-3A, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-B (Formosat-3B, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-D (Formosat-3D, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-E (Formosat-3E, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMIC-F (Formosat-3F, Constellation Observing System for Meteorology, Ionosphere and Climate), COSMO-Skymed 1 (Constellation of small Satellites for Mediterranean basin Observation), COSMO-Skymed 2 (Constellation of small Satellites for Mediterranean basin Observation), COSMO-Skymed 3 (Constellation of small Satellites for Mediterranean basin Observation)',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.83697509765625},\n",
              "   {'answer': 'COUNT > COSMO-Skymed 4 (Constellation of small Satellites for Mediterranean basin Observation), Cryosat-2 , Cubebug 1ï¿½ï¿½(Capitï¿½n Beto), Cubebug 2 (Manolito, LO-74), Cubesat XI-IV (Oscar 57, CO-57), Cubesat XI-V (Oscar 58, CO-58), CUNYSat-1ï¿½ï¿½(City University of New York Satellite-1), CUSat-1 (Cornell University Satellite 1), Cute-1 (Cubical Titech Engineering satellite, Oscar 55, CO-55), Cute-1.7 + APD II (Cubical Titech Engineering satellite+Avalanche Photodiode)',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.2243194580078},\n",
              "   {'answer': 'COUNT > Daichi-2 (Advanced Land Observing Satellite-2, ALOS 2 2), Daichi-2 (ALOS -2, Advanced Land Observing Satellite-2), DANDE (Drag and Atmospheric Neutral Density Explorer), Deimos 1, Deimos 2 , Delfi-C3 (DO-64), DirecTV-10, DirecTV-11, DirecTV-12, DirecTV-1R ',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 178.879638671875},\n",
              "   {'answer': 'COUNT > DirecTV-4S , DirecTV-5 (Tempo 1), DirecTV-7S , DirecTV-8, DirecTV-9S, DLR Tubsat , DMSP 5D-2 F13 (Defense Meteorological Satellites Program, USA 109), DMSP 5D-2 F14 (Defense Meteorological Satellites Program, USA 131), DMSP 5D-3 F15 (Defense Meteorological Satellites Program, USA 147), DMSP 5D-3 F16 (Defense Meteorological Satellites Program, USA 172), DMSP 5D-3 F17 (Defense Meteorological Satellites Program, USA 191)',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 164.93655395507812},\n",
              "   {'answer': 'COUNT > DMSP 5D-3 F18 (Defense Meteorological Satellites Program, USA 210), DMSP 5D-3 F19 (Defense Meteorological Satellites Program, USA 249), Dove-2, Dove-3, Dove-4, DRTS (Data Relay Test Satellite, Kodama), DSCS III-A3 (USA 167, DSCS III-A3) (Defense Satellite Communications System), DSCS III-B6 (USA 170, DSCS III B-6) (Defense Satellite Communications System), DSCS III-F10 (USA 135, DSCS III B-13) (Defense Satellite Communications System)',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 163.5852508544922},\n",
              "   {'answer': 'COUNT > DSCS III-F11 (USA 148, DSCS III B-8) (Defense Satellite Communications System), DSCS III-F12 (USA 153, DSCS III B-11) (Defense Satellite Communications System), DSCS III-F6 (USA 82, DSCS III B-12) (Defense Satellite Communications System), DSCS III-F8 (USA 97, DSCS III B-10) (Defense Satellite Communications System), DSCS III-F9 (USA 113, DSCS III B-7) (Defense Satellite Communications System), DSP 18 (USA 130) (Defense Support Program), DSP 20 (USA 149) (Defense Support Program), DSP 21 (USA 159) (Defense Support Program), DSP 22 (USA 176) (Defense Support Program), DubaiSat-1',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 186.83990478515625},\n",
              "   {'answer': 'COUNT > 35,776, 35,781',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.54350662231445},\n",
              "   {'answer': 'COUNT > Echostar 3 , Echostar 6 , Echostar 7 , Echostar 8 , Echostar 9/Galaxy 23 (G-23, Intelsat 1A-13, Telstar 13), Echostar G1 (ICO G1, DBSD G1), EduSAT, Egyptsat-2 (Misrsat 2), Electro-L1 (GOMS 2 [Geostationary Ioperational Meteorological Satellite 2], ELISA-E12 (ELectronic Intelligence by SAtellite)',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 170.2592315673828},\n",
              "   {'answer': 'COUNT > ELISA-E24 (ELectronic Intelligence by SAtellite), ELISA-W11 (ELectronic Intelligence by SAtellite), ELISA-W23 (ELectronic Intelligence by SAtellite), EO-1 (Earth Observing 1), EOS-AM Terra , EOS-CHEM Aura , EOS-PM Aqua (Advanced Microwave Scanning Radiometer for EOS, EOS PM-1), EROS A1 (Earth Resources Observation Satellite), EROS B1 (Earth Resources Observation Satellite), e-st@r',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 136.32806396484375},\n",
              "   {'answer': 'COUNT > EstCube-1, Eurasiasat 1 (Turksat 2A), Europe*Star 1 (Intelsat 12, Europe*Star FM1, IS-12, PAS-12), Eutelsat 10A (Eutelsat W-2A), Eutelsat 12 West A (Atlantic Bird 1) , Eutelsat 16A (Eutelsat W3C), Eutelsat 16B (Eurobird 16, Atlantic Bird 4, Hot Bird 4) , Eutelsat 16C (SESAT-1, Siberia-Europe Satellite), Eutelsat 172A (GE-23, AMC-23, Worldsat 3, Americom 23)',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 159.16976928710938},\n",
              "   {'answer': 'COUNT > Eutelsat 25B (Esï¿½hail 1), Eutelsat 25C (Eutelsat 70A, Eutelsat W-5) , Eutelsat 28A (Eurobird 1, Eurosat 1), Eutelsat 33A (Eurobird 3, E-bird) , Eutelsat 36A (Eutelsat W-4) , Eutelsat 36B (Eutelsat W-7), Eutelsat 3A (Sinosat-5C, Sinosat-3, Xinnuo 3, Chinasat-5C, XN-3), Eutelsat 3B, Eutelsat 3D ',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 147.9286346435547},\n",
              "   {'answer': 'COUNT > Eutelsat 48A (Eutelsat W-48, Eurobird 9, Hot Bird 2, Eutelsat 2-F8, Eutelsat HB2), Eutelsat 48B (Eutelsat 21B, Eutelsat W-6A), Eutelsat 48C (Eutelsat 21A, W-6, Eutelsat W-3), Eutelsat 5 West A (Atlantic Bird 3, Stellat 5), Eutelsat 7 West A (Atlantic Bird 7), Eutelsat 70B (Eutelsat W5A), Eutelsat 7A (Eutelsat W3A) , Eutelsat 8 West A (Atlantic Bird 2) , Eutelsat 8 West C (Eutelsat Hot Bird 13A, Hot Bird 6)',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.28717803955078},\n",
              "   {'answer': 'COUNT > Eutelsat 9A (Eurobird 9A, Hot Bird 7A), Eutelsat Hot Bird 13B (Hot Bird 8), exactView 1, exactView5R',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.65342712402344},\n",
              "   {'answer': 'COUNT > Fengniao 1 (Hummingbird 1)',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.73518753051758},\n",
              "   {'answer': 'COUNT > Fengniao 1A (Hummingbird 1A), Fengyun 2D (FY-2D), Fengyun 2E (FY-2E), Fengyun 2F (FY-2F), Fengyun 3A (FY-3A), Fengyun 3B (FY-3B), Fengyun 3C (FY-3C), Fermi Gamma-Ray Space Telescope (formerly GLAST), FIA Radar 1 (Future Imagery Architecture (FIA) Radar 1, NROL-41, USA 215, Topaz)',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 143.0053253173828},\n",
              "   {'answer': 'COUNT > FIA Radar 2 (Future Imagery Architecture (FIA) Radar 2, NROL-25, USA 234, Topaz), FIA Radar 3 (Future Imagery Architecture (FIA) Radar 3, NROL-39 , USA 247, Topaz), Firebird-A (Focused Investigations of Relativistic Electron Burst, Intensity, Range, and Dynamics), Firebird-B (Focused Investigations of Relativistic Electron Burst, Intensity, Range, and Dynamics), Firefly, FLTSATCOM-8 (USA 46), Formosat-2 (ROCSAT-2, Republic of China Satellite 2) , FORTï¿½ (Fast On-orbit Recording of Transient Events), FUNCube-1 (AO-73)',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 149.9161376953125},\n",
              "   {'answer': 'COUNT > Galaxy-11, Galaxy-12, Galaxy-13 (Horizons 1, Galaxy 13L), Galaxy-14, Galaxy-15, Galaxy-16 , Galaxy-17, Galaxy-18, Galaxy-19, Galaxy-25 (G-25, Intelsat 1A-5, Telstar 5), Galaxy-27 (G-27, Intelsat IA-7, Telstar 7)',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 180.00917053222656},\n",
              "   {'answer': 'COUNT > 23,242, 23,214, 23,217',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.48980712890625},\n",
              "   {'answer': 'COUNT > Genesis-2, GeoEye-1 (Orbview 5), Global Change Observation Mission - 1 Water (GCOM-1, Shikuzu)',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.26789474487305},\n",
              "   {'answer': 'COUNT > Globalstar M037 (Globalstar 16), Globalstar M039 (Globalstar 45), Globalstar M040 (Globalstar 10), Globalstar M056 (Globalstar 43), Globalstar M059 (Globalstar 42), Globalstar M063 (Globalstar 49), Globalstar M065 (Globalstar 65), Globalstar M066 (Globalstar 66), Globalstar M067 (Globalstar 67), Globalstar M068 (Globalstar 68), Globalstar M069 (Globalstar 69), Globalstar M070 (Globalstar 70), Globalstar M071 (Globalstar 71)',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 153.8344268798828},\n",
              "   {'answer': 'COUNT > Globalstar M072 (Globalstar 72), Globalstar M073 (Globalstar 73, Globalstar 2-6), Globalstar M074 (Globalstar 74, Globalstar 2-2), Globalstar M075 (Globalstar 75, Globalstar 2-5), Globalstar M076 (Globalstar 76, Globalstar 2-3), Globalstar M077 (Globalstar 77, Globalstar 2-4), Globalstar M078 (Globalstar 95, Globalstar 2-23), Globalstar M079 (Globalstar 79, Globalstar 2-1), Globalstar M080 (Globalstar 80, Globalstar 2-14), Globalstar M081 (Globalstar 81, Globalstar 2-11), Globalstar M082 (Globalstar 82, Globalstar 2-15)',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 163.96766662597656},\n",
              "   {'answer': 'COUNT > Globalstar M083 (Globalstar 83, Globalstar 2-7), Globalstar M084 (Globalstar 84, Globalstar 2-13) , Globalstar M085 (Globalstar 85, Globalstar 2-10), Globalstar M086 (Globalstar 86, Globalstar 2-18), Globalstar M088 (Globalstar 88, Globalstar 2-8), Globalstar M089 (Globalstar 89, Globalstar 2-12), Globalstar M090 (Globalstar 90, Globalstar 2-17), Globalstar M091 (Globalstar 91, Globalstar 2-9), Globalstar M092 (Globalstar 92, Globalstar 2-16), Globalstar M093 (Globalstar 87, Globalstar 2-20), Globalstar M094 (Globalstar 93, Globalstar 2-21)',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 147.45361328125},\n",
              "   {'answer': 'COUNT > Globalstar M095 (Globalstar 96, Globalstar 2-24), Globalstar M096 (Globalstar 94, Globalstar 2-22), Globalstar M097 (Globalstar 78, Globalstar 2-19), Glonass 701 (Glonass-K, Cosmos 2471), Glonass 712 (Glonass M, Cosmos 2413), Glonass 714 (Cosmos 2419), Glonass 715 (Glonass 35-1, Cosmos 2424), Glonass 716 (Glonass 35-2, Cosmos 2425), Glonass 717 (Glonass 35-3, Cosmos 2426), Glonass 719 (Glonass 36-2, Cosmos 2432), Glonass 720 (Glonass 36-3, Cosmos 2433)',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 167.44471740722656},\n",
              "   {'answer': 'COUNT > Glonass 721 (Glonass 37-1, Cosmos 2434), Glonass 722 (Glonass 37-2, Cosmos 2435), Glonass 723 (Glonass 37-3, Cosmos 2436), Glonass 724 (Glonass 38-1, Cosmos 2442), Glonass 725 (Glonass 38-2, Cosmos 2443), Glonass 726 (Glonass 38-3, Cosmos 2444), Glonass 727 (Glonass 39-1, Cosmos 2447), Glonass 729 (Glonass 39-3, Cosmos 2449), Glonass 730 (Glonass 41-1, Cosmos 2456), Glonass 731 (Glonass 42-1, Cosmos 2459), Glonass 732 (Glonass 42-3, Cosmos 2460)',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 178.66650390625},\n",
              "   {'answer': 'COUNT > Glonass 733 (Glonass 41-2, Cosmos 2457), Glonass 734 (Glonass 41-3, Cosmos 2458), Glonass 735 (Glonass 42-2, Cosmos 2461), Glonass 736 (Glonass 43-1, Cosmos 2464), Glonass 737 (Glonass 43-2, Cosmos 2465), Glonass 738 (Glonass 43-3, Cosmos 2466), Glonass 742 (Glonass-M, Cosmos 2474), Glonass 743 (Glonass 44-2, Cosmos 2476), Glonass 744 (Glonass 44-3, Cosmos 2477), Glonass 745 (Glonass 44-1, Cosmos 2475), Glonass 746 (Glonass-M, Cosmos 2478)',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 161.50311279296875},\n",
              "   {'answer': 'COUNT > 669',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.772414207458496},\n",
              "   {'answer': 'COUNT > Gonets M-13, Gonets M-15',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.59171676635742},\n",
              "   {'answer': 'COUNT > 35,777',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.0644989013672},\n",
              "   {'answer': 'COUNT > Haiyang 2A (HY 2A), HAMSat (VUSat Oscar 52), Helios 2A , Helios 2B, HellasSat 2 (Intelsat K-TV, NSS K-TV), HESSI (RHESSI, Reuven Ramaty High Energy Solar Spectroscopic Imager), Hinode (Solar B), Hisaki (Sprint A, Spectroscopic Planet Observatory for Recognition of Interaction of Atmosphere), Hispasat 1C ',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 182.3825225830078},\n",
              "   {'answer': 'COUNT > 621, 489',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.25057601928711},\n",
              "   {'answer': 'COUNT > HumSat-D, HYLAS 1 (Highly Adaptable Satellite), HYLAS 2 (Highly Adaptable Satellite), ICube',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.379852294921875},\n",
              "   {'answer': 'COUNT > IGS-8A (Information Gathering Satellite 8A, IGS Radar 4), IGS-8B (Information Gathering Satellite 8B, IGS Optical Demonstrator), Ikonos-2 , Improved Trumpet 4 (NROL-22, National Reconnaissance Office Launch-22, SBIRS HEO-1, Twins 1, USA 184), Improved Trumpet 5 (NROL-28, National Reconnaissance Office Launch-28, SBIRS HEO-2, Twins 2, USA 200), INMARSAT 3 F1 , INMARSAT 3 F2 , INMARSAT 3 F4 , INMARSAT 3 F5 , INMARSAT 4 F1',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.642578125},\n",
              "   {'answer': 'COUNT > 5,458, 5,960, 6,070, 2,950, 2,750, 2,090, 3,100, 3,028, 2,130, 4,000',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 178.5678253173828},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -30.34538459777832},\n",
              "   {'answer': 'COUNT > Intelsat 21 (IS-21), Intelsat 23',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.19451904296875},\n",
              "   {'answer': 'COUNT > Intelsat 706 , Intelsat 8 (IS-8, PAS-8) , Intelsat 805 , Intelsat 9 (IS-9, PAS-9) , Intelsat 901 , Intelsat 902 , Intelsat 903 , Intelsat 904 , Intelsat 905 , Intelsat 906 , Intelsat 907 , Intelsat APR-2 (INSAT 2E)',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 164.9451904296875},\n",
              "   {'answer': 'COUNT > Intelsat New Dawn, International Space Station (ISS [first element Zarya]), Interstellar Boundary EXplorer (IBEX), IPEX (Intelligent Payload Experiment, CalPoly 8), Iridium 10 (Iridium SV010), Iridium 11A (Iridium SVO88), Iridium 12 (Iridium SV012), Iridium 13 (Iridium SV013), Iridium 14A (Iridium SV092), Iridium 15 (Iridium SV015)',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 166.5548095703125},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 177.28839111328125},\n",
              "   {'answer': 'COUNT > Iridium 31 (Iridium SVO31), Iridium 32 (Iridium  SV032), Iridium 34 (Iridium SV034), Iridium 35 (Iridium SV035), Iridium 37 (Iridium SV037), Iridium 39 (Iridium SV039), Iridium 40 (Iridium SV040), Iridium 41 (Iridium SV041), Iridium 42 (Iridium SV042), Iridium 43 (Iridium SV043)',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 169.10614013671875},\n",
              "   {'answer': 'COUNT > Iridium 45 (Iridium SV045), Iridium 47 (Iridium SV047), Iridium 49 (Iridium SV049), Iridium 5 (Iridium SV005), Iridium 50 (Iridium SV050), Iridium 51 (Iridium SV 051), Iridium 52 (Iridium SV052), Iridium 53 (Iridium SV053), Iridium 54 (Iridium SV054), Iridium 55 (Iridium SV055)',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 156.10345458984375},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 177.40054321289062},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 184.98074340820312},\n",
              "   {'answer': 'COUNT > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 174.8973388671875},\n",
              "   {'answer': 'COUNT > Iridium 95 (Iridium SV095), Iridium 96 (Iridium SV096), Iridium 97 (Iridium SV097), Iridium 98 (Iridium SV098), IRIS (Interface Region Imaging Spectrometer), IRNSS-1A (Indian Regional Navigation Satellite System), IRNSS-1B (Indian Regional Navigation Satellite System), IRS-P6 (Resourcesat-1) , ITU-pSAT1 (Istanbul Technical University Picosat-1), Jason 2 ',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 170.2788848876953},\n",
              "   {'answer': 'COUNT > JCSat 10 (Japan Communications Satellite 10, JCSat 3A), JCSat 110 (N-Sat-110, Superbird-D, Superbird-5, N-Sat-110), JCSat 13 (Japan Communications Satellite 13), JCSat 1B (JCSAT 5), JCSat 2A (JCSAT 8, Japan Communications Satellite 8), JCSat 4A (JCSAT 6, Japan Communications Satellite 6), JCSat 9 (JCSat 5A, Japanese Communications Satellite 9), JCSat RA (JCSat 12, Japan Communications Satellite 12), Jugnu',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.03985595703125},\n",
              "   {'answer': 'COUNT > Kalpana-1 (Metsat-1), KazEOSat-1 (Kazcosmos Earth Observation Satellite), KazEOSat-2 (kazcosmos Earth Observation Satellite), KazSat-2, KazSat-3, Keyhole 3 (Advanced KH-11, KH-12-4, Advanced Keyhole, Misty-2, EIS-1, 8X Enhanced Imaging System, USA 144), Keyhole 4 (Advanced KH-11, Advanced Keyhole, Improved Crystal, EIS-2, 8X Enhanced Imaging System, NROL 14, USA 161), Keyhole 5 (Advanced KH-11, KH-12-5, Improved Crystal, EIS-3, USA 186), Keyhole 6 (NRO L49, Advanced KH-11, KH-12-6, Improved Crystal, USA 224)',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 151.37347412109375},\n",
              "   {'answer': 'COUNT > 35,778, 35,776, 651, 676, 679, 497',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.69413757324219},\n",
              "   {'answer': 'COUNT > Koreasat 5 (Mugungwha 5), Koreasat 6 (Mugungwha 6), Kuaizhou-1 (KZ-1), Lacrosse/Onyx 3 (Lacrosse-3, USA 133), Lacrosse/Onyx 4 (Lacrosse-4, USA 152), Lacrosse/Onyx 5 (Lacrosse-5, NROL 16, USA 182), Landsat 7 , Landsat 8, LAPAN-Tubsat, LatinSat A',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 118.47244262695312},\n",
              "   {'answer': 'COUNT > 35,760, 612, 900, 35,778, 35,772, 35,767, 984, 305, 457, 35,782',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.69270324707031},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -11.302692413330078},\n",
              "   {'answer': 'COUNT > Meteosat 10 (MSGalaxy-3,MSG 3), MetOp-B (Meteorological Operational satellite), Mexsat-3 (Mexsat Bicentenario)',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 83.0775146484375},\n",
              "   {'answer': 'COUNT > Milstar DFS-5 (USA 164, Milstar 2-F3) (Military Strategic and Tactical Relay), MiRï¿½ï¿½(Mikhail Reshetnev [MiR], Yubileiny-2/RS-40), MKA-FKI-1 (Zond PP)',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.2624626159668},\n",
              "   {'answer': 'COUNT > MTSAT-1R (Himawari 6), MUOS-1 (Mobile User Objective System 1)',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.34640121459961},\n",
              "   {'answer': 'COUNT > Navstar GPS II-14 (Navstar SVN 26, PRN 26, USA 83), Navstar GPS II-21 (Navstar SVN 39, PRN 09, USA 92), Navstar GPS II-23 (Navstar SVN 34, PRN 04, USA 96), Navstar GPS II-24 (Navstar SVN 36, PRN 06, USA 100), Navstar GPS II-25 (Navstar SVN 33, PRN 03, USA 117), Navstar GPS II-26 (Navstar SVN 40, PRN 10, USA 126), Navstar GPS II-28 (Navstar SVN 38, PRN 08, USA 135), Navstar GPS II-35 (Navstar SVN 35, PRN 30, USA 94), Navstar GPS IIF-1 (Navstar SVN 62, PRN 25, USA 213), Navstar GPS IIF-2 (Navstar SVN 63, PRN 01, USA 232)',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 166.0899200439453},\n",
              "   {'answer': 'COUNT > Navstar GPS IIF-3 (Navstar SVN 65, USA 239), Navstar GPS IIF-4 (Navstar SVN 66, USA 242), Navstar GPS IIF-5 (Navstar SVN 64, USA 248), Navstar GPS IIF-6 (Navstar SVN 67, USA 251), Navstar GPS IIR-10 (Navstar SVN 47, PRN 22, USA 175), Navstar GPS IIR-11 (Navstar SVN 59, PRN 19, USA 177), Navstar GPS IIR-12 (Navstar SVN 60, PRN 23, USA 178), Navstar GPS IIR-13 (Navstar SVN 61, PRN 02, USA 180), Navstar GPS IIR-2 (Navstar SVN 43, PRN 13, USA 132)',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 152.56980895996094},\n",
              "   {'answer': 'COUNT > Navstar GPS IIR-3 (Navstar SVN 46, PRN 11, USA 145), Navstar GPS IIR-4 (Navstar SVN 51, PRN 20, USA 150), Navstar GPS IIR-5 (Navstar SVN 44, PRN 28, USA 151), Navstar GPS IIR-6 (Navstar SVN 41, PRN 14, USA 154), Navstar GPS IIR-7 (Navstar SVN 54, PRN 18, USA 156), Navstar GPS IIR-8 (Navstar SVN 56, PRN 16, USA 166), Navstar GPS IIR-9 (Navstar SVN 45, PRN 21, USA 168), Navstar GPS IIR-M-1 (Navstar SVN 53, PRN 17, USA 183), Navstar GPS IIR-M-2 (Navstar SVN 52, PRN 31, USA 190)',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 166.38880920410156},\n",
              "   {'answer': 'COUNT > Navstar GPS IIR-M-3 (Navstar SVN 58, PRN 12, USA 192), Navstar GPS IIR-M-4 (Navstar SVN 55, PRN 15, USA 196), Navstar GPS IIR-M-5 (Navstar SVN 57, PRN 29, USA 199), Navstar GPS IIR-M-6 (Navstar SVN 48, PRN 07, USA 201), Navstar GPS IIR-M-8 (Navstar SVN 50, PRN 05, USA 206), NEOSSatï¿½(Near Earth Object Surveillance Satellite), NFIRE (Near Field InfraRed Experiment), NigComSat-1R, NigeriaSat-2, Nigeriasat-X',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 183.3744659423828},\n",
              "   {'answer': 'COUNT > Nilesat 102 , Nilesat 201, Nimiq 2 , Nimiq 4, Nimiq 5, Nimiq 6, NOAA-15 (NOAA-K), NOAA-18 (NOAA-N, COSPAS-SARSAT), NOAA-19 (NOAA-N Prime, COSPAS-SARSAT), NPP (National Polar-orbiting Operational Environmental Satellite System [NPOESS])',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 131.09524536132812},\n",
              "   {'answer': 'COUNT > NPS-SCATï¿½(Naval Post-graduate School - Solar Cell Array Tester), NSS-10 (AMC-12, Americom 12, Worldsat 2), NSS-11 (Worldsat-1, AAP-1, GE-1A) , NSS-12, NSS-5 (Intelsat 803), NSS-6 , NSS-7 , NSS-703 (Intelsat 703, Intelsat 7 F-3), NSS-806 (Intelsat 806)',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 184.7884521484375},\n",
              "   {'answer': 'COUNT > NuSTAR (Nuclear Spectroscopic Telescope Array)',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.448699951171875},\n",
              "   {'answer': 'COUNT > Oceansat-2, OCO 2ï¿½ï¿½(Orbiting Carbon Observatory), Odin , Ofeq 10, Ofeq 5 , Ofeq 7, Ofeq 9, OPTOS, Optus and Defence C1 , Optus B3',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 156.27169799804688},\n",
              "   {'answer': 'COUNT > 2,300, 2,400, 2,501, 45, 45, 45, 45, 45, 45, 45, 45',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 182.3033905029297},\n",
              "   {'answer': 'COUNT > ORBCOMM FM-19 (ORBCOMM B7), ORBCOMM FM-20 (ORBCOMM B8), ORBCOMM FM-23 (ORBCOMM C3), ORBCOMM FM-27 (ORBCOMM C7), ORBCOMM FM-30 (ORBCOMM D2), ORBCOMM FM-31 (ORBCOMM D3), ORBCOMM FM-32 (ORBCOMM D4), ORBCOMM FM-34 (ORBCOMM D6), ORBCOMM FM-35 (ORBCOMM D7), ORBCOMM FM-36 (ORBCOMM D8), ORBCOMM FM-4 (ORBCOM G2)',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 175.9008026123047},\n",
              "   {'answer': 'COUNT > ORBCOMM FM-5 (ORBCOMM A6), ORBCOMM FM-6 (ORBCOMM A7), ORBCOMM FM-7 (ORBCOMM A8), ORBCOMM FM-8 (ORBCOMM A1), ORBCOMM FM-9 (ORBCOMM A5), ORBCOMM OG2 FM-103, ORBCOMM OG2 FM-104, ORBCOMM OG2 FM-106, ORBCOMM OG2 FM-107, ORBCOMM OG2 FM-109, ORBCOMM OG2 FM-111, ORS - Tech 1',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 179.9287567138672},\n",
              "   {'answer': 'COUNT > ORS - Tech 2, ORS-1 (Operationally Responsive Space One, USA 231), ORSES (ORS Enabler Satellite), ï¿½rsted , Paksat-1R , Palapa C2 , Palapa D1, PAN-1 (Palladium at Night, P360, USA 207), PARASOL (Polarization and Anistropy of Reflectances for Atmospheric Science coupled with Observations from LIDAR), Parus-90 (Cosmos 2361), Parus-91 (Cosmos 2366)',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 159.26263427734375},\n",
              "   {'answer': 'COUNT > Parus-92 (Cosmos 2378), Parus-93 (Cosmos 2389), Parus-94 (Cosmos 2398), Parus-95 (Cosmos 2407), Parus-96 (Cosmos 2414), Parus-97 (Cosmos 2429), Parus-98 (Cosmos 2454), Parus-99 (Cosmos 2463), PCSat (Prototype Communications SATellite, Navy-Oscar 44, NO-44), Perseus M1, Perseus M2, Persona-2 (Cosmos 2486)',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 168.29525756835938},\n",
              "   {'answer': 'COUNT > 725, 678',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.62411499023438},\n",
              "   {'answer': 'COUNT > Proba 2 (Project for On-Board Autonomy), Proba V (Project for On-Board Autonomy), Prometheus 1A, Prometheus 1B, Prometheus 2A, Prometheus 2B, Prometheus 3A, Prometheus 3B, Prometheus 4A, Prometheus 4B, PUCPSat-1',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 163.18641662597656},\n",
              "   {'answer': 'COUNT > QB50P1ï¿½(EO 79), QB50P2 (EO 80), QuetzSat-1, Quickbird 2, QZS-1 (Quazi-Zenith Satellite System, Michibiki), Radarsat-2 , Raduga 1-8 (Raduga 1-M1, Cosmos 2450), Raduga 1-M2 (Raduga 1-9), Raduga 1-M3 ',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 152.8599395751953},\n",
              "   {'answer': 'COUNT > Rapid Pathfinder Program (NROL-66, USA 225), RapidEye-1 (RapidEye-C), RapidEye-2 (RapidEye A), RapidEye-3 (RapidEye D), RapidEye-4 (RapidEye E), RapidEye-5 (RapidEye B), RASAT, Rascom-QAF 1R, RazakSat (MACSat), Reimei (Innovative Technology Demonstration Experiment Satellite - INDEX), Relek (ICA-FC1)',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 179.0264129638672},\n",
              "   {'answer': 'COUNT > Resourcesat 2, Resurs-DK1 (Resurs - High Resolution 1), Resurs-P1, RISat-1 (Radar Imaging Satellite 1), RISat-2 (Radar Imaging Satellite 2), Rising-2, Rodnik (Cosmos 2416, Strela 3M), Rodnik (Cosmos 2437, Strela 3M), Rodnik (Cosmos 2438, Strela 3M), Rodnik (Cosmos 2439, Strela 3M)',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 104.35749816894531},\n",
              "   {'answer': 'COUNT > 1,498, 1,495, 1,481',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.17570114135742},\n",
              "   {'answer': 'COUNT > Rodnik (Cosmos 2496, Strela 3M), Rodnik (Cosmos 2497, Strela 3M), Rodnik (Cosmos 2498, Strela 3M), Rumba (part of Cluster quartet, Cluster 2 FM5), SAC-C (Satellite for Scientific Applications), SAC-D (Satellite for Scientific Applications), Salsa (part of Cluster quartet, Cluster 2 FM6), Samba (part of Cluster quartet, Cluster 2 FM7)',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 148.95594787597656},\n",
              "   {'answer': 'COUNT > Satmex 4 (Solidaridad 2), Satmex 5 , Satmex 6, Satmex 8',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.74563598632812},\n",
              "   {'answer': 'COUNT > Saudicomsat-1, Saudicomsat-2 , Saudicomsat-3, Saudicomsat-4, Saudicomsat-5, Saudicomsat-6, Saudicomsat-7, Saudisat 1C (Oscar 50, SO 50), Saudisat-2 , Saudisat-3 ',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 152.3950958251953},\n",
              "   {'answer': 'COUNT > Saudisat-4, SBIRS GEO 1 (Space Based Infrared System Geosynchronous 1, USA 230), SBIRS GEO 2 (Space Based Infrared System Geosynchronous 2, USA 241), SBSS-1 (Space Based Space Surveillance Satellite, SBSS Block 10 SV1, USA 216), SB-WASS 3-1 (Space Based Wide Area Surveillance System) (NOSS 3-1, NOSS C1-1, USA 160), SB-WASS 3-1 (Space Based Wide Area Surveillance System) (NOSS 3-1,USA 160, NOSS C1-2) , SB-WASS 3-2 (Space Based Wide Area Surveillance System) (NOSS 3-2, USA 173, NOSS C2-1) , SB-WASS 3-2 (Space Based Wide Area Surveillance System) (NOSS 3-2, USA 173, NOSS C2-2) ',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 188.2894287109375},\n",
              "   {'answer': 'COUNT > SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L23) , SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L28) , SB-WASS 3-4 (Space Based Wide Area Surveillance System) NOSS 3-4, USA 194, NRO L30), SB-WASS 3-4 (Space Based Wide Area Surveillance System) NOSS 3-4, USA 194, NRO L30), SB-WASS 3-5 (Space Based Wide Area Surveillance System) NOSS 3-5, USA 229, NRO L34), SB-WASS 3-5 (Space Based Wide Area Surveillance System) NOSS 3-5, USA 229, NRO L34), SB-WASS 3-6 (Space Based Wide Area Surveillance System) NOSS 3-6, USA 238, NRO L36), SB-WASS 3-6 (Space Based Wide Area Surveillance System) NOSS 3-6, USA 238, NRO L36)',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.465599060058594},\n",
              "   {'answer': 'COUNT > SDO (Solar Dynamics Observatory), SDS III-4 (Satellite Data System) (NRO L-1, Nemesis, USA 179) , SDS III-5 (Satellite Data System) (NRO L-24, Scorpius, USA 198) , SDS III-7 (Satellite Data System) NRO L-38, Drake, USA 236), SDS III-8 (Satellite Data System) NRO L-33, USA 252)',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.965768814086914},\n",
              "   {'answer': 'COUNT > 98.2',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.94888305664062},\n",
              "   {'answer': 'COUNT > SES-8, Shijian 11-01 (SJ-11-01), Shijian 11-02 (SJ-11-02), Shijian 11-03 (SJ-11-03), Shijian 11-05 (SJ-11-05), Shijian 11-06 (SJ-11-06), Shijian 12 (SJ-12), Shijian 15 (SJ-15)',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.55917358398438},\n",
              "   {'answer': 'COUNT > Shijian 16 (SJ-16), Shijian 6A (SJ-6A, Dong Fang Hong 60), Shijian 6B (SJ-6B), Shijian 6C (SJ-6-02A), Shijian 6D (SJ-6-02B), Shijian 6E (SJ6-03A, SJ-6E), Shijian 6F (SJ6-03B, SJ-6F)',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.91343688964844},\n",
              "   {'answer': 'COUNT > Shijian 6G (SJ6-04A), Shijian 6H (SJ6_04B), Shijian 7 (SJ7, Dong Fang Hong 65), Shijian 9A (SJ 9A), Shijian 9B (SJ 9B), ShindaiSat (Shinshu University Satellite), Shiyan 1 (SY 1, Tansuo 1, Experimental Satellite 1), Shiyan 3 (SY3, Experimental Satellite 3)',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 101.99515533447266},\n",
              "   {'answer': 'COUNT > Shiyan 4 (SY4, Experimental Satellite 4), Shiyan 5 (SY5, Experimental Satellite 5), Shiyan 7 (SY7, Experimental Satellite 7), Sich 2, Sicral 1A , Sicral 1B, Sina-1 (Sinah 1, ZS1), Sinosat-6 (Chinasat-6A, XN-6), Sirius 1 (SD Radio 1)',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 161.71633911132812},\n",
              "   {'answer': 'COUNT > Sirius 2 (SD Radio 2), Sirius 3 (SD Radio 3), Sirius 4 (Astra 4A), Sirius FM-5, Sirius FM-6, Sirius XM-5, Skynet 4C , Skynet 4E , Skynet 4F , Skynet 5A, Skynet 5B',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 165.56295776367188},\n",
              "   {'answer': 'COUNT > 35,786',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.72643280029297},\n",
              "   {'answer': \"COUNT > SNaP-3-1ï¿½ï¿½(Space and Missile Defense Command NanoSat Program), SOCRATESï¿½ï¿½(Space Optical Communications Research Advanced TEchnology Satellite), SOHLA 1 (Space Oriented Higashiosaka Leading Association, Maido 1), SORCE (SOlar Radiation and Climate Experiment), Spaceway 3, Spaceway F1, Spaceway F2, Spainsat, Spektr-R/RadioAstron, SPIRALE-A (Systï¿½me Prï¿½paratoire Infra-Rouge pour l'Alerte), SPIRALE-B (Systï¿½me Prï¿½paratoire Infra-Rouge pour l'Alerte)\",\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 163.54010009765625},\n",
              "   {'answer': 'COUNT > 696',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.89370727539062},\n",
              "   {'answer': 'COUNT > STARE-B (Horus [Space-Based Telescopes for Actionable Refinement of Ephemeris]), STPSat-2 (USA 217), STPSat-3 (Space Test Program Satellite-3) , STRaND-1 (Surrey Training, Research and Nanosatellite Demonstrator 1), Strela 3 (Cosmos 2384), Strela 3 (Cosmos 2385), Strela 3 (Cosmos 2386), Strela 3 (Cosmos 2390), Strela 3 (Cosmos 2391)',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.5550079345703},\n",
              "   {'answer': 'COUNT > Strela 3 (Cosmos 2400), Strela 3 (Cosmos 2401), Strela 3 (Cosmos 2408), Strela 3 (Cosmos 2409), STSat-2C, STSat-3, STSS ATRR (Space Tracking and Surveillance System Advanced Technology Risk Reduction Satellite, USA 205), STSS Demo-1 (Space Tracking and Surveillance System Demonstrator), STSS Demo-2 (Space Tracking and Surveillance System Demonstrator)',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 156.50840759277344},\n",
              "   {'answer': 'COUNT > STUDSat (Student Satellite), Superbird 7 (Superbird C2), Superbird-B2 (Superbird 4), Superbird-C , Suzaku (Astro E2), SWARM-A, SWARM-B, SWARM-C, Swift , SwissCube, Syracuse 3A (Systeme de Radio Communications Utilisant un Satellite)',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 162.9098663330078},\n",
              "   {'answer': 'COUNT > Syracuse 3B (Systeme de Radio Communications Utilisant un Satellite), TacSat 4, TacSat 6, TanDEM-X (TerraSAR-X add-on for Digital Elevation Measurement), Tango (part of Cluster quartet, Cluster 2 FM8), TDRS-10 (Tracking and Data Relay Satellite, TDRS-J), TDRS-11 (Tracking and Data Relay Satellite, TDRS K), TDRS-12 (Tracking and Data Relay Satellite, TDRS L), TDRS-3 (Tracking and Data Relay Satellite, TDRS-C) ',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 167.68536376953125},\n",
              "   {'answer': 'COUNT > TDRS-5 (Tracking and Data Relay Satellite, TDRS-E), TDRS-6 (Tracking and Data Relay Satellite, TDRS-F), TDRS-7 (Tracking and Data Relay Satellite, TDRS-G), TDRS-8 (Tracking and Data Relay Satellite, TDRS-H), TDRS-9 (Tracking and Data Relay Satellite, TDRS-I), TechDemoSat-1, Telkom 1 , Telkom 2',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 157.11236572265625},\n",
              "   {'answer': 'COUNT > Telstar 11N, Telstar 14R (Estrela do Sul 2), TerraStar 1, Thaicom-4 (Ipstar 1, Measat 5)',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.88294982910156},\n",
              "   {'answer': 'COUNT > THEMIS A (Time History of Events and Macroscale Interactions during Substorms) , THEMIS D (Time History of Events and Macroscale Interactions during Substorms) , THEMIS E (Time History of Events and Macroscale Interactions during Substorms) , THEOS (Thailand Earth Observation System), Thor-3 , Thor-5 (Thor 2R), Thor-6, Thuraya 2 , Thuraya 3, Tian Xun-1 (TX-1)',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 180.36500549316406},\n",
              "   {'answer': 'COUNT > Tianhui 1-02, TianLian 3 (TL-1-03, CTDRS)',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 123.61480712890625},\n",
              "   {'answer': 'COUNT > TIMED (Thermosphere ï¿½ Ionosphere ï¿½ Mesosphere ï¿½ Energetics and Dynamics), TISat-1 (Ticano Satellite), TKSat-1 (Tï¿½pac Katari Satellite 1), TRMM (Tropical Rainfall Measuring Mission), Trumpet 3 (NROL-4, National Reconnaissance Office Launch-4, USA 136), Trumpet 4 (NRO L-67, USA 250), Tselina-2 (Cosmos 2428), TUGSat-1ï¿½ï¿½(Technische Universitï¿½t Graz Satellit, CanX-3b, BRITE-Austria), Turksat 3A, Turksat 4A',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 153.1311798095703},\n",
              "   {'answer': 'COUNT > UKube-1 (UK Cubesat 1), UFO-10 (USA 146, UHF F/O F10) \"UHF Follow-On\", UFO-11  (USA 174) \"UHF Follow-On\", UFO-2 (USA 95) \"UHF Follow-On\", UFO-4 (USA 108, UFO F4 EHF) \"UHF Follow-On\", UFO-6 (USA 114, UFO F6 EHF) \"UHF Follow-On\", UFO-7 (USA 127, F7 EHF) \"UHF Follow-On\", UFO-8 (USA 138, UHF F/O F8) \"UHF Follow-On\", UK-DMC-2 (BNSCSat-2, British National Science Center Satellite 2), UNIFORM 1ï¿½ï¿½(UNiversity International FORmation Mission 1)',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 169.34317016601562},\n",
              "   {'answer': 'COUNT > ',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -11.000941276550293},\n",
              "   {'answer': 'COUNT > Vesselsat-2, Vinasat 2',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.49591827392578},\n",
              "   {'answer': 'COUNT > 35,785, 342',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.0410385131836},\n",
              "   {'answer': 'COUNT > Xinyan 1 (XY-1), XM Radio 3 (Rhythm), XM Radio 4 (Blues), XM Rock (XM 2), XM Roll (XM 1), XMM Newton (High Throughput X-ray Spectroscopy Mission), X-Sat, XTAR-EUR, XW-1 (Hope Oscar 68, HO-68, Xi Wang 1, Hope-1, CAS-1)',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 149.71163940429688},\n",
              "   {'answer': 'COUNT > Yahsat-1B (Y1B), Yamal-300K, Yamal-402',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.19380187988281},\n",
              "   {'answer': 'COUNT > Yaogan 14 (Remote Sensing Satellite 14), Yaogan 15 (Remote Sensing Satellite 15), Yaogan 16A (Remote Sensing Satellite 16A, Yaogan Weixing 16), Yaogan 16B (Remote Sensing Satellite 16B), Yaogan 16C (Remote Sensing Satlelite 16C)',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.68389892578125},\n",
              "   {'answer': 'COUNT > Yaogan 17C (Remote Sensing Satellite 17C), Yaogan 18 (Remote Sensing Satellite 18), Yaogan 19 (Remote Sensing Satellite 19), Yaogan 2 (Remote Sensing Satellite 2, Jian Bing 5-2, JB 5-2), Yaogan 3 (Remote Sensing Satellite 3, Jian Bing 5-3, JB 5-3), Yaogan 4 (Remote Sensing Satellite 4), Yaogan 5 (Remote Sensing Satellite 5, JB 5-C, Jian Bing 5-C)',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 141.2108917236328},\n",
              "   {'answer': 'COUNT > Yaogan 6 (Remote Sensing Satellite 6, Jian Bing 7-A), Yaogan 7 (Remote Sensing Satellite 7), Yaogan 8 (Remote Sensing Satellite 8), Yaogan 9A (Remote Sensing Satellite 9A), Yaogan 9B (Remote Sensing Satellite 9B), Yaogan 9C (Remote Sensing Satellite 9C), Youthsat, Yubileiny (Jubilejnyj, Radio Sputnik 30 (RS-30)), ZACube-1 (TshepisoSat)',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 154.38018798828125},\n",
              "   {'answer': 'COUNT > Zhongxing 11 (Chinasat 11), Zhongxing 12 (Chinasat 12, ZX-12), Zhongxing 1A (Chinasat 1A, Fenghuo 2)',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.04745483398438},\n",
              "   {'answer': 'COUNT > Zhongxing 2A (Chinasat 2A), Ziyuan 3 (ZY-3)',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 129.46334838867188}],\n",
              "  'class': 0,\n",
              "  'id': 'que_3',\n",
              "  'question': 'How many satellites were launched in the year 2012 ?'},\n",
              " 'que_4': {'answers': [{'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.21979331970215},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 0,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan', 'chunk': 1, 'model': 'TaPaS', 'score': 47.3975944519043},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 1,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.3563232421875},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Inclination (degrees)'], ''],\n",
              "    'chunk': 2,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.444610595703125},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 3,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan', 'chunk': 4, 'model': 'TaPaS', 'score': 45.6712760925293},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 4,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.93638610839844},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 5,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.90724754333496},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 6,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.28598022460938},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 7,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan', 'chunk': 8, 'model': 'TaPaS', 'score': 47.04230499267578},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 8,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan', 'chunk': 9, 'model': 'TaPaS', 'score': 56.77589416503906},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 9,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.77338409423828},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 10,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 135.42550659179688},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 11,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.992023468017578},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 12,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 136.48629760742188},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 13,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Deep Highly Eccentric, Sun-Synchronous',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.97346496582031},\n",
              "   {'answer': [['Eccentricity'], ''], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.9864485263824463},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 15,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.34816360473633},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 16,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.08082580566406},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 17,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 170.72494506835938},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 18,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 142.0310821533203},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 19,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.98687744140625},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 20,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.53929901123047},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 21,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.27023315429688},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 22,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.95097351074219},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 23,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 121.07588958740234},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 24,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Polar',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 29.94198989868164},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 25,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.45414352416992},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 26,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 154.33900451660156},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 27,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.066322326660156},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 28,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.19756317138672},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 29,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.505523681640625},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 30,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.647857666015625},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 31,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.5609130859375},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 32,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.81063461303711},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 33,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.12704086303711},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 34,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.151832580566406},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 35,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Deep Highly Eccentric',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.76483726501465},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 36,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 95.2930908203125},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 37,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.1825180053711},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 38,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 112.56208801269531},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 39,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.106101989746094},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 40,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.722999572753906},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 41,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.46963882446289},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 42,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Intermediate, Intermediate',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.529632568359375},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 43,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Polar',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.009071350097656},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 44,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, nan',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.97904205322266},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 45,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous, Intermediate',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.049062728881836},\n",
              "   {'answer': [['Eccentricity'], ''], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'nan, Polar, Sun-Synchronous',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.11146354675293},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 47,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.221683502197266},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 48,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 150.77377319335938},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 49,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Deep Highly Eccentric',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.94083023071289},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 50,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.93702697753906},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 51,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intelsat 21 (IS-21)',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.0358829498291},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 52,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.43415069580078},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 53,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Deep Highly Eccentric, Polar, Polar, Polar, Polar, Polar, Polar',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.00355529785156},\n",
              "   {'answer': [['Eccentricity'], ''], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Polar, Polar, Polar, Polar',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.32193756103516},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 55,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Polar, Polar, Polar',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.62443542480469},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 56,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Polar',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 77.67813110351562},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 57,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Polar, Polar, Polar, Polar, Polar, Polar, Polar, Polar, Polar',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.34281158447266},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 58,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Polar, Polar, Polar, Polar, Polar, Polar, Polar',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.47373962402344},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 59,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Polar',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.67113494873047},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 60,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Sun-Synchronous, nan, Sun-Synchronous, Sun-Synchronous, Intermediate',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.54100799560547},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 61,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, nan',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.7467155456543},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 62,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.66791534423828},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 63,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.73221588134766},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 64,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Intermediate, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.77716636657715},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 65,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.68490219116211},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 66,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Molniya, Molniya, Molniya, Polar, Polar',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.541465759277344},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 67,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Polar, Polar',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.50173950195312},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 68,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.712703704833984},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 69,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.40777587890625},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 70,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.34779739379883},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 71,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.02008819580078},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 72,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.59021759033203},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 73,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.67607116699219},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 74,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Polar',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.28736114501953},\n",
              "   {'answer': [['Eccentricity'], ''], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.3000259399414},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 76,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan', 'chunk': 77, 'model': 'TaPaS', 'score': 68.8863754272461},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 77,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 127.79478454589844},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 78,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Intermediate',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.03169250488281},\n",
              "   {'answer': [['Eccentricity'], ''], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.53575134277344},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 80,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Sun-Synchronous',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.15491485595703},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Inclination (degrees)'], ''],\n",
              "    'chunk': 81,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 122.2315902709961},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 82,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.15324401855469},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 83,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 143.6152801513672},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 84,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 181.8296356201172},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 85,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 144.46249389648438},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 86,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Polar, Sun-Synchronous, Intermediate, Sun-Synchronous',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.66020202636719},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 87,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 114.35346221923828},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 88,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.36204528808594},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 89,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Deep Highly Eccentric, Sun-Synchronous, Sun-Synchronous, Deep Highly Eccentric, Deep Highly Eccentric',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.111785888671875},\n",
              "   {'answer': [['Apogee (km)', 'Deep Highly Eccentric', 'Eccentricity'], ''],\n",
              "    'chunk': 90,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 102.09899139404297},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 91,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 158.0729522705078},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 92,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 168.1650390625},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 93,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.7529067993164},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 94,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Molniya, Molniya',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.65675735473633},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 95,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.5402603149414},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Inclination (degrees)'], ''],\n",
              "    'chunk': 96,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.38299560546875},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 97,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 163.53732299804688},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 98,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 108.39851379394531},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 99,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 119.39767456054688},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 100,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.814422607421875},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 101,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.041839599609375},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 102,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Intermediate',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.212631225585938},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 103,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 139.90745544433594},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 104,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.222161665558815},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 105,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Sun-Synchronous',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.481361389160156},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 106,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Intermediate, Polar, Polar, Polar, Intermediate, Sun-Synchronous',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 96.24613189697266},\n",
              "   {'answer': [['Eccentricity'], ''], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Deep Highly Eccentric',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.272367477416992},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 108,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.67585754394531},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 109,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.434417724609375},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 110,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.79166030883789},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 111,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.39496612548828},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 112,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Intermediate, Sun-Synchronous, Molniya, Sun-Synchronous',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.26118087768555},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 113,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.10071563720703},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 114,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 133.1787109375},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 115,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.10581970214844},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 116,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Cislunar, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.4747428894043},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 117,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Deep Highly Eccentric, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.645240783691406},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 118,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'nan, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.871623992919922},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity'], ''],\n",
              "    'chunk': 119,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 160.81106567382812},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 120,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 158.64303588867188},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 121,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 145.03704833984375},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 122,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 130.1168975830078},\n",
              "   {'answer': [['Apogee (km)',\n",
              "      'Eccentricity',\n",
              "      'Inclination (degrees)',\n",
              "      'Longitude of GEO (degrees)',\n",
              "      'Perigee (km)'],\n",
              "     ''],\n",
              "    'chunk': 123,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Sun-Synchronous, Sun-Synchronous',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.3171157836914},\n",
              "   {'answer': [['Apogee (km)', 'Eccentricity', 'Perigee (km)'], ''],\n",
              "    'chunk': 124,\n",
              "    'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_4',\n",
              "  'question': 'What are the different types of Orbits ?'},\n",
              " 'que_5': {'answers': [{'answer': '',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -8.175175666809082},\n",
              "   {'answer': [['Denmark'], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'USA', 'chunk': 1, 'model': 'TaPaS', 'score': 52.34969711303711},\n",
              "   {'answer': [['USA'], ''], 'chunk': 1, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.122039794921875},\n",
              "   {'answer': [['USA'], ''], 'chunk': 2, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.449920654296875},\n",
              "   {'answer': [['Japan'], ''], 'chunk': 3, 'model': 'TagOp'},\n",
              "   {'answer': 'USA', 'chunk': 4, 'model': 'TaPaS', 'score': 53.40781021118164},\n",
              "   {'answer': [['USA'], ''], 'chunk': 4, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.275535583496094},\n",
              "   {'answer': [['SES (Sociï¿½tï¿½ Europï¿½enne des Satellites (SES))'], ''],\n",
              "    'chunk': 5,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Canada',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.870479583740234},\n",
              "   {'answer': [['Canada'], ''], 'chunk': 6, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.514389038085938},\n",
              "   {'answer': [['USA/Argentina'], ''], 'chunk': 7, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.24821090698242},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 8, 'model': 'TagOp'},\n",
              "   {'answer': 'Luxembourg',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.333778381347656},\n",
              "   {'answer': [['Luxembourg'], ''], 'chunk': 9, 'model': 'TagOp'},\n",
              "   {'answer': 'Azerbaijan',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.48363813757896423},\n",
              "   {'answer': [['Luxembourg'], ''], 'chunk': 10, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.4677668809890747},\n",
              "   {'answer': [['Germany'], ''], 'chunk': 11, 'model': 'TagOp'},\n",
              "   {'answer': 'Brazil',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.305710792541504},\n",
              "   {'answer': [['Brazil'], ''], 'chunk': 12, 'model': 'TagOp'},\n",
              "   {'answer': 'Canada',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.41811752319336},\n",
              "   {'answer': [['Can-X2 (Canadian Advanced Nanospace experiment)'], ''],\n",
              "    'chunk': 13,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.370232582092285},\n",
              "   {'answer': [['India'], ''], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': 'Canada',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.5370875597000122},\n",
              "   {'answer': [['Canada'], ''], 'chunk': 15, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.35432815551758},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 16, 'model': 'TagOp'},\n",
              "   {'answer': 'South Korea',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.10384750366211},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 17, 'model': 'TagOp'},\n",
              "   {'answer': '', 'chunk': 18, 'model': 'TaPaS', 'score': -0.2194281816482544},\n",
              "   {'answer': [['Taiwan/USA'], ''], 'chunk': 18, 'model': 'TagOp'},\n",
              "   {'answer': 'Italy',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 21.93501091003418},\n",
              "   {'answer': [['Argentina'], ''], 'chunk': 19, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.83440113067627},\n",
              "   {'answer': [['Japan Aerospace Exploration Agency (JAXA)'], ''],\n",
              "    'chunk': 20,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.748451232910156},\n",
              "   {'answer': [['USA'], ''], 'chunk': 21, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.89371109008789},\n",
              "   {'answer': [['USA'], ''], 'chunk': 22, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.68315124511719},\n",
              "   {'answer': [['DSCS III-F11 (USA 148, DSCS III B-8) (Defense Satellite Communications System)'],\n",
              "     ''],\n",
              "    'chunk': 23,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.862667083740234},\n",
              "   {'answer': [['UAE'], ''], 'chunk': 24, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.97487449645996},\n",
              "   {'answer': [['USA'], ''], 'chunk': 25, 'model': 'TagOp'},\n",
              "   {'answer': '', 'chunk': 26, 'model': 'TaPaS', 'score': -0.9224648475646973},\n",
              "   {'answer': [['France'], ''], 'chunk': 26, 'model': 'TagOp'},\n",
              "   {'answer': 'Multinational',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.992922306060791},\n",
              "   {'answer': [['Estonia'], ''], 'chunk': 27, 'model': 'TagOp'},\n",
              "   {'answer': 'Multinational',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.206039428710938},\n",
              "   {'answer': [[], ''], 'chunk': 28, 'model': 'TagOp'},\n",
              "   {'answer': 'Multinational',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.13273239135742},\n",
              "   {'answer': [[], ''], 'chunk': 29, 'model': 'TagOp'},\n",
              "   {'answer': 'Multinational',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.974159240722656},\n",
              "   {'answer': [[], ''], 'chunk': 30, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.5116297006607056},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 31, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.80792236328125},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 32, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.013763427734375},\n",
              "   {'answer': [[], ''], 'chunk': 33, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.42302703857422},\n",
              "   {'answer': [['USA'], ''], 'chunk': 34, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.341388702392578},\n",
              "   {'answer': [['USA'], ''], 'chunk': 35, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.803443908691406},\n",
              "   {'answer': [['USA'], ''], 'chunk': 36, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.19478225708008},\n",
              "   {'answer': [['USA'], ''], 'chunk': 37, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.34947967529297},\n",
              "   {'answer': [['USA'], ''], 'chunk': 38, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.26666259765625},\n",
              "   {'answer': [['USA'], ''], 'chunk': 39, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.034161567687988},\n",
              "   {'answer': [['USA'], ''], 'chunk': 40, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.31001281738281},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 41, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.969017028808594},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 42, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.841046333312988},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 43, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.993804931640625},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 44, 'model': 'TagOp'},\n",
              "   {'answer': 'India',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.411579608917236},\n",
              "   {'answer': [['Germany/USA'], ''], 'chunk': 45, 'model': 'TagOp'},\n",
              "   {'answer': 'Greece',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.16404724121094},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'Spain',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.876008987426758},\n",
              "   {'answer': [['Spain'], ''], 'chunk': 47, 'model': 'TagOp'},\n",
              "   {'answer': 'Japan',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.686476230621338},\n",
              "   {'answer': [[], ''], 'chunk': 48, 'model': 'TagOp'},\n",
              "   {'answer': 'Japan',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.27719497680664},\n",
              "   {'answer': [['Japan'], ''], 'chunk': 49, 'model': 'TagOp'},\n",
              "   {'answer': 'United Kingdom',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.93375015258789},\n",
              "   {'answer': [['United Kingdom'], ''], 'chunk': 50, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.87016677856445},\n",
              "   {'answer': [['USA'], ''], 'chunk': 51, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.15571594238281},\n",
              "   {'answer': [['USA'], ''], 'chunk': 52, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.100006103515625},\n",
              "   {'answer': [['USA'], ''], 'chunk': 53, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.16403579711914},\n",
              "   {'answer': [['USA'], ''], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.55125427246094},\n",
              "   {'answer': [['USA'], ''], 'chunk': 55, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.47282791137695},\n",
              "   {'answer': [['USA'], ''], 'chunk': 56, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.77272415161133},\n",
              "   {'answer': [['USA'], ''], 'chunk': 57, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.618446350097656},\n",
              "   {'answer': [['USA'], ''], 'chunk': 58, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.05234909057617},\n",
              "   {'answer': [['USA'], ''], 'chunk': 59, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.767852783203125},\n",
              "   {'answer': [['USA'], ''], 'chunk': 60, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.193389892578125},\n",
              "   {'answer': [['USA'], ''], 'chunk': 61, 'model': 'TagOp'},\n",
              "   {'answer': 'Japan',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.30187225341797},\n",
              "   {'answer': [['Japan'], ''], 'chunk': 62, 'model': 'TagOp'},\n",
              "   {'answer': 'India',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.68414878845215},\n",
              "   {'answer': [['India'], ''], 'chunk': 63, 'model': 'TagOp'},\n",
              "   {'answer': 'Japan',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.2299346923828125},\n",
              "   {'answer': [[], ''], 'chunk': 64, 'model': 'TagOp'},\n",
              "   {'answer': 'South Korea',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.190515518188477},\n",
              "   {'answer': [['South Korea'], ''], 'chunk': 65, 'model': 'TagOp'},\n",
              "   {'answer': 'USA/Australia',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.192786455154419},\n",
              "   {'answer': [['USA/Australia'], ''], 'chunk': 66, 'model': 'TagOp'},\n",
              "   {'answer': 'Malaysia',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.766016006469727},\n",
              "   {'answer': [[], ''], 'chunk': 67, 'model': 'TagOp'},\n",
              "   {'answer': 'Multinational',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.629724502563477},\n",
              "   {'answer': [[], ''], 'chunk': 68, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.529415130615234},\n",
              "   {'answer': [[], ''], 'chunk': 69, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.42176055908203},\n",
              "   {'answer': [['USA'], ''], 'chunk': 70, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.31616973876953},\n",
              "   {'answer': [['DoD/US Air Force'], ''], 'chunk': 71, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.800472259521484},\n",
              "   {'answer': [['USA'], ''], 'chunk': 72, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.727542877197266},\n",
              "   {'answer': [['DoD/US Air Force'], ''], 'chunk': 73, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.17999267578125},\n",
              "   {'answer': [['DoD/US Air Force'], ''], 'chunk': 74, 'model': 'TagOp'},\n",
              "   {'answer': 'Egypt',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.790576934814453},\n",
              "   {'answer': [['Egypt'], ''], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': 'Netherlands',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.9236979484558105},\n",
              "   {'answer': [['USA'], ''], 'chunk': 76, 'model': 'TagOp'},\n",
              "   {'answer': 'Netherlands',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.14110565185547},\n",
              "   {'answer': [['Netherlands'], ''], 'chunk': 77, 'model': 'TagOp'},\n",
              "   {'answer': 'India',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.89216423034668},\n",
              "   {'answer': [['Israel'], ''], 'chunk': 78, 'model': 'TagOp'},\n",
              "   {'answer': 'Australia',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.247344970703125},\n",
              "   {'answer': [['Australia'], ''], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.53398132324219},\n",
              "   {'answer': [['USA'], ''], 'chunk': 80, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.70054244995117},\n",
              "   {'answer': [['USA'], ''], 'chunk': 81, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 32.46076583862305},\n",
              "   {'answer': [[], ''], 'chunk': 82, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.46700668334961},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 83, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.849776268005371},\n",
              "   {'answer': [['USA'], ''], 'chunk': 84, 'model': 'TagOp'},\n",
              "   {'answer': 'ESA',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.327621459960938},\n",
              "   {'answer': [['Belgium'], ''], 'chunk': 85, 'model': 'TagOp'},\n",
              "   {'answer': 'Japan',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.103131055831909},\n",
              "   {'answer': [['USA'], ''], 'chunk': 86, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.850521087646484},\n",
              "   {'answer': [['USA'], ''], 'chunk': 87, 'model': 'TagOp'},\n",
              "   {'answer': 'India',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.90227746963501},\n",
              "   {'answer': [['India'], ''], 'chunk': 88, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.07900619506836},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 89, 'model': 'TagOp'},\n",
              "   {'answer': 'ESA',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.627248764038086},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 90, 'model': 'TagOp'},\n",
              "   {'answer': 'Mexico',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.5196975469589233},\n",
              "   {'answer': [['UK/Canada'], ''], 'chunk': 91, 'model': 'TagOp'},\n",
              "   {'answer': 'Saudi Arabia',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.369873046875},\n",
              "   {'answer': [['Saudi Arabia'], ''], 'chunk': 92, 'model': 'TagOp'},\n",
              "   {'answer': 'Saudi Arabia',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.782588958740234},\n",
              "   {'answer': [['Saudi Arabia'], ''], 'chunk': 93, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.17328643798828},\n",
              "   {'answer': [['SB-WASS 3-3 (Space Based Wide Area Surveillance System) (NOSS 3-3, USA 181, NRO L23) '],\n",
              "     ''],\n",
              "    'chunk': 94,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.227760314941406},\n",
              "   {'answer': [['Brazil'], ''], 'chunk': 95, 'model': 'TagOp'},\n",
              "   {'answer': '', 'chunk': 96, 'model': 'TaPaS', 'score': -1.796831488609314},\n",
              "   {'answer': [['Japan'], ''], 'chunk': 96, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.95472526550293},\n",
              "   {'answer': [['USA'], ''], 'chunk': 97, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.65997314453125},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 98, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.90219497680664},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 99, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.994497776031494},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 100, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.08706283569336},\n",
              "   {'answer': [['USA'], ''], 'chunk': 101, 'model': 'TagOp'},\n",
              "   {'answer': 'United Kingdom',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.465797424316406},\n",
              "   {'answer': [['United Kingdom'], ''], 'chunk': 102, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.485294342041016},\n",
              "   {'answer': [['USA'], ''], 'chunk': 103, 'model': 'TagOp'},\n",
              "   {'answer': 'Japan',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.931078910827637},\n",
              "   {'answer': [['France/Belgium/Sweden'], ''], 'chunk': 104, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.771474838256836},\n",
              "   {'answer': [['USA'], ''], 'chunk': 105, 'model': 'TagOp'},\n",
              "   {'answer': 'Russia',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.982574462890625},\n",
              "   {'answer': [['Russia'], ''], 'chunk': 106, 'model': 'TagOp'},\n",
              "   {'answer': 'India',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.233583450317383},\n",
              "   {'answer': [['India'], ''], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.992219924926758},\n",
              "   {'answer': [['France'], ''], 'chunk': 108, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.36442947387695},\n",
              "   {'answer': [['USA'], ''], 'chunk': 109, 'model': 'TagOp'},\n",
              "   {'answer': 'Canada',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.131589889526367},\n",
              "   {'answer': [['Canada'], ''], 'chunk': 110, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 6.843508720397949},\n",
              "   {'answer': [[], ''], 'chunk': 111, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.86206817626953},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 112, 'model': 'TagOp'},\n",
              "   {'answer': 'Turkey',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.466259479522705},\n",
              "   {'answer': [['USA'], ''], 'chunk': 113, 'model': 'TagOp'},\n",
              "   {'answer': 'United Kingdom',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.09356689453125},\n",
              "   {'answer': [['United Kingdom'], ''], 'chunk': 114, 'model': 'TagOp'},\n",
              "   {'answer': 'Venezuela',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.131942272186279},\n",
              "   {'answer': [['Italy'], ''], 'chunk': 115, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.97837448120117},\n",
              "   {'answer': [['USA'], ''], 'chunk': 116, 'model': 'TagOp'},\n",
              "   {'answer': 'USA',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.6247673034668},\n",
              "   {'answer': [[], ''], 'chunk': 117, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 5.092986106872559},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 118, 'model': 'TagOp'},\n",
              "   {'answer': 'United Arab Emirates',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.237308502197266},\n",
              "   {'answer': [['United Arab Emirates'], ''], 'chunk': 119, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.546974182128906},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 120, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.451332092285156},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 121, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 19.273170471191406},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 122, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.5950927734375},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 123, 'model': 'TagOp'},\n",
              "   {'answer': 'China (PR)',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.180328369140625},\n",
              "   {'answer': [['China (PR)'], ''], 'chunk': 124, 'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_5',\n",
              "  'question': 'Which country has the most number of contractors ?'},\n",
              " 'que_6': {'answers': [{'answer': 'Advanced Orion 2 (NROL 6, USA 139)',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.76141357421875},\n",
              "   {'answer': 'AEHF-1 (Advanced Extremely High Frequency satellite-1, USA 214)',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.78270721435547},\n",
              "   {'answer': 'Afghansat-1 (Eutelsat 28B, Eutelsat 48B, Eutelsat W2M)',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.95323181152344},\n",
              "   {'answer': 'Amazonas-2',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.57975769042969},\n",
              "   {'answer': 'AMC-18 (Americom 18)',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.87165832519531},\n",
              "   {'answer': 'Amos 5',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.120370864868164},\n",
              "   {'answer': 'Anik G1',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.67951965332031},\n",
              "   {'answer': 'Apstar 7',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.25203323364258},\n",
              "   {'answer': 'Astra 1KR',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.04887390136719},\n",
              "   {'answer': 'Astra 1M',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.42547607421875},\n",
              "   {'answer': 'Badr 5 (Arabsat 5B)',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.71788787841797},\n",
              "   {'answer': 'Brazilsat B-3 (Brasilsat B-3)',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.105201721191406},\n",
              "   {'answer': 'Brazilsat B-4 (Brasilsat B-4)',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.25465393066406},\n",
              "   {'answer': 'Can-X3aï¿½ï¿½(BRIght-star Target Explorer (BRITE), UniBRITE-1)',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.58221435546875},\n",
              "   {'answer': 'Chandra X-Ray Observatory (CXO)',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.73860168457031},\n",
              "   {'answer': 'Compass G-10 (Beidou ISGO-5)',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.5758056640625},\n",
              "   {'answer': 'Compass G-8 (Beidou IGSO-3)',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.86195373535156},\n",
              "   {'answer': 'COMSATBw-1 (COmmunications SATellite fï¿½r BundesWehr)',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 128.0279998779297},\n",
              "   {'answer': 'COSMIC-E (Formosat-3E, Constellation Observing System for Meteorology, Ionosphere and Climate)',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.829063415527344},\n",
              "   {'answer': 'CUSat-1 (Cornell University Satellite 1)',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.64208984375},\n",
              "   {'answer': 'DirecTV-1R ',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.514671325683594},\n",
              "   {'answer': 'DirecTV-5 (Tempo 1)',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.300506591796875},\n",
              "   {'answer': 'DSCS III-F10 (USA 135, DSCS III B-13) (Defense Satellite Communications System)',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.6895751953125},\n",
              "   {'answer': 'DubaiSat-1',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.20121765136719},\n",
              "   {'answer': 'Echostar 15',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.049628257751465},\n",
              "   {'answer': 'ELISA-E12 (ELectronic Intelligence by SAtellite)',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.280153274536133},\n",
              "   {'answer': 'e-st@r',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.81147766113281},\n",
              "   {'answer': 'Eutelsat 10A (Eutelsat W-2A)',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.0869255065918},\n",
              "   {'answer': 'Eutelsat 36A (Eutelsat W-4) ',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.38778305053711},\n",
              "   {'answer': 'Eutelsat 48A (Eutelsat W-48, Eurobird 9, Hot Bird 2, Eutelsat 2-F8, Eutelsat HB2)',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.217506170272827},\n",
              "   {'answer': 'Eutelsat KA-SAT 9A (KA-SAT)',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.40044021606445},\n",
              "   {'answer': 'Express-AM44',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.88880157470703},\n",
              "   {'answer': 'Fengyun 2F (FY-2F)',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.15291213989258},\n",
              "   {'answer': 'FLTSATCOM-8 (USA 46)',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 97.61143493652344},\n",
              "   {'answer': 'Galaxy-25 (G-25, Intelsat 1A-5, Telstar 5)',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.454376220703125},\n",
              "   {'answer': 'Garuda-1 (Aces 1)',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 16.24802589416504},\n",
              "   {'answer': 'Geotail (Geomagnetic Tail Laboratory)',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 138.08599853515625},\n",
              "   {'answer': 'Globalstar M037 (Globalstar 16)',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.01874542236328},\n",
              "   {'answer': 'Globalstar M072 (Globalstar 72)',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.52101516723633},\n",
              "   {'answer': 'Globalstar M089 (Globalstar 89, Globalstar 2-12)',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.867919921875},\n",
              "   {'answer': 'Glonass 719 (Glonass 36-2, Cosmos 2432)',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.2724723815918},\n",
              "   {'answer': 'Glonass 730 (Glonass 41-1, Cosmos 2456)',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.06369972229004},\n",
              "   {'answer': 'Glonass 737 (Glonass 43-2, Cosmos 2465)',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.71477508544922},\n",
              "   {'answer': 'GOES 13 (Geostationary Operational Environmental Satellite, GOES-N)',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.16497039794922},\n",
              "   {'answer': 'Gonets M-19',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.10928344726562},\n",
              "   {'answer': 'GSSAP 2 (Geosynchronous Space Situational Awareness Program, USA 254)',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.51362609863281},\n",
              "   {'answer': 'Hispasat 1C ',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.29373168945312},\n",
              "   {'answer': 'Hispasat 1D ',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 134.7987518310547},\n",
              "   {'answer': 'HYLAS 1 (Highly Adaptable Satellite)',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 121.2439956665039},\n",
              "   {'answer': 'Improved Trumpet 5 (NROL-28, National Reconnaissance Office Launch-28, SBIRS HEO-2, Twins 2, USA 200)',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.55111694335938},\n",
              "   {'answer': 'Integral (INTErnational Gamma-Ray Astrophysics Laboratory)',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 83.17616271972656},\n",
              "   {'answer': 'Intelsat 1R (IS-1R, PAS-1R, PanAmSat 1R)',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 1.571664571762085},\n",
              "   {'answer': 'Intelsat 21 (IS-21)',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.28163146972656},\n",
              "   {'answer': 'Intelsat 903 ',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 8.626311302185059},\n",
              "   {'answer': 'Interstellar Boundary EXplorer (IBEX)',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 113.07810974121094},\n",
              "   {'answer': 'Iridium 21A (Iridium SVO93)',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.54952049255371},\n",
              "   {'answer': 'Iridium 31 (Iridium SVO31)',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.818260192871094},\n",
              "   {'answer': 'Iridium 47 (Iridium SV047)',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.70093536376953},\n",
              "   {'answer': 'Iridium 6 (Iridium SV006)',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.422140121459961},\n",
              "   {'answer': 'Iridium 77 (Iridium SV077)',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 15.843857765197754},\n",
              "   {'answer': 'Iridium 94 (Iridium SV094)',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.4059944152832},\n",
              "   {'answer': 'IRNSS-1B (Indian Regional Navigation Satellite System)',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.499849319458008},\n",
              "   {'answer': 'JCSat 4A (JCSAT 6, Japan Communications Satellite 6)',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.39317321777344},\n",
              "   {'answer': 'Kalpana-1 (Metsat-1)',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.52274322509766},\n",
              "   {'answer': 'Kizuna (WINDS - Wideband InterNetworking engineering Demonstration Satellite)',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.74967956542969},\n",
              "   {'answer': 'Koreasat 6 (Mugungwha 6)',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.81832122802734},\n",
              "   {'answer': 'Leasat 5 (Syncom IV-5, Leased Satellite F5)',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 146.46951293945312},\n",
              "   {'answer': 'Meridian-4',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.74831771850586},\n",
              "   {'answer': 'Milstar DFS-1 (USA 99, Milstar 1-F1) (Military Strategic and Tactical Relay)',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.696746826171875},\n",
              "   {'answer': 'Milstar DFS-5 (USA 164, Milstar 2-F3) (Military Strategic and Tactical Relay)',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 71.76608276367188},\n",
              "   {'answer': 'MUOS-2 (Mobile User Objective System 2)',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.5509147644043},\n",
              "   {'answer': 'Navstar GPS IIF-2 (Navstar SVN 63, PRN 01, USA 232)',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.1500473022461},\n",
              "   {'answer': 'Navstar GPS IIF-5 (Navstar SVN 64, USA 248)',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 129.9794464111328},\n",
              "   {'answer': 'Navstar GPS IIR-6 (Navstar SVN 41, PRN 14, USA 154)',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.29814910888672},\n",
              "   {'answer': 'NigComSat-1R',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 33.410980224609375},\n",
              "   {'answer': 'Nilesat 201',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 85.44219970703125},\n",
              "   {'answer': 'NSS-11 (Worldsat-1, AAP-1, GE-1A) , NSS-7 ',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 71.43812561035156},\n",
              "   {'answer': 'N-Star C ',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 102.0172119140625},\n",
              "   {'answer': 'Optus B3',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.30234909057617},\n",
              "   {'answer': 'Optus D2',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.95954895019531},\n",
              "   {'answer': 'ORBCOMM FM-4 (ORBCOM G2)',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.87266540527344},\n",
              "   {'answer': 'ORBCOMM FM-8 (ORBCOMM A1)',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.63300323486328},\n",
              "   {'answer': 'Paksat-1R ',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.99701690673828},\n",
              "   {'answer': 'Parus-99 (Cosmos 2463)',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.90113830566406},\n",
              "   {'answer': 'Picard',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.5279769897461},\n",
              "   {'answer': 'Proba V (Project for On-Board Autonomy)',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 126.91551208496094},\n",
              "   {'answer': 'QZS-1 (Quazi-Zenith Satellite System, Michibiki)',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 153.30538940429688},\n",
              "   {'answer': 'Rascom-QAF 1R',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.80899429321289},\n",
              "   {'answer': 'Rodnik (Cosmos 2437, Strela 3M)',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.518638610839844},\n",
              "   {'answer': 'Rodnik (Cosmos 2482, Strela 3M)',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.78318214416504},\n",
              "   {'answer': 'Samba (part of Cluster quartet, Cluster 2 FM7)',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 85.60282897949219},\n",
              "   {'answer': 'Satmex 6',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.096893310546875},\n",
              "   {'answer': 'Saudicomsat-2 ',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.39849853515625},\n",
              "   {'answer': 'SBIRS GEO 1 (Space Based Infrared System Geosynchronous 1, USA 230)',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 122.70062255859375},\n",
              "   {'answer': 'SB-WASS 3-5 (Space Based Wide Area Surveillance System) NOSS 3-5, USA 229, NRO L34)',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 127.48051452636719},\n",
              "   {'answer': 'SDS III-5 (Satellite Data System) (NRO L-24, Scorpius, USA 198) ',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.35859680175781},\n",
              "   {'answer': 'SES-3',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.32516479492188},\n",
              "   {'answer': 'SES-8',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.76409912109375},\n",
              "   {'answer': 'Shijian 16 (SJ-16)',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.22039031982422},\n",
              "   {'answer': 'Shiyan 3 (SY3, Experimental Satellite 3)',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.990535736083984},\n",
              "   {'answer': 'Sirius 1 (SD Radio 1)',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.66089630126953},\n",
              "   {'answer': 'Sirius 3 (SD Radio 3)',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.262004852294922},\n",
              "   {'answer': 'Skynet 5C',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 146.74403381347656},\n",
              "   {'answer': 'Spektr-R/RadioAstron',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.42518997192383},\n",
              "   {'answer': 'Star One C3',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.58186340332031},\n",
              "   {'answer': 'Strela 3 (Cosmos 2391)',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.61088180541992},\n",
              "   {'answer': 'Strela 3 (Cosmos 2400)',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.00448608398438},\n",
              "   {'answer': 'Superbird 7 (Superbird C2)',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.98482131958008},\n",
              "   {'answer': 'Tango (part of Cluster quartet, Cluster 2 FM8)',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 77.39327239990234},\n",
              "   {'answer': 'TDRS-9 (Tracking and Data Relay Satellite, TDRS-I)',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.25281524658203},\n",
              "   {'answer': 'Telstar 11N',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 77.66845703125},\n",
              "   {'answer': 'THEMIS E (Time History of Events and Macroscale Interactions during Substorms) ',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.799339294433594},\n",
              "   {'answer': 'TianLian 2 (TL-1-02, CTDRS)',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.29878234863281},\n",
              "   {'answer': 'Trumpet 3 (NROL-4, National Reconnaissance Office Launch-4, USA 136)',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 96.14167022705078},\n",
              "   {'answer': 'UFO-7 (USA 127, F7 EHF) \"UHF Follow-On\"',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.268054008483887},\n",
              "   {'answer': 'US-KS Oko 87 (Cosmos-2422)',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.66532135009766},\n",
              "   {'answer': 'ViaSat-1',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.37351989746094},\n",
              "   {'answer': 'Wind (International Solar-Terrestrial Program)',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 96.32394409179688},\n",
              "   {'answer': 'XMM Newton (High Throughput X-ray Spectroscopy Mission)',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 85.54000091552734},\n",
              "   {'answer': 'Yamal-300K',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.2052993774414},\n",
              "   {'answer': 'Yaogan 15 (Remote Sensing Satellite 15)',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.51039123535156},\n",
              "   {'answer': 'Yaogan 19 (Remote Sensing Satellite 19)',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.62989807128906},\n",
              "   {'answer': 'Yubileiny (Jubilejnyj, Radio Sputnik 30 (RS-30))',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.36209106445312},\n",
              "   {'answer': 'Zhongxing 12 (Chinasat 12, ZX-12)',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.859130859375},\n",
              "   {'answer': 'Zhongxing 9 (Chinasat 9, Chinastar 9)',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.24851989746094}],\n",
              "  'class': 0,\n",
              "  'id': 'que_6',\n",
              "  'question': 'Which satellite has the highest Apogee ?'},\n",
              " 'que_7': {'answers': [{'answer': 'AVERAGE > 1, 6,330, 3,775, 4,143, 2,894, 3,500, 115, 4,500, 4,500, 5,000',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 93.90670776367188},\n",
              "   {'answer': [0.0215, ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,000, 6,169',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.18299102783203},\n",
              "   {'answer': [1.0056, ''], 'chunk': 1, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 14',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.88262939453125},\n",
              "   {'answer': [0.8771, ''], 'chunk': 2, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 6,650',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 103.96418762207031},\n",
              "   {'answer': [0.0, ''], 'chunk': 3, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,340, 4,200, 4,312, 2,081, 2,648, 2,500, 2,845, 3,909, 1,698',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.1693115234375},\n",
              "   {'answer': [0.9994, ''], 'chunk': 4, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,901, 1,935, 2,015, 4,100, 29',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.34746170043945},\n",
              "   {'answer': [1.0001, ''], 'chunk': 5, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,710, 4,500, 5,910, 4,715, 4,905, 2, 12, 12, 12, 12, 12, 12',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.833187103271484},\n",
              "   {'answer': [0.9996, ''], 'chunk': 6, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12, 1,383, 1,383, 4,680, 5,054, 4,630, 3,600, 1, 3,105, 3,480',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.16487884521484},\n",
              "   {'answer': [0.8573, ''], 'chunk': 7, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,137, 3,760, 3,813, 2,775, 2,924, 3,010, 3,690',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.18556213378906},\n",
              "   {'answer': ['', ''], 'chunk': 8, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,500, 5,345, 5,350, 3,635, 3,315, 3,643, 1,420, 6,052, 6,000, 1,500',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.34971237182617},\n",
              "   {'answer': [0.9993, ''], 'chunk': 9, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,471, 5,724, 3,080, 25, 3,250, 70, 3,304, 5,420, 4,940, 3,400',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.911949157714844},\n",
              "   {'answer': [0.0007, ''], 'chunk': 10, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1, 1, 1, 92',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.51013946533203},\n",
              "   {'answer': [inf, 'million'], 'chunk': 11, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,495, 10, 10, 1,980, 2,060, 2,910, 25, 384, 587, 400',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.04206848144531},\n",
              "   {'answer': [-0.0023, ''], 'chunk': 12, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5, 15',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.687828063964844},\n",
              "   {'answer': ['', ''], 'chunk': 13, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 159, 4,742, 4,600',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.71159362792969},\n",
              "   {'answer': [13.8839, ''], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,585, 3, 3, 3, 848, 2,200, 2,300, 2,300',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.792274475097656},\n",
              "   {'answer': ['', 'million'], 'chunk': 15, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,200, 6,000, 2,200, 3,800, 2,200, 2,300, 2,300, 2,200',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.27775573730469},\n",
              "   {'answer': ['', ''], 'chunk': 16, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,440, 2,440',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.98686218261719},\n",
              "   {'answer': ['', ''], 'chunk': 17, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 70, 70, 70, 70, 70, 1,700, 1,700, 1,700',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 86.31423950195312},\n",
              "   {'answer': ['', ''], 'chunk': 18, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,700, 720, 2, 2, 1',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.519020080566406},\n",
              "   {'answer': ['', ''], 'chunk': 19, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,120, 2,120, 50, 90, 310, 5,900, 5,900, 3,420',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 61.26896667480469},\n",
              "   {'answer': [0.9921, 'million'], 'chunk': 20, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 45',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.9378662109375},\n",
              "   {'answer': ['', ''], 'chunk': 21, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,155, 1,230, 6, 6, 6, 2,650, 1,156, 1,156, 1,156',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 70.93479919433594},\n",
              "   {'answer': ['', ''], 'chunk': 22, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,156, 1,156, 2,380, 2,380, 2,380, 200',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.061880111694336},\n",
              "   {'answer': [0.0, ''], 'chunk': 23, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 300, 27, 2, 3,287',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 35.0280647277832},\n",
              "   {'answer': [0.0022, ''], 'chunk': 24, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,674, 3,700, 4,027, 4,660, 4,000, 6,600, 10, 1,050, 1,766, 120',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.79253387451172},\n",
              "   {'answer': [-0.0017, ''], 'chunk': 25, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 120, 120, 120, 572, 4,854, 2,967, 2,934, 240, 350',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.54961395263672},\n",
              "   {'answer': [0.9783, ''], 'chunk': 26, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1, 3,535, 4,167, 5,900, 2,700, 5,400, 2,885, 2,500, 5,000',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.58464050292969},\n",
              "   {'answer': ['', ''], 'chunk': 27, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, 3,170, 2,950, 1,525, 3,190, 5,600, 2,200, 5,967, 5,404',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 98.22549438476562},\n",
              "   {'answer': [0.0007, ''], 'chunk': 28, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,915, 5,102, 2,490, 4,050, 4,600, 5,250, 4,300, 3,150, 3,800',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.25628662109375},\n",
              "   {'answer': [24.8992, ''], 'chunk': 29, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,100, 4,892, 4,900, 4,880, 6,150, 2,600, 2,600',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.57454299926758},\n",
              "   {'answer': [0.0, ''], 'chunk': 30, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,600, 2,542, 2,600, 2,600, 2,532, 3,400, 1,726, 1,427, 46, 55',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.62755584716797},\n",
              "   {'answer': [0.9993, ''], 'chunk': 31, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,390, 1,390, 1,390, 2,300, 2,300, 2,300',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 80.78605651855469},\n",
              "   {'answer': [27.3734, ''], 'chunk': 32, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2, 2, 3, 2,310, 764, 215, 1',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.05825424194336},\n",
              "   {'answer': ['', ''], 'chunk': 33, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,488, 1,760, 4,060, 2,086, 2,033, 4,640, 4,100, 4,642, 4,690, 3,515, 3,790',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.256404876708984},\n",
              "   {'answer': [0.9997, ''], 'chunk': 34, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.0011, 0.0011099999999999999, 0.000321, 0.000169',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.242637634277344},\n",
              "   {'answer': ['', ''], 'chunk': 35, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 586, 686, 104,552, 704',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.75135040283203},\n",
              "   {'answer': [0.8669, ''], 'chunk': 36, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 450',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 73.2220230102539},\n",
              "   {'answer': ['', ''], 'chunk': 37, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 450, 700, 700, 700, 700, 700',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.460174560546875},\n",
              "   {'answer': [0.9993, ''], 'chunk': 38, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 700, 700, 700, 700, 700',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.32332611083984},\n",
              "   {'answer': [1.0007, ''], 'chunk': 39, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 700, 700, 700, 935, 1,480, 1,480, 1,480, 1,480, 1,480, 1,480, 1,480',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.25567626953125},\n",
              "   {'answer': ['', ''], 'chunk': 40, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Baikonur Cosmodrome, Baikonur Cosmodrome, Baikonur Cosmodrome',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.54616165161133},\n",
              "   {'answer': ['', ''], 'chunk': 41, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,415',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.58680534362793},\n",
              "   {'answer': ['', 'million'], 'chunk': 42, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,415, 1,415',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 29.559303283691406},\n",
              "   {'answer': ['', ''], 'chunk': 43, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 480',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.859619140625},\n",
              "   {'answer': ['', ''], 'chunk': 44, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 434',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.09075164794922},\n",
              "   {'answer': [4.8764, ''], 'chunk': 45, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,200, 4,200',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.77861022949219},\n",
              "   {'answer': ['', ''], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,288, 5,320, 470, 470, 890, 60, 60, 4, 2,300, 7',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 68.02720642089844},\n",
              "   {'answer': [-0.0008, ''], 'chunk': 47, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 11,110, 1, 2,242, 3,311, 1, 850, 850, 1,600, 1,600, 1,600',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.53001022338867},\n",
              "   {'answer': ['', ''], 'chunk': 48, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,600, 726, 4,000, 4,200, 2,064, 2,070, 2,066, 2,000, 5,959',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.131134033203125},\n",
              "   {'answer': ['', ''], 'chunk': 49, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,458, 6,070',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 80.45121765136719},\n",
              "   {'answer': [0.9992, ''], 'chunk': 50, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,739, 5,576, 2,491, 5,613, 2,550, 2,450, 5,540, 3,200, 4,793',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.3415641784668},\n",
              "   {'answer': [0.9994, ''], 'chunk': 51, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,984, 6,199, 3,200, 4,100, 3,124, 2,730, 4,200, 3,833, 3,642, 3,642',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.804752349853516},\n",
              "   {'answer': [-616.7931, ''], 'chunk': 52, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,653, 3,592, 3,420, 3,659, 4,723, 4,723, 4,723, 4,680, 4,723, 4,723, 4,685, 2,550',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.85007095336914},\n",
              "   {'answer': [0.9995, ''], 'chunk': 53, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,000, 462, 1, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.005126953125},\n",
              "   {'answer': [1091.4634, 'million'], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.44557189941406},\n",
              "   {'answer': [1.0039, ''], 'chunk': 55, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 93.34547424316406},\n",
              "   {'answer': [1.0039, ''], 'chunk': 56, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689, 689, 689, 689, 689, nan, 689',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.97926330566406},\n",
              "   {'answer': [1.0039, ''], 'chunk': 57, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 102.8634262084961},\n",
              "   {'answer': [1.0039, ''], 'chunk': 58, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.59919738769531},\n",
              "   {'answer': [0.1109, ''], 'chunk': 59, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689, 689, 689, 689, 689, 689, 689',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 94.16342163085938},\n",
              "   {'answer': [1.0039, ''], 'chunk': 60, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 689, 689, 689, 689',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 86.06685638427734},\n",
              "   {'answer': [1.0387, ''], 'chunk': 61, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,400, 3,531, 4,500, 2,982, 2,500, 2,900, 4,400, 4,000',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.82682037353516},\n",
              "   {'answer': [178860000.0, ''], 'chunk': 62, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 830, 185, 1,300, nan',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.389705657958984},\n",
              "   {'answer': [0.0005, ''], 'chunk': 63, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,800, 4,850, 3, 6,000, 800, 980, 1,400, 1,150',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 96.94756317138672},\n",
              "   {'answer': ['', ''], 'chunk': 64, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 60',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.768083572387695},\n",
              "   {'answer': [1.0003, ''], 'chunk': 65, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,400, 4, 5,000, 950, 1,282, 1,148, 47, 1, 2, 1,450',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.03497314453125},\n",
              "   {'answer': [357.3141, ''], 'chunk': 66, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,900, 2,417, 1,000, 8,000, 3,900, 2,500, 2,500, nan, 2,700, 2,778',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.341650009155273},\n",
              "   {'answer': [-3.2751, ''], 'chunk': 67, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,000, 696, 2,000, 2,000, 4,193, 4,085, 2,900',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 110.24163818359375},\n",
              "   {'answer': ['', ''], 'chunk': 68, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,536, 4,536, 4,536, 4,536, 65, 110, 53, 64, 2,850, 2,850',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.74930191040039},\n",
              "   {'answer': [0.0001, 'million'], 'chunk': 69, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 587, 2,900, 2,900, 6,804, 6,804, 1,430, 1,816',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.394493103027344},\n",
              "   {'answer': ['', ''], 'chunk': 70, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,816, 1,816, 1,816, 1,816, 1,816, 1,816, 1,816, 1,816, 1,630, 1,630',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.67483520507812},\n",
              "   {'answer': ['', ''], 'chunk': 71, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,630, 1,630, 1,630, 1,630, 2,217, 2,217, 2,217, 2,217, 2,217',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 93.1053466796875},\n",
              "   {'answer': [0.9992, ''], 'chunk': 72, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,217, 2,217, 2,217, 2,217, 2,217, 2,217, 2,217, 2,217, 2,060',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 116.10538482666016},\n",
              "   {'answer': ['', ''], 'chunk': 73, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,060, 2,217, 2,060, 2,217, 2,059, 74, 494, 300, 100',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.740291595458984},\n",
              "   {'answer': [1.0079, ''], 'chunk': 74, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,825, 3,200, 3,600, 4,850, 4,745, 4,745, 2,223, 2,223, 1,420, 2,128',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.19865798950195},\n",
              "   {'answer': [-0.0002, ''], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1, 5,396, 3,582, 3,412, 4,575, 4,500, 3,642, 3,720',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.13446044921875},\n",
              "   {'answer': ['', ''], 'chunk': 76, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,238, 1,625, 360, 6, 700, 650, 700, 700, 650, 650, 650, 700',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.04937744140625},\n",
              "   {'answer': [0.0051, ''], 'chunk': 77, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Svobodny Cosmodrome',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.81831359863281},\n",
              "   {'answer': ['', ''], 'chunk': 78, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,300, 2,501',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.039485931396484},\n",
              "   {'answer': [223.5037, ''], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 45',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.7071533203125},\n",
              "   {'answer': ['', ''], 'chunk': 80, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 45, 45, 45, 5',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.6113395690918},\n",
              "   {'answer': ['', ''], 'chunk': 81, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5, 434, 5, 61, 5,120, 3,014, 4,100, 120, 795, 795',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.706207275390625},\n",
              "   {'answer': [249.5, 'million'], 'chunk': 82, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 825, 825, 820, 825, 825, 810, 810, 820, 10, 6, 6, 7,000',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 71.1332778930664},\n",
              "   {'answer': ['', ''], 'chunk': 83, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1, 6,800, 150, 1, 1,000, 970, 1, 5, 100',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.581947326660156},\n",
              "   {'answer': ['', ''], 'chunk': 84, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 130',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 80.50508117675781},\n",
              "   {'answer': ['', ''], 'chunk': 85, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2, 2, 5,514, nan, 4,000, 2,924, nan, 2,400, 2,500, 2,400',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.45012283325195},\n",
              "   {'answer': ['', ''], 'chunk': 86, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 175, 175, 175, 175, 175',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.78751373291016},\n",
              "   {'answer': [0.9734, 'million'], 'chunk': 87, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,206, 6,650, 5,900, 1,858, 93, 41, 225, 280, 280, 280',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.82379150390625},\n",
              "   {'answer': ['', ''], 'chunk': 88, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 280, 280, 280, 225, 225, 280, 250, 250, 250, 280',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.82083511352539},\n",
              "   {'answer': ['', ''], 'chunk': 89, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 280, 280, 280, 1,200, 485, 1,600, 1,200, 1,200',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.181007385253906},\n",
              "   {'answer': [0.1428, ''], 'chunk': 90, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 770, 770, 770, 770, 770',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.86554718017578},\n",
              "   {'answer': [1.0791, 'million'], 'chunk': 91, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Saudicomsat-1, Saudicomsat-3, Saudicomsat-6, Saudicomsat-7, Saudisat 1C (Oscar 50, SO 50)',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.520599365234375},\n",
              "   {'answer': ['', ''], 'chunk': 92, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 100, 4,500, 4,500, 1,031, 5,000+, 5,000+, 5,000+, 5,000+',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 118.11769104003906},\n",
              "   {'answer': ['', ''], 'chunk': 93, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,000+, 5,000+, 5,000+, 5,000+, 5,000+, nan, 6,500, 6,500',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.40100860595703},\n",
              "   {'answer': [63.4, ''], 'chunk': 94, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 110, 3,100',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.90279769897461},\n",
              "   {'answer': [0.5323, ''], 'chunk': 95, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 50, 3, 2,300, nan, 3,200, 3,112, nan, 6,007, 6,140, 4,007',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.8304672241211},\n",
              "   {'answer': [-354.2673, ''], 'chunk': 96, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,138, nan, nan, nan, nan, nan, nan, nan',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.230104446411133},\n",
              "   {'answer': ['', ''], 'chunk': 97, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Long March 4B, Long March 4B, Long March 4B, Long March 4B, Long March 4B, Long March 4B, Long March 4B',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.80322265625},\n",
              "   {'answer': ['', ''], 'chunk': 98, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, nan, nan, nan, nan, 35, 204, 300',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.241348266601562},\n",
              "   {'answer': ['', ''], 'chunk': 99, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 300, 204, 169, 2,596, 3,038, 170, 5,000, 3,727',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.63231658935547},\n",
              "   {'answer': ['', ''], 'chunk': 100, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,792, 3,727, 4,400, 5,800, 6,020, 5,983, 1,474, 1,510, 1,510, 4,700, 4,635',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.03288650512695},\n",
              "   {'answer': ['', ''], 'chunk': 101, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,600, 4,638, nan, 90, 5,360, 5, 3, 5, 5, 658',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.27249526977539},\n",
              "   {'answer': ['', ''], 'chunk': 102, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 48, 50, 315, 6,100, 5,993, 5,993, 3,680, 3,660, 117, 117',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.60171890258789},\n",
              "   {'answer': ['', ''], 'chunk': 103, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,030, 714, 7, 4,100',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.613956451416016},\n",
              "   {'answer': [11.0, ''], 'chunk': 104, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3, 180, 137, 4, 225, 225, 225, 225, 225',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.39582061767578},\n",
              "   {'answer': ['', ''], 'chunk': 105, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 225, 225, 225, 93',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.641700744628906},\n",
              "   {'answer': ['', ''], 'chunk': 106, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1, 5,000, 4,051, 3,130, 1,600, 468, 468, 468, 1,463, 1, 3,725',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.47677612304688},\n",
              "   {'answer': ['', ''], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,350',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.37103271484375},\n",
              "   {'answer': [0.0, ''], 'chunk': 108, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3,180, 3,180, 3,180, 3,180, 3,180',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.67234802246094},\n",
              "   {'answer': [-214.0302, ''], 'chunk': 109, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,230, 120',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.404447555541992},\n",
              "   {'answer': ['', ''], 'chunk': 110, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 126, 126, 750, 1,400, 2,024, 3,050',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.84489059448242},\n",
              "   {'answer': [0.0053, ''], 'chunk': 111, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8,506, 2,500, 2,500, 3,750, 2,250, 2,200, 9, 1',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 85.63496398925781},\n",
              "   {'answer': [0.0022, ''], 'chunk': 112, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 587, 1, 5,000, 3,820, 3,900, 3,200, 14, 3,100, 4,869',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.86172103881836},\n",
              "   {'answer': [0.9993, ''], 'chunk': 113, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3, 3,200, 97',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.28569793701172},\n",
              "   {'answer': [1565000.0, ''], 'chunk': 114, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.97459411621094},\n",
              "   {'answer': [0.8755, ''], 'chunk': 115, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1, 29, 29, 6,740, 2,600, 2,970, 115, 880, 5,900, 5,987',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 95.09237670898438},\n",
              "   {'answer': [-311.087, ''], 'chunk': 116, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.92576599121094},\n",
              "   {'answer': [1.0, ''], 'chunk': 117, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4,703, 5,193, 4,682, 4,667',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 105.03084564208984},\n",
              "   {'answer': [73.1738, ''], 'chunk': 118, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5,953, 6,000, 1,360, 1,320, 1,640, 4,463',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.67422103881836},\n",
              "   {'answer': ['', ''], 'chunk': 119, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Long March 4B, Long March 4C, Long March 4C, Long March 4C, Long March 4C, Long March 4C, Long March 4C',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.24679946899414},\n",
              "   {'answer': ['', ''], 'chunk': 120, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, nan, 1,200, 2,700, 2,700, 2,700',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.787071228027344},\n",
              "   {'answer': ['', ''], 'chunk': 121, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > Long March 2C, Long March 2D, Long March 4C, Long March 4C, Long March 4C, Long March 4C, PSLV C16, Rokot, Dnepr',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 29.62810516357422},\n",
              "   {'answer': ['', ''], 'chunk': 122, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,500, 4, 4, 5,000, 5,100, 5,500, 5,200',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.77869415283203},\n",
              "   {'answer': ['', ''], 'chunk': 123, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2,300, 2,300, 5,200, nan, 1,500, 2,650',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.66473388671875},\n",
              "   {'answer': ['', ''], 'chunk': 124, 'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_7',\n",
              "  'question': 'What is the average Launch Mass of satellites by Germany ?'},\n",
              " 'que_8': {'answers': [{'answer': 'AVERAGE > 15 yrs., 15 yrs., 12 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 101.30130004882812},\n",
              "   {'answer': [6945.3433, 'percent'], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 14 yrs., 14 yrs., 14 yrs., 2 yrs., 2 yrs., 3 yrs.',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.645748138427734},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 1, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.36173629760742},\n",
              "   {'answer': [97.98, 'percent'], 'chunk': 2, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.53787994384766},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 3, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.53853607177734},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 4, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs.',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.15343475341797},\n",
              "   {'answer': [1436.12, 'percent'], 'chunk': 5, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs.',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 26.42368507385254},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 6, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, 15 yrs., 15 yrs., 12 yrs., .25 yrs., 10 yrs., 15 yrs.',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.454345703125},\n",
              "   {'answer': [40.6667, 'percent'], 'chunk': 7, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 14 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.30188751220703},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 8, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 15 yrs., 14yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 10 yrs.',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 109.26180267333984},\n",
              "   {'answer': [11450.0, 'percent'], 'chunk': 9, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 1 yr., 15 yrs., 2+ yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 103.22330474853516},\n",
              "   {'answer': [7.12, 'percent'], 'chunk': 10, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1 yr., 1 yr., 1 yr., 7 yrs., 1.5 yrs., 5 yrs., 12 yrs., 12 yrs., 12 yrs.',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.49038696289062},\n",
              "   {'answer': [13098.9367, 'percent'], 'chunk': 11, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12 yrs., 2 yrs., 13 yrs., 15 yrs., 15 yrs., 3 yrs., 5 yrs.',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.43099975585938},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 12, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, .5 yrs., 6 yrs.',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.849693298339844},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 13, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 5 yrs., 15 yrs.',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.08335876464844},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': '15 yrs., 2 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.2411117553711},\n",
              "   {'answer': [17862.03, 'percent'], 'chunk': 15, 'model': 'TagOp'},\n",
              "   {'answer': '8 yrs.',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.9147720336914},\n",
              "   {'answer': ['', 'percent'], 'chunk': 16, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 54.4943733215332},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 17, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 2 yrs., 2 yrs., 2 yrs., 2 yrs., 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 88.40713500976562},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 18, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 3 yrs., nan, nan, nan, nan, .5 yrs., 1 yr., 1 yr.',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.16881561279297},\n",
              "   {'answer': [39153.0, 'percent'], 'chunk': 19, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs., 7 yrs., 1.5 yrs., 5 yrs., 10 yrs., nan, 15 yrs., nan, 15 yrs., 15 yrs.',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.52177047729492},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 20, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 1 yr., 5 yrs., 5 yrs.',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.63761520385742},\n",
              "   {'answer': [2868.045, 'percent'], 'chunk': 21, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 5 yrs., .5 yrs., nan, 7 yrs., 10 yrs., 10 yrs., 10 yrs.',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 104.5320053100586},\n",
              "   {'answer': [500.97, 'percent'], 'chunk': 22, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs., 5 yrs., 7-9 yrs., 7-9 yrs., 7-9 yrs., 5 yrs.',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.3155403137207},\n",
              "   {'answer': [1062.0, 'percent'], 'chunk': 23, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 12 yrs., 15 yrs., 15 yrs., 18 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 59.78915023803711},\n",
              "   {'answer': [4095.6667, 'percent'], 'chunk': 24, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12 yrs., 12 yrs., 13 yrs., 15 yrs., 10 yrs.',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.10994338989258},\n",
              "   {'answer': [10500.0, 'percent'], 'chunk': 25, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 6 yrs., 5+ yrs., 6 yrs., 6 yrs., 10 yrs., 1 yr.',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.076290130615234},\n",
              "   {'answer': [105.5567, 'percent'], 'chunk': 26, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1 yr., 18 yrs., 16 yrs.',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.742828369140625},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 27, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 12 yrs., 12 yrs., 10 yrs., 12 yrs., 15 yrs., 8 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 100.4262924194336},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 28, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 14 yrs., 15 yrs., 12 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 12 yrs.',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.1546401977539},\n",
              "   {'answer': [1436.07, 'percent'], 'chunk': 29, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., nan, 12 yrs., 10 yrs.',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.178794860839844},\n",
              "   {'answer': [11666.6667, 'percent'], 'chunk': 30, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7-10 yrs., 12 yrs., 12 yrs., 12 yrs., 12 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.720130920410156},\n",
              "   {'answer': [3718.05, 'percent'], 'chunk': 31, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 3 yrs.',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.61983871459961},\n",
              "   {'answer': [1005.0, 'percent'], 'chunk': 32, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 3 yrs., 1 yr., 5 yrs., 5 yrs.',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.88414764404297},\n",
              "   {'answer': [107.0, 'percent'], 'chunk': 33, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 14 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 13 yrs., 13 yrs.',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.91002655029297},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 34, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 13 trs,, 15 yrs., 12 yrs., 12 yrs., 12 yrs., 12 yrs., 5-8 yrs., 12 yrs., 5 yrs.',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.5349235534668},\n",
              "   {'answer': [11806.6667, 'percent'], 'chunk': 35, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 7+ yrs., nan, 5 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs.',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.41838455200195},\n",
              "   {'answer': ['', 'percent'], 'chunk': 36, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs.',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 74.89752960205078},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 37, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.66961669921875},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 38, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs.',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.23526000976562},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 39, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 10 yrs., 7 yrs., 7 yrs., 7 yrs., 7 yrs., 7 yrs., 7 yrs., 7 yrs.',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 83.01280975341797},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 40, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs.',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.31983947753906},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 41, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs., 7 yrs., 7 yrs., 7 yrs., 7 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs., 10 yrs.',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.05794525146484},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 42, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 10 yrs.',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 37.12495803833008},\n",
              "   {'answer': [2166.6667, 'percent'], 'chunk': 43, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5-7 yrs.',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.05841064453125},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 44, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 15 yrs.',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 53.16807174682617},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 45, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 5 yrs., 5 yrs., 15 yrs., 3 yrs., 15 yrs.',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.35281372070312},\n",
              "   {'answer': [1932.6667, 'percent'], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 3 yrs., 3 yrs., 3 yrs., nan, 1 yr., 15 yrs., 1 yr.',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.69114303588867},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 47, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 15 yrs., 15 yrs., nan, 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.837318420410156},\n",
              "   {'answer': [4512.8333, 'percent'], 'chunk': 48, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 7 yrs., nan, 13 yrs., 13 yrs., 13 yrs., 13 yrs., 15 yrs.',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 48.333335876464844},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 49, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 12 yrs., 12 yrs., 12 yrs., 12 yrs., 2 yrs.',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.06145477294922},\n",
              "   {'answer': [8050.0, 'percent'], 'chunk': 50, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 13 yrs., 15 yrs., 16 yrs., 17 yrs., 15 yrs., 16 yrs., 15 yrs., 15 yrs., 15 yrs., 18 yrs.',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.93738555908203},\n",
              "   {'answer': [6218.04, 'percent'], 'chunk': 51, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 18 yrs., 15-18 yrs., 16 yrs., 12 yrs., 13 yrs., 10-15 yrs., 15 yrs.',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.480125427246094},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 52, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 14 yrs., 15 yrs., 13 yrs., 13 yrs., 13 yrs., 13 yrs., 13 yrs., 13 yrs., 13 yrs., 12 yrs.',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.832725524902344},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 53, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 30+ yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 88.5094985961914},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 85.75652313232422},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 55, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs.',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 90.7217025756836},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 56, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 76.36093139648438},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 57, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 99.32646942138672},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 58, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.82839965820312},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 59, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs., 8 yrs.',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 86.47067260742188},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 60, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8 yrs., 8 yrs., 8 yrs., 8 yrs., 2 yrs., 10 yrs., 10 yrs., 5 yrs., 1 yr., 5 yrs.',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 106.29405212402344},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 61, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 12 yrs., 11 yrs., 14.5 yrs., 12 yrs., 15 yrs., 1 yr.',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 92.78665924072266},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 62, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5-7 yrs., 7.25 yrs., 7 yrs., 12 yrs., 15 yrs., 5+ yrs., 5+ yrs., 5+ yrs., 5+ yrs.',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.20838165283203},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 63, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5+ yrs., nan, 1 yr., .3 yrs.,  3 yrs., 4 yrs., 5 yrs., nan',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.37214279174805},\n",
              "   {'answer': [14000.0, 'percent'], 'chunk': 64, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 5 yrs., 9 yrs., 9 yrs., 5 yrs.',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.237104415893555},\n",
              "   {'answer': [2000.0, 'percent'], 'chunk': 65, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 10 yrs., 2-3 yrs., 11.5 yrs.',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.05385208129883},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 66, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 5 yrs.',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 31.30333709716797},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 67, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs., 5 yrs., 12 yrs., 12 yrs., 5 yrs., 5 yrs., 15 yrs., 10 yrs.',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 88.20038604736328},\n",
              "   {'answer': [405.3333, 'percent'], 'chunk': 68, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 12 yrs., 12 hrs., 12 yrs., 1 yr., 3 yrs., 1 yr., nan, 12 yrs., 10 yrs.',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.42509460449219},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 69, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 5 yrs., 5 yrs., 10-12 yrs., 10-12 yrs., nan, 7 yrs., 7.5 yrs.',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 49.295833587646484},\n",
              "   {'answer': [1570.475, 'percent'], 'chunk': 70, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.5 yrs., 12 yrs., 12 yrs.',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 67.83979034423828},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 71, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 10 yrs.',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.27927017211914},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 72, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs.',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 89.16983032226562},\n",
              "   {'answer': [1136.0, 'percent'], 'chunk': 73, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 10 yrs., 15 yrs., 7 yrs.',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.16499710083008},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 74, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 12 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.88456344604492},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 16 yrs., 15 yrs., 15 yrs., 12 yrs.',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.60858917236328},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 76, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.5760498046875},\n",
              "   {'answer': [9.49, 'percent'], 'chunk': 77, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 2 yrs., 2 yrs., 4 yrs., 5 yrs., nan, 15 yrs., 10-14 yrs.',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 55.11336135864258},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 78, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.05035400390625},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.18600463867188},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 80, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5+ yrs., 5+ yrs., 5+ yrs., 5+ yrs., 5+ yrs., 5+ yrs.',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 80.78717041015625},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 81, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 15 yrs., 15 yrs., 15 yrs., 2 yrs., 2-3 yrs., 5 yrs.',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.895729064941406},\n",
              "   {'answer': [67.55, 'percent'], 'chunk': 82, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2-3 yrs., 5 yrs., 2-3 yrs., 2-3 yrs., 2-3 yrs., 2-3 yrs., 2-3 yrs., 2-3 yrs., 5 yrs., 3-5 yrs.',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 56.79261779785156},\n",
              "   {'answer': [93.85, 'percent'], 'chunk': 83, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 5 yrs., 5 yrs., 2 yrs.',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.718570709228516},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 84, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 11/2/2009, 5/7/2013',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.73051071166992},\n",
              "   {'answer': [67.595, 'percent'], 'chunk': 85, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15+ yrs., 10 yrs., nan, 7 yrs., 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 46.571834564208984},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 86, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7 yrs., 7 yrs., 7 yrs., 7 yrs., 7 yrs., 3 yrs., 15 yrs., 3 yrs., nan',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 44.779441833496094},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 87, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 3 yrs., 3 yrs., nan, 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.98577880859375},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 88, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 5 yrs., 5 yrs., 3 yrs., 3 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.5748519897461},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 89, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 5 yrs.',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 60.7519416809082},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 90, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 14 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 34.60118103027344},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 91, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, nan, nan, nan, nan, nan, nan',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 50.89985275268555},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 92, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12 yrs., 12 yrs.',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 72.92953491210938},\n",
              "   {'answer': [97.98, 'percent'], 'chunk': 93, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, nan, nan, nan, nan, nan, nan',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 47.38970184326172},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 94, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 5 yrs., 7-10 yrs., 7-10 yrs.',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.11732482910156},\n",
              "   {'answer': [1575.0, 'percent'], 'chunk': 95, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 7 yrs., 15 yrs., 15-18 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 88.7563705444336},\n",
              "   {'answer': ['', 'percent'], 'chunk': 96, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs.',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 78.48284912109375},\n",
              "   {'answer': [2287.035, 'percent'], 'chunk': 97, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 2 yrs., nan, 3 yrs., 3 yrs.',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 69.4412841796875},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 98, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs.',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.4379768371582},\n",
              "   {'answer': [97.15, 'percent'], 'chunk': 99, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, 5 yrs., 10-12 yrs., 10-12 yrs., 3 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 84.70873260498047},\n",
              "   {'answer': [200.35, 'percent'], 'chunk': 100, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15  yrs., 7 yrs., 7 yrs., 7 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 64.67870330810547},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 101, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, 3+ yrs.',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.467220306396484},\n",
              "   {'answer': ['', 'percent'], 'chunk': 102, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 6 yrs., 12 yrs., 12.6 yrs., 5+ yrs.',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 41.639495849609375},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 103, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5 yrs., 10 yrs., 10 yrs., 1 yr., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 65.51049041748047},\n",
              "   {'answer': [2715.0, 'percent'], 'chunk': 104, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 3 yrs.',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 82.49258422851562},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 105, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3 yrs., 3 yrs.',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 57.70785140991211},\n",
              "   {'answer': [141.0667, 'percent'], 'chunk': 106, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > .5 yrs., 15 yrs., 10 yrs., 10 yrs., 5 yrs., 4 yrs., 4 yrs., 4 yrs., 7 yrs., .25-1 yr., 12 yrs.',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 98.77252960205078},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 12 yrs., 5 yrs., 4 yrs., 11 yrs., 15 yrs., 15 yrs., 10 yrs.',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 79.77114868164062},\n",
              "   {'answer': [2252.6667, 'percent'], 'chunk': 108, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 10 yrs., 10 yrs., 10 yrs., 11 yrs., 11 yrs., nan, 15 yrs., 15 yrs.',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.1651611328125},\n",
              "   {'answer': [13525.3333, 'percent'], 'chunk': 109, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 13 yrs., 15 yrs., 13 yrs., 5 yrs., 15 yrs., 12 yrs., 12 yrs., 15 yrs.',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 75.990234375},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 110, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 2 yrs., 2 yrs., 5 yrs., 12 yrs., 15 yrs., 15 yrs., 12 yrs., 12 yrs.',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.352989196777344},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 111, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs.',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 51.0439567565918},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 112, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., nan, 3 yrs., nan, nan, nan',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.91007423400879},\n",
              "   {'answer': [5000.0, 'percent'], 'chunk': 113, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 4 yrs., 4 yrs., 4 yrs., 4 yrs., 4 yrs., 4 yrs., 5 yrs.',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 40.32115173339844},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 114, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 10.480424880981445},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 115, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 5 yrs., 5 yrs.',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.79913330078125},\n",
              "   {'answer': ['', 'percent'], 'chunk': 116, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.45545196533203},\n",
              "   {'answer': [16262.3967, 'percent'], 'chunk': 117, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 15 yrs., 15 yrs., 15 yrs., 15 yrs.',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 91.71639251708984},\n",
              "   {'answer': [9047.255, 'percent'], 'chunk': 118, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 15 yrs., 12 yrs., 12 yrs., 11 yrs.',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 30.863861083984375},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 119, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 63.40138626098633},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 120, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 58.33226013183594},\n",
              "   {'answer': ['', 'percent'], 'chunk': 121, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, nan, nan, nan, 2 yrs., nan, nan',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.46050262451172},\n",
              "   {'answer': [nan, 'percent'], 'chunk': 122, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 3-5 yrs., 15 yrs.',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 43.847625732421875},\n",
              "   {'answer': [93.6, 'percent'], 'chunk': 123, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > nan, nan, nan, nan, 4 yrs.',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 52.15537643432617},\n",
              "   {'answer': [13368.7167, 'percent'], 'chunk': 124, 'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_8',\n",
              "  'question': 'What is the average expected lifetime of all the satellites ?'},\n",
              " 'que_9': {'answers': [{'answer': 'AVERAGE > ',\n",
              "    'chunk': 0,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -30.194246292114258},\n",
              "   {'answer': ['', ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -13.970991134643555},\n",
              "   {'answer': ['', ''], 'chunk': 1, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 2,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -12.133447647094727},\n",
              "   {'answer': ['', ''], 'chunk': 2, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1.1900000000000001e-05',\n",
              "    'chunk': 3,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 22.620765686035156},\n",
              "   {'answer': ['', ''], 'chunk': 3, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 4,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -23.490360260009766},\n",
              "   {'answer': ['', ''], 'chunk': 4, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 5,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -10.523737907409668},\n",
              "   {'answer': ['', ''], 'chunk': 5, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 6,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -35.18122863769531},\n",
              "   {'answer': ['', ''], 'chunk': 6, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00725',\n",
              "    'chunk': 7,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.067094802856445},\n",
              "   {'answer': ['', ''], 'chunk': 7, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5.93e-05',\n",
              "    'chunk': 8,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 66.84782409667969},\n",
              "   {'answer': ['', ''], 'chunk': 8, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 9,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -23.447113037109375},\n",
              "   {'answer': ['', ''], 'chunk': 9, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 10,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -28.344526290893555},\n",
              "   {'answer': ['', ''], 'chunk': 10, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00195',\n",
              "    'chunk': 11,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 3.414961338043213},\n",
              "   {'answer': ['', ''], 'chunk': 11, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 12,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -12.887223243713379},\n",
              "   {'answer': ['', ''], 'chunk': 12, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 13,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -11.987433433532715},\n",
              "   {'answer': ['', 'percent'], 'chunk': 13, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 14,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -62.32418441772461},\n",
              "   {'answer': ['', ''], 'chunk': 14, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.120000000000001e-05',\n",
              "    'chunk': 15,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.419650077819824},\n",
              "   {'answer': ['', ''], 'chunk': 15, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 16,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -47.97289276123047},\n",
              "   {'answer': ['', ''], 'chunk': 16, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 17,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -59.53180694580078},\n",
              "   {'answer': ['', ''], 'chunk': 17, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs., 5 yrs.',\n",
              "    'chunk': 18,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 13.510796546936035},\n",
              "   {'answer': ['', ''], 'chunk': 18, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 19,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.177724838256836},\n",
              "   {'answer': ['', ''], 'chunk': 19, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 20,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -35.98250961303711},\n",
              "   {'answer': [-5.0, ''], 'chunk': 20, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 21,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -33.03533935546875},\n",
              "   {'answer': ['', ''], 'chunk': 21, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 22,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -17.611351013183594},\n",
              "   {'answer': ['', ''], 'chunk': 22, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.0019199999999999998',\n",
              "    'chunk': 23,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 27.470762252807617},\n",
              "   {'answer': ['', ''], 'chunk': 23, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 24,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -53.15953826904297},\n",
              "   {'answer': ['', ''], 'chunk': 24, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 8.3e-05, 8.3e-05',\n",
              "    'chunk': 25,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.649349212646484},\n",
              "   {'answer': [-34345.93, ''], 'chunk': 25, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 26,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -8.72337532043457},\n",
              "   {'answer': ['', ''], 'chunk': 26, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 27,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -7.631039142608643},\n",
              "   {'answer': ['', ''], 'chunk': 27, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.120000000000001e-05',\n",
              "    'chunk': 28,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 81.9957275390625},\n",
              "   {'answer': ['', ''], 'chunk': 28, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 29,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -49.73286437988281},\n",
              "   {'answer': ['', ''], 'chunk': 29, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 30,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -23.927541732788086},\n",
              "   {'answer': ['', ''], 'chunk': 30, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 31,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -33.153072357177734},\n",
              "   {'answer': ['', ''], 'chunk': 31, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 32,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -37.82139587402344},\n",
              "   {'answer': ['', ''], 'chunk': 32, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.0315',\n",
              "    'chunk': 33,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 12.78778076171875},\n",
              "   {'answer': ['', ''], 'chunk': 33, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 35,785, 35,781',\n",
              "    'chunk': 34,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 20.448524475097656},\n",
              "   {'answer': ['', ''], 'chunk': 34, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 35,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -19.864540100097656},\n",
              "   {'answer': ['', ''], 'chunk': 35, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 36,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -49.427703857421875},\n",
              "   {'answer': ['', ''], 'chunk': 36, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1,411, 1,415',\n",
              "    'chunk': 37,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.72182846069336},\n",
              "   {'answer': ['', ''], 'chunk': 37, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00025699999999999996',\n",
              "    'chunk': 38,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 2.0622031688690186},\n",
              "   {'answer': ['', ''], 'chunk': 38, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 6.42e-05',\n",
              "    'chunk': 39,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 71.62413024902344},\n",
              "   {'answer': ['', ''], 'chunk': 39, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 40,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -46.533267974853516},\n",
              "   {'answer': ['', ''], 'chunk': 40, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 41,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -3.471998929977417},\n",
              "   {'answer': ['', ''], 'chunk': 41, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00524',\n",
              "    'chunk': 42,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 38.158294677734375},\n",
              "   {'answer': ['', ''], 'chunk': 42, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 43,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -9.948397636413574},\n",
              "   {'answer': ['', ''], 'chunk': 43, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 44,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -23.248165130615234},\n",
              "   {'answer': ['', ''], 'chunk': 44, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 45,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -7.785728931427002},\n",
              "   {'answer': ['', ''], 'chunk': 45, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 46,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -52.44559097290039},\n",
              "   {'answer': ['', ''], 'chunk': 46, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 47,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -25.0770206451416},\n",
              "   {'answer': [0.1, ''], 'chunk': 47, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 48,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -26.55965805053711},\n",
              "   {'answer': ['', ''], 'chunk': 48, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 49,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -42.90772247314453},\n",
              "   {'answer': ['', ''], 'chunk': 49, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.120000000000001e-05',\n",
              "    'chunk': 50,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 62.19413757324219},\n",
              "   {'answer': ['', ''], 'chunk': 50, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 51,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -10.31198787689209},\n",
              "   {'answer': ['', ''], 'chunk': 51, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.000439',\n",
              "    'chunk': 52,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 29.16707992553711},\n",
              "   {'answer': ['', ''], 'chunk': 52, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 53,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -47.68742370605469},\n",
              "   {'answer': ['', ''], 'chunk': 53, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 54,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -20.101139068603516},\n",
              "   {'answer': ['', ''], 'chunk': 54, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 55,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -41.47695541381836},\n",
              "   {'answer': ['', ''], 'chunk': 55, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00021',\n",
              "    'chunk': 56,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 14.70130729675293},\n",
              "   {'answer': ['', ''], 'chunk': 56, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 57,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -36.240318298339844},\n",
              "   {'answer': ['', ''], 'chunk': 57, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 58,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -32.54266357421875},\n",
              "   {'answer': ['', ''], 'chunk': 58, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 59,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -17.049449920654297},\n",
              "   {'answer': ['', ''], 'chunk': 59, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5/5/1997',\n",
              "    'chunk': 60,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 18.666101455688477},\n",
              "   {'answer': ['', ''], 'chunk': 60, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7e-05',\n",
              "    'chunk': 61,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 24.730670928955078},\n",
              "   {'answer': ['', ''], 'chunk': 61, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 62,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -39.35661315917969},\n",
              "   {'answer': ['', ''], 'chunk': 62, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.120000000000001e-05',\n",
              "    'chunk': 63,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.698204040527344},\n",
              "   {'answer': ['', ''], 'chunk': 63, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 64,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.3238422870635986},\n",
              "   {'answer': ['', ''], 'chunk': 64, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 65,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -15.043643951416016},\n",
              "   {'answer': ['', ''], 'chunk': 65, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 66,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -48.267127990722656},\n",
              "   {'answer': ['', ''], 'chunk': 66, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 67,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -25.019140243530273},\n",
              "   {'answer': ['', ''], 'chunk': 67, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 68,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -32.98393249511719},\n",
              "   {'answer': ['', ''], 'chunk': 68, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 69,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -75.17662811279297},\n",
              "   {'answer': ['', ''], 'chunk': 69, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 70,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -75.60210418701172},\n",
              "   {'answer': ['', ''], 'chunk': 70, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 71,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -2.4049293994903564},\n",
              "   {'answer': ['', ''], 'chunk': 71, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 72,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -40.655277252197266},\n",
              "   {'answer': ['', ''], 'chunk': 72, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 73,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -55.49543762207031},\n",
              "   {'answer': ['', ''], 'chunk': 73, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 74,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -50.368892669677734},\n",
              "   {'answer': ['', ''], 'chunk': 74, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 75,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -36.09507751464844},\n",
              "   {'answer': ['', ''], 'chunk': 75, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 76,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -55.062320709228516},\n",
              "   {'answer': ['', ''], 'chunk': 76, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 9.49e-05',\n",
              "    'chunk': 77,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 39.30948257446289},\n",
              "   {'answer': ['', ''], 'chunk': 77, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 78,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -52.10136032104492},\n",
              "   {'answer': ['', ''], 'chunk': 78, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 79,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -33.42888641357422},\n",
              "   {'answer': [35618.02, ''], 'chunk': 79, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 80,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -37.00088882446289},\n",
              "   {'answer': ['', ''], 'chunk': 80, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 81,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -17.842998504638672},\n",
              "   {'answer': ['', ''], 'chunk': 81, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 5.93e-05',\n",
              "    'chunk': 82,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.00644302368164},\n",
              "   {'answer': [0.5, ''], 'chunk': 82, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 83,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -29.589569091796875},\n",
              "   {'answer': ['', ''], 'chunk': 83, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 84,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -9.625581741333008},\n",
              "   {'answer': ['', ''], 'chunk': 84, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 98.3',\n",
              "    'chunk': 85,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 0.4202617108821869},\n",
              "   {'answer': ['', ''], 'chunk': 85, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 86,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.635944366455078},\n",
              "   {'answer': ['', ''], 'chunk': 86, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 87,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -9.50013542175293},\n",
              "   {'answer': ['', ''], 'chunk': 87, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 88,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -49.17655563354492},\n",
              "   {'answer': ['', ''], 'chunk': 88, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 89,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -34.139827728271484},\n",
              "   {'answer': ['', ''], 'chunk': 89, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 90,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -63.04682922363281},\n",
              "   {'answer': ['', ''], 'chunk': 90, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 91,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -56.459251403808594},\n",
              "   {'answer': ['', ''], 'chunk': 91, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 92,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -48.92900466918945},\n",
              "   {'answer': ['', ''], 'chunk': 92, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 93,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -39.262184143066406},\n",
              "   {'answer': ['', ''], 'chunk': 93, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 94,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -72.51899719238281},\n",
              "   {'answer': ['', ''], 'chunk': 94, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 95,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -56.52124786376953},\n",
              "   {'answer': ['', ''], 'chunk': 95, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs.',\n",
              "    'chunk': 96,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 25.522781372070312},\n",
              "   {'answer': ['', ''], 'chunk': 96, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 97,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -17.48969268798828},\n",
              "   {'answer': ['', ''], 'chunk': 97, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 98,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -68.69195556640625},\n",
              "   {'answer': ['', ''], 'chunk': 98, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00411',\n",
              "    'chunk': 99,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 42.74561309814453},\n",
              "   {'answer': ['', ''], 'chunk': 99, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 100,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -68.37284851074219},\n",
              "   {'answer': ['', ''], 'chunk': 100, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.120000000000001e-05',\n",
              "    'chunk': 101,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 4.3279032707214355},\n",
              "   {'answer': ['', ''], 'chunk': 101, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 102,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -16.071683883666992},\n",
              "   {'answer': ['', ''], 'chunk': 102, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2.37e-05',\n",
              "    'chunk': 103,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 11.257740020751953},\n",
              "   {'answer': ['', ''], 'chunk': 103, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 104,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -54.853389739990234},\n",
              "   {'answer': ['', ''], 'chunk': 104, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 105,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -53.2568244934082},\n",
              "   {'answer': ['', ''], 'chunk': 105, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 106,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -75.74154663085938},\n",
              "   {'answer': ['', ''], 'chunk': 106, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 0.00143',\n",
              "    'chunk': 107,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 7.779305934906006},\n",
              "   {'answer': ['', ''], 'chunk': 107, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 108,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -35.28150939941406},\n",
              "   {'answer': ['', ''], 'chunk': 108, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 109,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -29.921165466308594},\n",
              "   {'answer': ['', ''], 'chunk': 109, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 110,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -32.513858795166016},\n",
              "   {'answer': ['', ''], 'chunk': 110, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 2 yrs.',\n",
              "    'chunk': 111,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 17.302188873291016},\n",
              "   {'answer': ['', ''], 'chunk': 111, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 112,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.8641300201416},\n",
              "   {'answer': ['', ''], 'chunk': 112, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.159999999999999e-05',\n",
              "    'chunk': 113,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 28.64375877380371},\n",
              "   {'answer': ['', ''], 'chunk': 113, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1.1900000000000001e-05',\n",
              "    'chunk': 114,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 45.06011962890625},\n",
              "   {'answer': ['', ''], 'chunk': 114, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 115,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -44.971046447753906},\n",
              "   {'answer': ['', ''], 'chunk': 115, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 116,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -19.625568389892578},\n",
              "   {'answer': ['', ''], 'chunk': 116, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 1.1900000000000001e-05, 1.1900000000000001e-05',\n",
              "    'chunk': 117,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 36.021053314208984},\n",
              "   {'answer': [1.0, ''], 'chunk': 117, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 118,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -9.58420181274414},\n",
              "   {'answer': ['', ''], 'chunk': 118, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 119,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -20.937580108642578},\n",
              "   {'answer': ['', ''], 'chunk': 119, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 120,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -67.25403594970703},\n",
              "   {'answer': ['', ''], 'chunk': 120, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 121,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -33.38118362426758},\n",
              "   {'answer': ['', ''], 'chunk': 121, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 7.269999999999999e-05',\n",
              "    'chunk': 122,\n",
              "    'model': 'TaPaS',\n",
              "    'score': 23.126148223876953},\n",
              "   {'answer': ['', ''], 'chunk': 122, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 123,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -18.320816040039062},\n",
              "   {'answer': ['', ''], 'chunk': 123, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ',\n",
              "    'chunk': 124,\n",
              "    'model': 'TaPaS',\n",
              "    'score': -31.366268157958984},\n",
              "   {'answer': ['', ''], 'chunk': 124, 'model': 'TagOp'}],\n",
              "  'class': 1,\n",
              "  'id': 'que_9',\n",
              "  'question': 'What is the difference between Dry Mass and Launch Mass of ABS-7 ?'}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('./answers.json', 'w') as f:\n",
        "  json.dump(answers, f, indent=2)"
      ],
      "metadata": {
        "id": "ZFFcZzNh1uOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('answers.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-wVyTt0JTXHw",
        "outputId": "1d8a1537-b90c-45c3-f27f-4f551f304714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d6568cbd-d9ad-4c45-8f72-df0f666a436a\", \"answers.json\", 447582)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {
        "id": "_DV6UQLJkI0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./outputs"
      ],
      "metadata": {
        "id": "pEcg9m8wm7Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.file_handling import write_output\n",
        "\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "write_output(tag_op_input, AlQA_pred, dataclass)\n",
        "AlQA_pred"
      ],
      "metadata": {
        "id": "pyKvVqFWkKTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AlQA_pred"
      ],
      "metadata": {
        "id": "1N6qtdG3VnSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r outputs.zip ./outputs/"
      ],
      "metadata": {
        "id": "nHrDYP67XJmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "WgD9QKtt1F95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tapas"
      ],
      "metadata": {
        "id": "p8_P1a2OaXmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing.tapas_preprop import prepare_tapas_data"
      ],
      "metadata": {
        "id": "C7Y_wT1EVrSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tapas_tab, queries = prepare_tapas_data(table, questions)\n",
        "# predicted_answer_coordinates, predicted_aggregation_indices = table_qa_model(tapas_tab, queries)\n",
        "answers = process_tapas_answers(predicted_answer_coordinates, predicted_aggregation_indices, tapas_tab, answers, 0)"
      ],
      "metadata": {
        "id": "2g2y_Fg_An9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
        "model = TFTapasForQuestionAnswering.from_pretrained(\"google/tapas-base-finetuned-wtq\")\n",
        "inputs = tokenizer(table=tapas_tab, queries=queries, padding=\"max_length\", return_tensors=\"tf\", truncation=True)\n",
        "outputs = model(**inputs)\n",
        "predicted_answer_coordinates, predicted_aggregation_indices = tokenizer.convert_logits_to_predictions(\n",
        "    inputs, outputs.logits, outputs.logits_aggregation\n",
        ")"
      ],
      "metadata": {
        "id": "2uWX1yljjQcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "EbZAQ8q8adkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f69d30-3310-47c8-e657-3dbabb04103f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([113.56533  ,  79.81653  ,   5.968251 ,   8.502582 ,  93.502785 ,\n",
              "       101.66904  ,  20.96558  ,   6.1085396,   9.733978 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(answers.keys())"
      ],
      "metadata": {
        "id": "B7C3529wb4lX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4076601-ad99-4bc9-df84-65bd89edaf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyXHtFuo8TnC",
        "outputId": "bbddf9f7-b3ba-4e4b-9330-538dbef5a3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'que_0': {'question': 'what is the average payment volume per transaction for american express?', 'id': 'que_0', 'class': 1, 'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'}, {'answer': [1.0157, ''], 'chunk': 0, 'model': 'TagOp'}]}, 'que_1': {'question': 'what is the average payment volume per transaction for JCB?', 'id': 'que_1', 'class': 1, 'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'}, {'answer': [1.1091, ''], 'chunk': 0, 'model': 'TagOp'}]}, 'que_2': {'question': 'what is the difference between total volume and payment volume for american express?', 'id': 'que_2', 'class': 1, 'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'}, {'answer': [10.0, 'million'], 'chunk': 0, 'model': 'TagOp'}]}, 'que_3': {'question': 'what is the difference between the total volume for JCB and total volume for mastercard?', 'id': 'que_3', 'class': 1, 'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'}, {'answer': [2215.0, 'million'], 'chunk': 0, 'model': 'TagOp'}]}, 'que_4': {'question': 'what is exclueded from the mastercard figures?', 'id': 'que_4', 'class': 1, 'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'}, {'answer': [['based debit card figures on mastercard cards , but not maestro or cirrus figures .'], ''], 'chunk': 0, 'model': 'TagOp'}]}, 'que_5': {'question': 'which companies are the largest operators of open-loop and closed-loop retail electronic payments networks?', 'id': 'que_5', 'class': 1, 'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'}, {'answer': [['visa , mastercard , american express , discover , jcb'], ''], 'chunk': 0, 'model': 'TagOp'}]}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'que_0': {'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': [1.0157, ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 637', 'chunk': 1, 'model': 'TaPaS', 'score': None}],\n",
              "  'class': 1,\n",
              "  'id': 'que_0',\n",
              "  'question': 'what is the average payment volume per transaction for american express?'},\n",
              " 'que_1': {'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': [1.1091, ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > 55', 'chunk': 1, 'model': 'TaPaS', 'score': None}],\n",
              "  'class': 1,\n",
              "  'id': 'que_1',\n",
              "  'question': 'what is the average payment volume per transaction for JCB?'},\n",
              " 'que_2': {'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': [10.0, 'million'], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ', 'chunk': 1, 'model': 'TaPaS', 'score': None}],\n",
              "  'class': 1,\n",
              "  'id': 'que_2',\n",
              "  'question': 'what is the difference between total volume and payment volume for american express?'},\n",
              " 'que_3': {'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': [2215.0, 'million'], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': 'AVERAGE > ', 'chunk': 1, 'model': 'TaPaS', 'score': None}],\n",
              "  'class': 1,\n",
              "  'id': 'que_3',\n",
              "  'question': 'what is the difference between the total volume for JCB and total volume for mastercard?'},\n",
              " 'que_4': {'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': [['based debit card figures on mastercard cards , but not maestro or cirrus figures .'],\n",
              "     ''],\n",
              "    'chunk': 0,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': '916', 'chunk': 1, 'model': 'TaPaS', 'score': None}],\n",
              "  'class': 1,\n",
              "  'id': 'que_4',\n",
              "  'question': 'what is exclueded from the mastercard figures?'},\n",
              " 'que_5': {'answers': [{'answer': [[], ''], 'chunk': 0, 'model': 'TagOp'},\n",
              "   {'answer': [['visa , mastercard , american express , discover , jcb'], ''],\n",
              "    'chunk': 0,\n",
              "    'model': 'TagOp'},\n",
              "   {'answer': 'Visa Inc.<sup>(1)</sup>',\n",
              "    'chunk': 1,\n",
              "    'model': 'TaPaS',\n",
              "    'score': None}],\n",
              "  'class': 1,\n",
              "  'id': 'que_5',\n",
              "  'question': 'which companies are the largest operators of open-loop and closed-loop retail electronic payments networks?'}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}